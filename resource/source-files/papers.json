[
	{
		"category": ["ai", "agent"],
		"title": "MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains",
		"authors": "Guoli Yin, Haoping Bai, Shuang Ma, Feng Nan, Yanchao Sun, Zhaoyang Xu, Shen Ma, Jiarui Lu, Xiang Kong, Aonan Zhang, Dian Ang Yap, Yizhe zhang, Karsten Ahnert, Vik Kamath, Mathias Berglund, Dominic Walsh, Tobias Gindele, Juergen Wiest, Zhengfeng Lai, Xiaoming Wang, Jiulong Shan, Meng Cao, Ruoming Pang, Zirui Wang",
		"url": {
			"arxiv": "https://arxiv.org/abs/2407.18961"
		},
		"date": "18-Jul-2024",
		"last_revised": "15-Aug-2024"
	},
	{
		"category": ["optimization"],
		"title": "Compact Model Parameter Extraction via Derivative-Free Optimization",
		"authors": "Rafael Perez Martinez, Masaya Iwamoto, Kelly Woo, Zhengliang Bian, Roberto Tinti, Stephen Boyd, Srabanti Chowdhury",
		"url": {
			"Boyd's homepage": "https://stanford.edu/~boyd/papers/compact_model_parameter_extraction.html",
			"arxiv": "https://arxiv.org/abs/2406.16355"
		},
		"date": "24-Jun-2024",
		"last_revised": "11-Nov-2024"
	},
	{
		"category": ["optimization", "cvxopt"],
		"title": "Optimization Algorithm Design via Electric Circuits",
		"authors": "Stephen P. Boyd, Tetiana Parshakova, Ernest K. Ryu, Jaewook J. Suh",
		"url": {
			"arxiv": "https://arxiv.org/abs/2411.02573"
		},
		"date": "04-Nov-2024"
	},
	{
		"category": ["code repo", "ai"],
		"title": "OpenAI's Swarm",
		"authors": "OpenAI",
		"url": {
			"github": "https://github.com/openai/swarm"
		},
		"date": "10-Oct-2024"
	},
	{
		"category": ["ai", "bio"],
		"title": "Predicting Gene Ontology Annotations from CAFA Using Distance Machine Learning and Transfer Metric Learning",
		"authors": "Shilpa Choudhary, MD Khaja Shaik, Sivaneasan Bala Krishnan, Sunita Gupta",
		"url": {
			"Wiley": "https://onlinelibrary.wiley.com/doi/abs/10.1002/9781394268832.ch21"
		},
		"date": "30-Sep-2024"
	},
	{
		"category": ["ai", "fundamentals"],
		"title": "Classification is a Strong Baseline for Deep Metric Learning",
		"authors": "Andrew Zhai, Hao-Yu Wu",
		"url": {
			"arxiv": "https://arxiv.org/abs/1811.12649"
		},
		"date": "30-Nov-2018"
	},
	{
		"category": ["ai", "bio"],
		"title": "Geometry-Aware Generative Autoencoders for Warped Riemannian Metric Learning and Generative Modeling on Data Manifolds",
		"authors": "Xingzhi Sun, Danqi Liao, Kincaid MacDonald, Yanlei Zhang, Chen Liu, Guillaume Huguet, Guy Wolf, Ian Adelstein, Tim G. J. Rudner, Smita Krishnaswamy",
		"url": {
			"arxiv": "https://arxiv.org/abs/2410.12779"
		},
		"date": "16-Oct-2024",
		"postfix": [
			"&nbsp;",
			"<div class=\"foldable-toggle\">abstract</div>",
			"<div class=\"foldable-content\">",
			"<p>",
			"Rapid growth of high-dimensional datasets in fields such as single-cell RNA sequencing and spatial genomics has led to unprecedented opportunities for scientific discovery, but it also presents unique computational and statistical challenges. Traditional methods struggle with geometry-aware data generation, interpolation along meaningful trajectories, and transporting populations via feasible paths. To address these issues, we introduce Geometry-Aware Generative Autoencoder (GAGA), a novel framework that combines extensible manifold learning with generative modeling. GAGA constructs a neural network embedding space that respects the intrinsic geometries discovered by manifold learning and learns a novel warped Riemannian metric on the data space. This warped metric is derived from both the points on the data manifold and negative samples off the manifold, allowing it to characterize a meaningful geometry across the entire latent space. Using this metric, GAGA can uniformly sample points on the manifold, generate points along geodesics, and interpolate between populations across the learned manifold using geodesic-guided flows. GAGA shows competitive performance in simulated and real-world datasets, including a 30% improvement over the state-of-the-art methods in single-cell population-level trajectory inference.",
			"</p>",
			"</div>"
		]
	},
	{
		"category": ["ai", "fundamentals"],
		"title": "How Classification Baseline Works for Deep Metric Learning: A Perspective of Metric Space",
		"authors": "Yuanqu Mou, Zhengxue Jian, Haiyang Bai, Chang Gou",
		"url": {
			"OpenReview.net": "https://openreview.net/forum?id=DVl5GAuBXA"
		},
		"date": "05-Sep-2024",
		"postfix": [
			"&nbsp;",
			"<div class=\"foldable-toggle\">abstract</div>",
			"<div class=\"foldable-content\">",
			"<p>",
			"Deep Metric Learning (DML) stands as a powerful technique utilized for training models to capture semantic similarities between data points across various domains, including computer vision, natural language processing, and recommendation systems. Current approaches in DML often prioritize the development of novel network structures or loss functions while overlooking metric properties and the intricate relationship between classification and metric learning. This oversight results in significant time overhead, particularly when the number of categories increases. To address this challenge, we propose extending the loss function used in classification to function as a metric, thereby imposing constraints on the distances between training samples based on the triangle inequality. This approach is akin to proxy-based methods and aims to enhance the efficiency of DML. Drawing inspiration from metrically convex metrics, we introduce the concept of a \"weak-metric\" to overcome the limitations associated with certain loss functions that cannot be straightforwardly extended to full metrics. This ensures the effectiveness of DML under various circumstances. Furthermore, we extend the Cross Entropy loss function to function as a weak-metric and introduce a novel metric loss derived from Cross Entropy for experimental comparisons with other methods. The results underscore the credibility and reliability of our proposal, showcasing its superiority over state-of-the-art techniques. Notably, our approach also exhibits significantly faster training times as the number of categories increases, making it a compelling choice for large-scale datasets.",
			"</p>",
			"</div>"
		]
	},
	{
		"category": ["ai", "nn-models"],
		"title": "Statistical Mechanics of Neural Networks",
		"authors": "Haim Sompolinsky",
		"url": {
			"Physics Today": "https://lnkd.in/gjRQWUb9"
		},
		"date": "01-Dec-1988",
		"postfix": [
			"(Haim Sompolinsky is a professor of physics at the Racah\nInstitute of Physics of the Hebrew University of Jerusalem.)"
		]

	},
	{
		"category": ["ai", "dl-models"],
		"title": "Generating Sentences from a Continuous Space",
		"date": "19-Nov-2015",
		"url": {
			"arxiv": "https://arxiv.org/abs/1511.06349"
		}
	},
	{
		"category": ["ai", "dl-models"],
		"title": "Improved Variational Autoencoders for Text Modeling using Dilated Convolutions",
		"date": "27-Feb-2017",
		"url": {
			"arxiv": "https://arxiv.org/abs/1702.08139"
		}
	},
	{
		"category": ["ai", "dl-models"],
		"title": "Robust flight navigation out of distribution with liquid neural networks",
		"date": "19-Apr-2023",
		"url": {
			"ScienceRobotics": "https://www.science.org/doi/10.1126/scirobotics.adc8892"
			}
	},
	{
		"category": ["ai", "llm"],
		"title": "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?",
		"date": "10-Oct-2023",
		"url": {
			"arxiv": "https://arxiv.org/abs/2310.06770"
		},
		"authors": "Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik Narasimhan",
		"org": "Princeton University, Princeton Language and Intelligence, University of Chicago",
		"postfix": [
			"&nbsp;",
			"<div class=\"foldable-toggle\">RAG pipeline for Chatbots</div>",
			"<div class=\"foldable-content\">",
			"<div class=\"fig-container\">",
			"<figure>",
			"<img src=\"/assets/images/ai/llm/rag-pipeline-for-chatbot-nvidia.png\">",
			"<figcaption>",
			"Control Points in a typical RAG pipeline when building Chatbots",
			"</figcaption>",
			"</figure>",
			"</div>",
			"</div>"
		]
	},
	{
		"id": "facts",
		"category": ["ai", "llm"],
		"title": "FACTS About Building Retrieval Augmented Generation-based Chatbots",
		"date": "10-Jul-2024",
		"url": {
			"arxiv": "https://arxiv.org/abs/2407.07858"
		},
		"authors": "Rama Akkiraju, Anbang Xu, Deepak Bora, Tan Yu, Lu An, Vishal Seth, Aaditya Shukla, Pritam Gundecha, Hridhay Mehta, Ashwin Jha, Prithvi Raj, Abhinav Balasubramanian, Murali Maram, Guru Muthusamy, Shivakesh Reddy Annepally, Sidney Knowles, Min Du, Nick Burnett, Sean Javiya, Ashok Marannan, Mamta Kumari, Surbhi Jha, Ethan Dereszenski, Anupam Chakraborty, Subhash Ranjan, Amina Terfai, Anoop Surya, Tracey Mercer, Vinodh Kumar Thanigachalam, Tamar Bar, Sanjana Krishnan, Samy Kilaru, Jasmine Jaksic, Nave Algarici, Jacob Liberman, Joey Conway, Sonu Nayyar, Justin Boitano",
		"org": "NVIDIA",
		"postfix": [
			"&nbsp;",
			"<div class=\"foldable-toggle\">RAG pipeline for Chatbots</div>",
			"<div class=\"foldable-content\">",
			"<div class=\"fig-container\">",
			"<figure>",
			"<img src=\"/assets/images/ai/llm/rag-pipeline-for-chatbot-nvidia.png\">",
			"<figcaption>",
			"Control Points in a typical RAG pipeline when building Chatbots",
			"</figcaption>",
			"</figure>",
			"</div>",
			"</div>"
		]
	},
	{
		"category": ["ai", "llm"],
		"title": "To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning arxiv",
		"date": "18-Sep-2024",
		"url": {
			"arxiv": "https://arxiv.org/abs/2409.12183"
		},
		"postfix": [
			"&nbsp;",
			"<div class=\"foldable-toggle\">abstract</div>",
			"<div class=\"foldable-content\">",
			"<p>",
			"Chain-of-thought (CoT) via prompting is the de facto method",
			" for eliciting reasoning capabilities from large language models (LLMs).",
			" But for what kinds of tasks is this extra \"thinking\" really helpful?",
			" To analyze this, we conducted a quantitative meta-analysis covering over 100 papers",
			" using CoT and ran our own evaluations of 20 datasets across 14 models.",
			" Our results show that CoT gives strong performance benefits primarily",
			" on tasks involving math or logic, with much smaller gains on other types of tasks.",
			" On MMLU, directly generating the answer without CoT leads",
			" to almost identical accuracy as CoT unless the question or model's response",
			" contains an equals sign, indicating symbolic operations and reasoning.",
			" Following this finding, we analyze the behavior of CoT on these problems",
			" by separating planning and execution and comparing against tool-augmented LLMs.",
			" Much of CoT's gain comes from improving symbolic execution,",
			" but it underperforms relative to using a symbolic solver.",
			" Our results indicate that CoT can be applied selectively,",
			" maintaining performance while saving inference costs.",
			" Furthermore, they suggest a need to move beyond prompt-based CoT to new paradigms",
			" that better leverage intermediate computation",
			" across the whole range of LLM applications.",
			"</p>",
			"</div>"
		]
	},
	{
		"category": ["ai", "llm"],
		"title": "OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs arxiv",
		"date": "08-Sep-2024",
		"url": {
			"arxiv": "https://arxiv.org/abs/2409.05152"
		}
	},
	{
		"category": ["ai", "llm"],
		"title": "Distilling System 2 into System 1",
		"authors": "Ping Yu, Jing Xu, Jason Weston, Ilia Kulikov",
		"org": "META Fair",
		"date": "24-Jul-2024",
		"url": {
			"arxiv": "https://arxiv.org/abs/2407.06023"
		}
	},
	{
		"category": ["ai", "llm"],
		"title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
		"authors": "Tu Vu, Kalpesh Krishna, Salaheddin Alzubi, Chris Tar, Manaal Faruqui, Yun-Hsuan Sung",
		"org": "Google, Google DeepMind &amp; UMass Amherst",
		"date": "15-Jul-2024",
		"url": {
			"arxiv": "https://arxiv.org/abs/2407.10817"
		}
	},
	{
		"category": ["ai", "slm"],
		"title": "Mistral AI Released Mistral-Small-Instruct-2409: A Game-Changing Open-Source Language Model Empowering Versatile AI Applications with Unmatched Efficiency and Accessibility",
		"date": "18-Sep-2024",
		"url": {
			"MarkTechPost": "https://www.marktechpost.com/2024/09/18/mistral-ai-released-mistral-small-instruct-2409-a-game-changing-open-source-language-model-empowering-versatile-ai-applications-with-unmatched-efficiency-and-accessibility/?amp",
			"model card": "https://huggingface.co/mistralai/Mistral-Small-Instruct-2409"
		}
	},
	{
		"category": ["ai", "cv"],
		"title": "Mask2Map: Vectorized HD Map Construction Using Bird's Eye View Segmentation Masks",
		"authors": "Sehwan Choi, Jungho Kim, Hongjae Shin, Jun Won Choi",
		"org": "Hanyang University &amp; SNU",
		"date": "18-Jul-2024"
	}
]