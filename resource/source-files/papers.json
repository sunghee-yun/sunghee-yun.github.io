[
	{
		"category": ["ai", "nn-models"],
		"title": "Statistical Mechanics of Neural Networks",
		"authors": "Haim Sompolinsky",
		"url": {
			"Physics Today": "https://lnkd.in/gjRQWUb9"
		},
		"date": "01-Dec-1988",
		"postfix": [
			"(Haim Sompolinsky is a professor of physics at the Racah\nInstitute of Physics of the Hebrew University of Jerusalem.)"
		]

	},
	{
		"category": ["ai", "dl-models"],
		"title": "Robust flight navigation out of distribution with liquid neural networks",
		"date": "19-Apr-2023",
		"url": {
			"ScienceRobotics": "https://www.science.org/doi/10.1126/scirobotics.adc8892"
			}
	},
	{
		"category": ["ai", "llm"],
		"title": "To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning arxiv",
		"date": "18-Sep-2024",
		"url": {
			"arxiv": "https://arxiv.org/abs/2409.12183"
		},
		"postfix": [
			"&nbsp;",
			"<div class=\"foldable-toggle\">abstract</div>",
			"<div class=\"foldable-content\">",
			"<p>",
			"Chain-of-thought (CoT) via prompting is the de facto method",
			" for eliciting reasoning capabilities from large language models (LLMs).",
			" But for what kinds of tasks is this extra \"thinking\" really helpful?",
			" To analyze this, we conducted a quantitative meta-analysis covering over 100 papers",
			" using CoT and ran our own evaluations of 20 datasets across 14 models.",
			" Our results show that CoT gives strong performance benefits primarily",
			" on tasks involving math or logic, with much smaller gains on other types of tasks.",
			" On MMLU, directly generating the answer without CoT leads",
			" to almost identical accuracy as CoT unless the question or model's response",
			" contains an equals sign, indicating symbolic operations and reasoning.",
			" Following this finding, we analyze the behavior of CoT on these problems",
			" by separating planning and execution and comparing against tool-augmented LLMs.",
			" Much of CoT's gain comes from improving symbolic execution,",
			" but it underperforms relative to using a symbolic solver.",
			" Our results indicate that CoT can be applied selectively,",
			" maintaining performance while saving inference costs.",
			" Furthermore, they suggest a need to move beyond prompt-based CoT to new paradigms",
			" that better leverage intermediate computation",
			" across the whole range of LLM applications.",
			"</p>",
			"</div>"
		]
	},
	{
		"category": ["ai", "llm"],
		"title": "OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs arxiv",
		"date": "08-Sep-2024",
		"url": {
			"arxiv": "https://arxiv.org/abs/2409.05152"
		}
	},
	{
		"category": ["ai", "llm"],
		"title": "Distilling System 2 into System 1",
		"authors": "Ping Yu, Jing Xu, Jason Weston, Ilia Kulikov",
		"org": "META Fair",
		"date": "24-Jul-2024",
		"url": {
			"arxiv": "https://arxiv.org/abs/2407.06023"
		}
	},
	{
		"category": ["ai", "llm"],
		"title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
		"authors": "Tu Vu, Kalpesh Krishna, Salaheddin Alzubi, Chris Tar, Manaal Faruqui, Yun-Hsuan Sung",
		"org": "Google, Google DeepMind &amp; UMass Amherst",
		"date": "15-Jul-2024",
		"url": {
			"arxiv": "https://arxiv.org/abs/2407.10817"
		}
	},
	{
		"category": ["ai", "slm"],
		"title": "Mistral AI Released Mistral-Small-Instruct-2409: A Game-Changing Open-Source Language Model Empowering Versatile AI Applications with Unmatched Efficiency and Accessibility",
		"date": "18-Sep-2024",
		"url": {
			"MarkTechPost": "https://www.marktechpost.com/2024/09/18/mistral-ai-released-mistral-small-instruct-2409-a-game-changing-open-source-language-model-empowering-versatile-ai-applications-with-unmatched-efficiency-and-accessibility/?amp",
			"model card": "https://huggingface.co/mistralai/Mistral-Small-Instruct-2409"
		}
	},
	{
		"category": ["ai", "cv"],
		"title": "Mask2Map: Vectorized HD Map Construction Using Bird's Eye View Segmentation Masks",
		"authors": "Sehwan Choi, Jungho Kim, Hongjae Shin, Jun Won Choi",
		"org": "Hanyang University &amp; SNU",
		"date": "18-Jul-2024"
	}
]