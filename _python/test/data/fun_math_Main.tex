\documentclass[17pt,landscape]{foils}

\newcommand\pdfpsfrag{no}
%\newcommand\reusepsfragpdf{no}
\newcommand\foiltex{yes}
%\newcommand\algfontmode{no}

\ifdefined\showincomplete
\else
	\newcommand\showincomplete{no}
\fi
\def\allslides{yes}
\newcommand{\isp}{\yesnoexec{\allslides}{/}{\item}}


\input{../../FrequentlyUsed/latex/mydefs}


%%% master options %%%
% for Beth
\def\trig{no}
\def\probstat{no}

\def\mathfontexamples{no}

\def\preamble{yes}
\def\stories{yes}

\ifthenelse{\boolean{true}}{%
%\ifthenelse{\boolean{false}}{%

\def\algebra{yes}
\def\aalgebra{yes}

\def\meastheory{yes}
\def\topspaces{yes}
\def\absmeas{yes}

\def\measprob{yes}
\def\cvxopt{yes}
\def\sproof{yes}
\def\ynindex{yes}

}{%
\def\preamble{no}
\def\trig{no}
\def\stories{no}

\def\algebra{no}
\def\aalgebra{yes}

\def\meastheory{no}
\def\topspaces{no}
\def\absmeas{no}

\def\measprob{no}
\def\cvxopt{no}
\def\sproof{yes}
\def\ynindex{yes}
}

\def\titlepostfix{}

\ifdefined\algebraonly
\def\allslides{no}

\def\titlepostfix{Algebra}

\def\trig{no}
\def\probstat{no}
\def\mathfontexamples{no}

\def\preamble{yes}
\def\stories{no}

\def\algebra{yes}
\def\aalgebra{no}

\def\meastheory{no}
\def\topspaces{no}
\def\absmeas{no}

\def\measprob{no}
\def\cvxopt{no}
\def\sproof{yes}
\def\ynindex{yes}
\def\showincomplete{no}
\fi

\ifdefined\aalgebraonly
\def\allslides{no}

\def\titlepostfix{Abstract Algebra}

\def\trig{no}
\def\probstat{no}
\def\mathfontexamples{no}

\def\preamble{yes}
\def\stories{no}

\def\algebra{no}
\def\aalgebra{yes}

\def\meastheory{no}
\def\topspaces{no}
\def\absmeas{no}

\def\measprob{no}
\def\cvxopt{no}
\def\sproof{yes}
\def\ynindex{yes}
\def\showincomplete{no}
\fi

\ifdefined\ranalysisonly
\def\allslides{no}

\def\titlepostfix{Real Analysis}

\def\trig{no}
\def\probstat{no}
\def\mathfontexamples{no}

\def\preamble{yes}
\def\stories{no}

\def\algebra{no}
\def\aalgebra{no}

\def\meastheory{yes}
\def\topspaces{yes}
\def\absmeas{yes}

\def\measprob{no}
\def\cvxopt{no}
\def\sproof{yes}
\def\ynindex{yes}
\def\showincomplete{no}
\fi

\ifdefined\mtheoryonly
\def\allslides{no}

\def\titlepostfix{Measure Theory}

\def\trig{no}
\def\probstat{no}
\def\mathfontexamples{no}

\def\preamble{yes}
\def\stories{no}

\def\algebra{no}
\def\aalgebra{no}

\def\meastheory{yes}
\def\topspaces{no}
\def\absmeas{no}

\def\measprob{no}
\def\cvxopt{no}
\def\sproof{yes}
\def\ynindex{yes}
\def\showincomplete{no}
\fi

\ifdefined\topspacesonly
\def\allslides{no}

\def\titlepostfix{Topological Spaces}

\def\trig{no}
\def\probstat{no}
\def\mathfontexamples{no}

\def\preamble{yes}
\def\stories{no}

\def\algebra{no}
\def\aalgebra{no}

\def\meastheory{no}
\def\topspaces{yes}
\def\absmeas{no}

\def\measprob{no}
\def\cvxopt{no}
\def\sproof{yes}
\def\ynindex{yes}
\def\showincomplete{no}
\fi

\ifdefined\absmeasonly
\def\allslides{no}

\def\titlepostfix{Abstract Measure Theory}

\def\trig{no}
\def\probstat{no}
\def\mathfontexamples{no}

\def\preamble{yes}
\def\stories{no}

\def\algebra{no}
\def\aalgebra{no}

\def\meastheory{no}
\def\topspaces{no}
\def\absmeas{yes}

\def\measprob{no}
\def\cvxopt{no}
\def\sproof{yes}
\def\ynindex{yes}
\def\showincomplete{no}
\fi

\ifdefined\measprobonly
\def\allslides{no}

\def\titlepostfix{Measure-theoretic Treatment of Statistics}

\def\trig{no}
\def\probstat{no}
\def\mathfontexamples{no}

\def\preamble{yes}
\def\stories{no}

\def\algebra{no}
\def\aalgebra{no}

\def\meastheory{no}
\def\topspaces{no}
\def\absmeas{no}

\def\measprob{yes}
\def\cvxopt{no}
\def\sproof{yes}
\def\ynindex{yes}
\def\showincomplete{no}
\fi

\ifdefined\cvxoptonly
\def\allslides{no}

\def\titlepostfix{Convex Optimization}

\def\trig{no}
\def\probstat{no}
\def\mathfontexamples{no}

\def\preamble{yes}
\def\stories{no}

\def\algebra{no}
\def\aalgebra{no}

\def\meastheory{no}
\def\topspaces{no}
\def\absmeas{no}

\def\measprob{no}
\def\cvxopt{yes}
\def\sproof{yes}
\def\ynindex{yes}
\def\showincomplete{no}
\fi

\def\ranalysis{no}
\yesnoexec{\meastheory}{
	\def\ranalysis{yes}
}{}
\yesnoexec{\topspaces}{
	\def\ranalysis{yes}
}{}
\yesnoexec{\absmeas}{
	\def\ranalysis{yes}
}{}


% count # sections for which proofs written

\newcounter{numsectionsforwhichproofexists}

\yesnoexec{\aalgebra}{\addtocounter{numsectionsforwhichproofexists}{1}}{}
\yesnoexec{\topspaces}{\addtocounter{numsectionsforwhichproofexists}{1}}{}
\yesnoexec{\measprob}{\addtocounter{numsectionsforwhichproofexists}{1}}{}
\yesnoexec{\cvxopt}{\addtocounter{numsectionsforwhichproofexists}{1}}{}


%%% PACKAGE IMPORTS %%

%\usepackage{libertine}
%\usepackage[T1]{fontenc}
%\usepackage[subscriptcorrection,slantedGreek,nofontinfo,mtpcal,mtpccal,mtpfrak,mtphrb]{mtpro2}
%\usepackage[mtpccal,mtpfrak]{mtpro2}
%\usepackage[mtpfrak]{mtpro2}
%\def\importmathabx{no}
%\yesnoexec{\importmathabx}{\usepackage{mathabx}}{}

\hypersetup{
	colorlinks = true,
	linkcolor = [rgb]{0, .5, 1.},
	linkbordercolor = {white},
}

\usepackage[normalem]{ulem}
\usepackage{mathtools}
\usepackage{algpseudocode}
%\usepackage{algpseudocodex}

%\usepackage{curves}

%%% NEW COMMANDS / DEFINITIONS / THEOREMS / ENVIRONMENTS %%%

\newcommand\bitem{\item [$\because$]}

% math formula related

\newcommand\normal{\mathcalfont{N}}

\newcommand\coll{\collk{C}}
\newcommand\collB{\collk{B}}
\newcommand\collF{\collk{F}}
\newcommand\collG{\collk{G}}
\newcommand\openconv{\collk{U}}

\newcommand{\alg}{\algk{A}}
\newcommand{\algA}{\algk{A}}
\newcommand{\algB}{\algk{B}}
\newcommand{\algC}{\algk{C}}
\newcommand{\algF}{\algk{F}}
\newcommand{\algR}{\algk{R}}
\newcommand{\algX}{\algk{X}}
\newcommand{\algY}{\algk{Y}}

\newcommand{\tXJ}{\topos{X}{J}}
\newcommand{\tJ}{\topol{J}}
\newcommand{\tS}{\topol{S}}

% page references

\newcommand\funpageref[1]{-~\pageref{#1}}

\newcommand\funhyperrefnotstarted[2]{\ifthenelse{\equal{#2}{}}{%
\textcolor{tiffanyblue}{#1}%
}{%
\textcolor{tiffanyblue}{#1}~\funpageref{#2}%
}}

\newcommand\funhyperref[2]{%
\hyperref[#2]{#1}~\funpageref{#2}%
}

\newcommand\tocpageref[1]{-~\pageref{#1}}

\newcommand\tochyperref[2]{%
\hyperref[#2]{#1}~\tocpageref{#2}%
}

% references

\newcommand\refertocounterpartmessage[2]{\vitem [] \hspace{-1.0em}(refer to page~\pageref{#2}\ for {#1} counterpart)}
\newcommand\refertocounterpart[2]{
\ifthenelse{%
	\equal{#1}{abstract}
	\or
	\equal{#1}{Lebesgue}
	\or
	\equal{#1}{normed spaces}
	\or
	\equal{#1}{complete measure spaces}
}{%
\refertocounterpartmessage{#1}{#2}
}{%
	\errmessage{{#1} should be either 'abstract' or 'Lebesgue' or 'normed spaces' or 'complete measure spaces'}
}%
}

% SLIDE LAYOUTS

\def\talkdate{\today}
\def\talktitle{\hyperref[page:toc]{Searching for Universal Truths}}

\MyLogo{\talktitle}
\rightfooter{\quad\textsf{\thepage}} % this is the default
\leftheader{Sunghee Yun}
\rightheader{\talkdate} % \foiltexdate is defined above

%% diagrams

\newcommand\diagtrirud[6]{%
\begin{array}{lcr}%
{#1}& \overset{#2}{\longrightarrow} &{#3}%
\\%
{}_{#6}\searrow&&\nearrow{}_{#4}%
\\%
&{#5}&%
\end{array}%
}

\newcommand\diagtriruu[6]{%
\begin{array}{lcr}%
{#1}& \overset{#2}{\longrightarrow} &{#3}%
\\%
{}_{#6}\nwarrow&&\nearrow{}_{#4}%
\\%
&{#5}&%
\end{array}%
}

\newcommand\diagsquare[8]{%
\begin{array}{rccl}%
{#1}&\overset{#2}{\longrightarrow}&{#3}\ \ \ \ %
\\%
{}_{#8}\uparrow&&\uparrow{}_{#4}%
\\%
{#7}&\underset{#6}{\longrightarrow}&{#5}\ \ \ \ %
\end{array}%
}

\begin{document}

\idxtodo{DONE - 2025 0414 - 1 - change mathematicians' names}
\idxtodo{1 - convert bullet points to proper theorem, definition, lemma, corollary, proposition, etc.}
\idxtodo{0 - apply new comma conventions}
\idxtodo{DONE - 2024 0324 - python script for converting slides to doc}
\idxtodo{DONE - 2024 0324 - python script extracting theorem-like list $\to$ using ``list of theorem'' functionality on doc}
\idxtodo{DONE - 2024 0324 - python script extracting figure list $\to$ using ``list of figures'' functionality on doc}
\idxtodo{CANCELED - $<$ 2024 0421 - python script extracting important list}
\idxtodo{DONE - 2024 0324 - change tocpageref and funpageref to hyperlink}


\myfoilhead{\LARGE \talktitle\notempty{\titlepostfix}{\\\LARGE\titlepostfix}{}}
\thispagestyle{empty}
\addtocounter{page}{-1}

\vfill
\vfill
\vfill

{\centering

{\large \bf Sunghee Yun}\\
sunghee.yun@gmail.com\\
%\vfill
%{VP - AI Technology \& Product Strategy\\ Erudio Bio, Inc.}
}
\vfill

\myfoilhead{Navigating Mathematical and Statistical Territories}
\pagelabel{page:toc}

\bit
\yesnoexec{\preamble}{
\item
	Notations \& definitions \& conventions

	\bit
	\item \tochyperref{notations}
		{page:Notations}
	\isp
%	\item
	\tochyperref{some definitions}
		{page:Some definitions}
	\isp
%	\item
	\tochyperref{some conventions}
		{page:Some conventions}
	\eit

\yesnoexec{\mathfontexamples}{%
	\item \tochyperref{Math font examples}
		{page:Math-font-examples}
}{}
}{}

\yesnoexec{\stories}{%
\item
	\funhyperref{Math stories}{super-title-page:Stories}
}{}

\yesnoexec{\algebra}{%
\item
	\funhyperref{Algebra}{super-title-page:algebra}

	\bit
	\item
		\funhyperref{inequalities}{title-page:Inequalities}
		\isp \funhyperref{number theory}{title-page:number-theory}%
		\yesnoexec{\trig}{%
			\isp \funhyperref{trigonometric functions}{title-page:Trigonometric-functions}%
		}{}%
		\yesnoexec{\showincomplete}{%
			\isp \funhyperrefnotstarted{linear algebra}{}
		}{}
	\eit
}{}

\yesnoexec{\aalgebra}{%
\item
	\funhyperref{Abstract algebra}{super-title-page:Abstract Algebra}

	\bit
	\item
		\funhyperref{groups}{title-page:Groups}
		\isp \funhyperref{rings}{title-page:Rings}
		\isp \funhyperref{polynomials}{title-page:Polynomials}
%		\funhyperref{polynomials \& group rings}{title-page:Polynomials and Group Rings},
%		\funhyperref{modules}{title-page:Modules}

	\item
		\funhyperref{algebraic extension}{title-page:Algebraic Extension}
		\isp \funhyperref{Galois theory}{title-page:Galois Theory}

\iffalse
	\item
		\funhyperrefnotstarted{modules \& vector spaces}{}
		\isp
		\funhyperrefnotstarted{field theory \& Galois theory}{}

	\item
		\funhyperrefnotstarted{commutative rings, algebraic geometry, homological algebra}{},
		\isp
		\funhyperrefnotstarted{representation theory of finite groups}{}
\fi
	\eit
}{}

\newcommand{\itemranalysis}{
\item
	\funhyperref{Real analysis}{super-title-page:Real-Analysis}
}
\newcommand{\subitemsmeastheory}{
	\item
		\funhyperref{Lebesgue measure}{title-page:lebesgue-measure}
		\isp \funhyperref{Lebesgue measurable functions}{title-page:measurable-functions}
		\isp \funhyperref{Lebesgue integral}{title-page:lebesgue-integral}%
		\yesnoexec{\showincomplete}{
		\isp \funhyperrefnotstarted{differentiation \& integration}{}
		}{}
}
\newcommand{\subitemstopspaces}{
	\item
		\funhyperref{space overview}{title-page:Space Overview}
		\isp \funhyperref{classical Banach spaces}{title-page:classical-banach-spaces}

	\item
		\funhyperref{metric spaces}{title-page:metric-spaces}
		\isp \funhyperref{topological spaces}{title-page:topological-spaces}
		\isp \funhyperref{compact and locally compact spaces}{title-page:Compact-and-Locally-Compact-Spaces}
		\isp \funhyperref{Banach spaces}{title-page:Banach-Spaces}
}
\newcommand{\subitemsgenmeas}{
	\item
		\funhyperref{measure and integration}{title-page:Measure-and-Integration}
		\isp \funhyperref{measure and outer measure}{title-page:Measure and Outer Measure}%
		\yesnoexec{\showincomplete}{
			\isp \funhyperrefnotstarted{measure and topology}{title-page:Measure and Topology}
		}{}
}

\yesnoexec{\ranalysis}{%

\itemranalysis
	\bit
	\item
		\funhyperref{set theory}{title-page:set-theory}
		\isp \funhyperref{real number system}{title-page:real-number-system}

	\yesnoexec{\meastheory}{
		\subitemsmeastheory
	}{}

	\yesnoexec{\topspaces}{
		\subitemstopspaces
	}{}

	\yesnoexec{\absmeas}{
		\subitemsgenmeas
	}{}
	\eit
}{}

\yesnoexec{\probstat}{%
\item
	\funhyperref{Probability \& statistics}{super-title-page:Probability}
	\bit
	\item
		\funhyperref{statistics for Beth}{title-page:Statistics for Beth}
	\eit
}{}

\yesnoexec{\measprob}{%
\item
	\funhyperref{Measure-theoretic treatment of probabilities}{super-title-page:Measure Theoretic Treatment of Probabilities}

	\bit
	\item
		\funhyperref{probability measure}{title-page:Probability Measure}
		\isp \funhyperref{random variables}{title-page:Random Variables}
		\isp \funhyperref{convergence of random variables}{title-page:Convergence of Random Variables}%
		\yesnoexec{\showincomplete}{
			\isp \funhyperrefnotstarted{conditional probability}{title-page:Conditional Probability}
			\isp \funhyperrefnotstarted{stochastic processes}{title-page:Stochastic Processes}
		}{}
	\eit
}{}

\yesnoexec{\cvxopt}{%
\item
	\funhyperref{Convex optimization}{super-title-page:Convex Optimization}
	\bit
	\item
		\funhyperref{convex sets}{title-page:Convex Sets}
		\isp \funhyperref{convex functions}{title-page:Convex Functions}
		\isp \funhyperref{convex optimization problems}{title-page:Convex Optimization Problems}

	\item
		\funhyperref{duality}{title-page:Duality}
		\isp \funhyperref{theorems of alternatives}{title-page:Theorems of Alternatives}
		\isp \funhyperref{convex optimization with generalized inequalities}{title-page:Convex Optimization with Generalized Inequalities}


	\item
		\funhyperref{unconstrained minimization}{title-page:Unconstrained Minimization}
		\isp \funhyperref{equality constrained minimization}{title-page:Equality Constrained Minimization}
		\isp \funhyperref{barrier interior-point methods}{title-page:Barrier Interior-point Methods}%
		\yesnoexec{\showincomplete}{
			\isp \funhyperrefnotstarted{barrier method for generalized inequalities}{title-page:Barrier Method for Generalized Inequalities}%
		}{}
		\isp \funhyperref{primal-dual interior-point methods}{title-page:Primal-dual Interior-point Methods}
	\eit
}{}

\yesnoexec{\showincomplete}{%
\item
	\funhyperrefnotstarted{numerical linear algebra}{}

\item
	\funhyperrefnotstarted{machine learning}{}
	\bit
	\item
		\funhyperrefnotstarted{supervised \& unsupervised}{}
		\isp \funhyperrefnotstarted{reinforcement learnings}{}
		\isp \funhyperrefnotstarted{deep learning}{}

	\item
		\funhyperrefnotstarted{statistical learning}{}
		\isp \funhyperrefnotstarted{Bayesian inference}{}
		\isp \funhyperrefnotstarted{graphical models}{}

	\item
		\funhyperrefnotstarted{computer vision}{}
		\isp \funhyperrefnotstarted{NLP}{}

	\item
		\funhyperrefnotstarted{LLM}{}
		\isp \funhyperrefnotstarted{genAI}{}
		\isp \funhyperrefnotstarted{multimodal AI}{}
	\eit

\item
	\funhyperrefnotstarted{optimization}{}
	\bit
	\item
		\funhyperrefnotstarted{linear \& nonlinear optimization}{}
	\item
		\funhyperrefnotstarted{NP-complete and NP-hard}{}
		\isp \funhyperrefnotstarted{combinatorial optimization}{}
	\eit
}{}

\item
	Proof \& references \& indices
	\bit
	\item
	\ifthenelse{\equal{\value{numsectionsforwhichproofexists}}{0} \OR \equal{\sproof}{no}}{}{%
	%\ifthenelse{\equal{\value{numsectionsforwhichproofexists}}{0}}{}{%
	%\ifthenelse{\equal{\sproof}{no}}{}{%
		\tochyperref{selected proofs}
			{super-title-page:Proofs}
		\isp
	}
		\tochyperref{references}
			{super-title-page:References}
		\yesnoexec{\ynindex}{
		\isp
		\tochyperref{index}
			{super-title-page:Index}
		}{}
	\eit
\eit


\yesnoexec{\preamble}{%
\myfoilhead{Notations}
\pagelabel{page:Notations}

\bit
\item
	sets of numbers
	\bit
	\item
		$\naturals$ - set of natural numbers%
			\index{natural number}%
			\index{number!natural number}
	\item
		$\integers$ - set of integers%
			\index{integer}%
			\index{number!integer}
	\item
		$\integers_+$ - set of nonnegative integers
	\item
		$\rationals$ - set of rational numbers%
			\index{rational number}%
			\index{number!rational number}
	\item
		$\reals$ - set of real numbers%
			\index{real number}%
			\index{number!real number}
	\item
		$\preals$ - set of nonnegative real numbers
	\item
		$\ppreals$ - set of positive real numbers
	\item
		$\complexes$ - set of complex numbers%
			\index{complex number}%
			\index{number!complex number}
	\eit

\item
	sequences $\seq{x_i}$ and the like%
		\index{sequence}
	\bit
	\item
		finite $\seq{x_i}_{i=1}^n$, infinite $\seq{x_i}_{i=1}^\infty$ - use $\seq{x_i}$ whenever unambiguously understood%
			\index{finite sequence}%
			\index{sequence!finite sequence}%
			\index{infinite sequence}%
			\index{sequence!infinite sequence}
	\item
		similarly for other operations, \eg, $\sum x_i$, $\prod x_i$, $\cup A_i$, $\cap A_i$, $\bigtimes A_i$
	\item
		similarly for integrals, \eg, $\int f$ for $\int_{-\infty}^\infty f$
	\eit

\item
	sets
	\bit
	\item
		$\compl{A}$ - complement of $A$%
			\index{complement!set}%
			\index{set!complement}
	\item
		$A\sim B$ - $A\cap \compl{B}$%
			\index{difference!set}%
			\index{set!difference}
	\item
		$A\Delta B$ - $(A\cap \compl{B}) \cup (\compl{A} \cap B)$
	\item
		$\powerset(A)$ - set of all subsets of $A$
	\eit

\item
	sets in metric vector spaces
	\bit
	\item
		\closure{A} - closure of set $A$%
			\index{closure!set}%
			\index{set!closure}
	\item
		\interior{A} - interior of set $A$%
			\index{interior!set}%
			\index{set!interior}
	\item
		$\relint A$ - relative interior of set $A$%
			\index{relative interior!set}%
			\index{set!relative interior}
	\item
		$\boundary A$ - boundary of set $A$%
			\index{boundary!set}%
			\index{set!boundary}
	\eit

\item
	set algebra
	\bit
	\item
		$\sigma(\subsetset{A})$ - $\sigma$-algebra generated by \subsetset{A},
			\ie, smallest $\sigma$-algebra containing \subsetset{A}%
				\index{smallest $\sigma$-algebra containing subsets}
	\eit

\item
	norms in $\reals^n$%
		\index{vector!norm}%
		\index{norm!vector}
	\bit
	\item
		$\|x\|_p$ ($p\geq1$) - $p$-norm of $x\in\reals^n$, \ie, $(|x_1|^p + \cdots + |x_n|^p)^{1/p}$
	\item
		\eg, $\|x\|_2$ - Euclidean norm
%		\eg, $\|x\|_2$ - Euclidean norm\index{Euclidean norm}
	\eit
%
%\item
%	balls
%	\bit
%	\item
%		$B_x(r)$ - ball of radius $r>0$ centered at $x$, \ie, \set{x}{\rho(y,x)< r\|}
%	\eit

\item
	matrices and vectors
	\bit
	\item $a_{i}$ - $i$-th entry of vector $a$
	\item $A_{ij}$ - entry of matrix $A$ at position $(i,j)$,
		\ie, entry in $i$-th row and $j$-th column
	\item $\Tr(A)$ - trace of $A \in\reals^{n\times n}$,
		\ie, $A_{1,1}+ \cdots + A_{n,n}$%
			\index{matrix!trace}%
			\index{trace!matrix}
	\eit

\item
	symmetric, positive definite, and positive semi-definite matrices
	\bit
	\item
		$\symset{n}\subset \reals^{n\times n}$ - set of symmetric matrices%
			\index{symmetric matrix}%
			\index{matrix!symmetric}

	\item
		$\possemidefset{n}\subset \symset{n}$ - set of positive semi-definite matrices;
		$A\succeq0 \Leftrightarrow A \in \possemidefset{n}$%
			\index{positive semi-definite matrix}%
			\index{matrix!positive semi-definite}

	\item
		$\posdefset{n}\subset \symset{n}$ - set of positive definite matrices;
		$A\succ0 \Leftrightarrow A \in \posdefset{n}$%
			\index{positive definite matrix}%
			\index{matrix!positive definite}
	\eit

\item
	sometimes,
	use Python script-like notations
	(with serious abuse of mathematical notations)
	\bit
	\item
		use $f:\reals\to\reals$ as if it were $f:\reals^n \to \reals^n$,
		\eg,
		$$
			\exp(x) = (\exp(x_1), \ldots, \exp(x_n)) \quad \mbox{for } x\in\reals^n
		$$
		and
		$$
			\log(x) = (\log(x_1), \ldots, \log(x_n)) \quad \mbox{for } x\in\ppreals^n
		$$
		which corresponds to Python code {\tt numpy.exp(x)} or {\tt numpy.log(x)}
		where {\tt x} is instance of {\tt numpy.ndarray}, \ie, {\tt numpy} array

	\item
		use $\sum x$ to mean $\ones^T x$ for $x\in\reals^n$,
		\ie\
		$$
			\sum x = x_1 + \cdots + x_n
		$$
		which corresponds to Python code {\tt x.sum()}
		where {\tt x} is {\tt numpy} array

	\item
		use $x/y$ for $x,y\in\reals^n$ to mean
		$$
			\rowvecthree{x_1/y_1}{\cdots}{x_n/y_n}^T
		$$
		which corresponds to Python code {\tt x / y}
		where {\tt x} and {\tt y} are $1$-d {\tt numpy} arrays

	\item
		use $X/Y$ for $X,Y\in\reals^{m\times n}$ to mean
		$$
			\begin{my-matrix}{cccc}
				X_{1,1}/Y_{1,1} & X_{1,2}/Y_{1,2} & \cdots & X_{1,n}/Y_{1,n}
				\\
				X_{2,1}/Y_{2,1} & X_{2,2}/Y_{2,2} & \cdots & X_{2,n}/Y_{2,n}
				\\
				\vdots & \vdots & \ddots & \vdots
				\\
				X_{m,1}/Y_{m,1} & X_{m,2}/Y_{m,2} & \cdots & X_{m,n}/Y_{m,n}
			\end{my-matrix}
		$$
		which corresponds to Python code {\tt X / Y}
		where {\tt X} and {\tt Y} are $2$-d {\tt numpy} arrays

	\eit

\eit
\vfill


\myfoilhead{Some definitions}%
	\pagelabel{page:Some definitions}

\begin{mydefinition}{infinitely often - i.o.}%
		\index{infinitely often}%
		\index{i.o.!infinitely often}
	statement $P_n$, said to happen \define{infinitely often} or \define{i.o.} if
	$$
		\left(
			\forall N\in\naturals
		\right)
		\left(
			\exists n > N
		\right)
		\left(
			P_n
		\right)
	$$
\end{mydefinition}

\vfill
\begin{mydefinition}{almost everywhere - a.e.}%
		\index{almost everywhere}%
		\index{a.e.!almost everywhere}%
		\index{almost surely}%
		\index{a.s.!almost surely}
	statement $P(x)$,
	said to happen \define{almost everywhere} or \define{a.e.} or \define{almost surely} or \define{a.s.}\
	(depending on context)
	associated with
	measure space \meas{X}{\algB}{\mu}\
	if
	$$
		\mu \set{x}{P(x)} = 1
	$$
	or equivalently
	$$
		\mu \set{x}{\sim P(x)} = 0
	$$
\end{mydefinition}

\vfill

\myfoilhead{Some conventions}%
	\pagelabel{page:Some conventions}

\bit
\item
	(for some subjects) use following conventions
	\bit
	\vitem
		$0\cdot \infty = \infty \cdot 0 = 0$
	\vitem
		$(\forall x\in\ppreals)(x\cdot \infty = \infty \cdot x = \infty)$
	\vitem
		$\infty \cdot \infty = \infty$
	\eit
\eit
\vfillfi


\yesnoexec{\mathfontexamples}{%
\myfoilhead{Math font examples}
\pagelabel{page:Math-font-examples}

\bit
	\item {\tt mathcal} - upper case
	\[
		\mathcal{A},
		\mathcal{B},
		\mathcal{C},
		\mathcal{D},
		\mathcal{E},
		\mathcal{F},
		\mathcal{G},
		\mathcal{H},
		\mathcal{I},
		\mathcal{J},
		\mathcal{K},
		\mathcal{L},
		\mathcal{M},
		\mathcal{N},
		\mathcal{O},
		\mathcal{P},
		\mathcal{Q},
		\mathcal{R},
		\mathcal{S},
		\mathcal{T},
		\mathcal{U},
		\mathcal{V},
		\mathcal{W},
		\mathcal{X},
		\mathcal{Y},
		\mathcal{Z}
		\]

	\item {\tt mathscr} - upper case
	\[
		\mathscr{A},
		\mathscr{B},
		\mathscr{C},
		\mathscr{D},
		\mathscr{E},
		\mathscr{F},
		\mathscr{G},
		\mathscr{H},
		\mathscr{I},
		\mathscr{J},
		\mathscr{K},
		\mathscr{L},
		\mathscr{M},
		\mathscr{N},
		\mathscr{O},
		\mathscr{P},
		\mathscr{Q},
		\mathscr{R},
		\mathscr{S},
		\mathscr{T},
		\mathscr{U},
		\mathscr{V},
		\mathscr{W},
		\mathscr{X},
		\mathscr{Y},
		\mathscr{Z}
		\]

	\item {\tt mathfrak} - upper case
	\[
		\mathfrak{A},
		\mathfrak{B},
		\mathfrak{C},
		\mathfrak{D},
		\mathfrak{E},
		\mathfrak{F},
		\mathfrak{G},
		\mathfrak{H},
		\mathfrak{I},
		\mathfrak{J},
		\mathfrak{K},
		\mathfrak{L},
		\mathfrak{M},
		\mathfrak{N},
		\mathfrak{O},
		\mathfrak{P},
		\mathfrak{Q},
		\mathfrak{R},
		\mathfrak{S},
		\mathfrak{T},
		\mathfrak{U},
		\mathfrak{V},
		\mathfrak{W},
		\mathfrak{X},
		\mathfrak{Y},
		\mathfrak{Z}
		\]

	\item {\tt mathfrak} - lower case
	\[
		\mathfrak{a},
		\mathfrak{b},
		\mathfrak{c},
		\mathfrak{d},
		\mathfrak{e},
		\mathfrak{f},
		\mathfrak{g},
		\mathfrak{h},
		\mathfrak{i},
		\mathfrak{j},
		\mathfrak{k},
		\mathfrak{l},
		\mathfrak{m},
		\mathfrak{n},
		\mathfrak{o},
		\mathfrak{p},
		\mathfrak{q},
		\mathfrak{r},
		\mathfrak{s},
		\mathfrak{t},
		\mathfrak{u},
		\mathfrak{v},
		\mathfrak{w},
		\mathfrak{x},
		\mathfrak{y},
		\mathfrak{z}
		\]

	\item {\tt mathcal} - lower case
	\[
		\mathcal{a},
		\mathcal{b},
		\mathcal{c},
		\mathcal{d},
		\mathcal{e},
		\mathcal{f},
		\mathcal{g},
		\mathcal{h},
		\mathcal{i},
		\mathcal{j},
		\mathcal{k},
		\mathcal{l},
		\mathcal{m},
		\mathcal{n},
		\mathcal{o},
		\mathcal{p},
		\mathcal{q},
		\mathcal{r},
		\mathcal{s},
		\mathcal{t},
		\mathcal{u},
		\mathcal{v},
		\mathcal{w},
		\mathcal{x},
		\mathcal{y},
		\mathcal{z}
		\]

	\item {\tt mathscr} - lower case
	\[
		\mathscr{a},
		\mathscr{b},
		\mathscr{c},
		\mathscr{d},
		\mathscr{e},
		\mathscr{f},
		\mathscr{g},
		\mathscr{h},
		\mathscr{i},
		\mathscr{j},
		\mathscr{k},
		\mathscr{l},
		\mathscr{m},
		\mathscr{n},
		\mathscr{o},
		\mathscr{p},
		\mathscr{q},
		\mathscr{r},
		\mathscr{s},
		\mathscr{t},
		\mathscr{u},
		\mathscr{v},
		\mathscr{w},
		\mathscr{x},
		\mathscr{y},
		\mathscr{z}
		\]

	\item {\tt mathcalfont} - upper case
	\[
		\mathcalfont{A},
		\mathcalfont{B},
		\mathcalfont{C},
		\mathcalfont{D},
		\mathcalfont{E},
		\mathcalfont{F},
		\mathcalfont{G},
		\mathcalfont{H},
		\mathcalfont{I},
		\mathcalfont{J},
		\mathcalfont{K},
		\mathcalfont{L},
		\mathcalfont{M},
		\mathcalfont{N},
		\mathcalfont{O},
		\mathcalfont{P},
		\mathcalfont{Q},
		\mathcalfont{R},
		\mathcalfont{S},
		\mathcalfont{T},
		\mathcalfont{U},
		\mathcalfont{V},
		\mathcalfont{W},
		\mathcalfont{X},
		\mathcalfont{Y},
		\mathcalfont{Z}
	\]

	\item {\tt mathalgfont} - upper case
	\[
		\mathalgfont{A},
		\mathalgfont{B},
		\mathalgfont{C},
		\mathalgfont{D},
		\mathalgfont{E},
		\mathalgfont{F},
		\mathalgfont{G},
		\mathalgfont{H},
		\mathalgfont{I},
		\mathalgfont{J},
		\mathalgfont{K},
		\mathalgfont{L},
		\mathalgfont{M},
		\mathalgfont{N},
		\mathalgfont{O},
		\mathalgfont{P},
		\mathalgfont{Q},
		\mathalgfont{R},
		\mathalgfont{S},
		\mathalgfont{T},
		\mathalgfont{U},
		\mathalgfont{V},
		\mathalgfont{W},
		\mathalgfont{X},
		\mathalgfont{Y},
		\mathalgfont{Z}
	\]
\eit
}{}

}{}


\yesnoexec{\stories}{%

\TITLEFOIL{Math Stories}{Stories}


\myfoilhead{Dualities}

\bit
\item
	duality
	\bit
	\vitem
		``very pervasive and important concept in (modern) mathematics''
	\vitem
		``important general theme having manifestations in almost every area of mathematics''
	\eit

\vitem dualities appear in many places in mathematics, \eg\
	\bit
	\vitem
		\emph{dual} of normed space
		is space of bounded linear functionals on the space
		(page~\pageref{page:Dual-of-normed-spaces})
	\vitem
		\emph{dual} cones and \emph{dual} norms are defined
		(\definitionname~\ref{definition:dual cones} \& \definitionname~\ref{definition:dual norms})

	\vitem
		can define \emph{dual} generalized inequalities using dual cones
		(\propositionname~\ref{proposition:generalized inequalities and dual generalized inequalities})

	\vitem
		can find necessary and sufficient conditions for $K$-convexity
		using \emph{dual} generalized inequalities
		(\propositionname~\ref{proposition:dual characterization of $K$-convexity})

	\vitem
		duality can be observed even in fundamental theorem for Galois theory,
		\ie, $G(K/E) \leftrightarrow E$ \& $H \leftrightarrow K^H$
		(\theoremname~\ref{theorem:fundamental theorem for Galois theory})

	\vitem
		exist \emph{dualities} in continuous / discrete functions in time domain
		and continuous / discrete functions in frequency domain,
		\ie,
		as in Fourier Transformation
	\eit
\eit

\myfoilhead{}

\bit
\item
	However,
	never fascinated more
	than \hyperref[title-page:Duality]{\cemph{duality appearing in optimization}},
	\eg,
	\bit
	\vitem
		properties such as weak duality (\definitionname~\ref{definition:weak duality})
		and strong duality (\definitionname~\ref{definition:strong duality})
	\vitem
		dual problem provides some bound for the optimal value of the original problem,
		hence certificate of suboptimality!
	\vitem
		constraint qualifications such as Slater's condition (\theoremname~\ref{theorem:Slater's theorem})
		guarantee strong duality!
	\eit
\eit
\vfill
}{}

\yesnoexec{\algebra}{
\TITLEFOIL{Algebra}{algebra}
\nocite{HLP:52}

\titlefoil{Inequalities}{Inequalities}

\myfoilhead{Jensen's inequality}

\bit
\item
	strictly convex function: for any $x\neq y$ and $0< \alpha <1$%
		\index{convex functions}%
		\index{convex functions!strictly}
		(\definitionname~\ref{definition:convex functions})
	\[
		\alpha f(x) + (1-\alpha) f(y) > f(\alpha x + (1-\alpha) y)
	\]

	\item convex function: for any $x, y$ and $0< \alpha <1$%
		\index{convex functions}
		(\definitionname~\ref{definition:convex functions})
	\[
		\alpha f(x) + (1-\alpha) f(y) \geq f(\alpha x + (1-\alpha) y)
	\]
%
%\vitem
%	\name{Jensen's inequality -}
%	for strictly convex function $f$ and \emph{distinct} $x_i$
%		and $0< \alpha_i <1$ with $\alpha_1 + \cdots = \alpha_n=1$%
%			\index{Jensen's inequality}%
%			\index{inequalities!Jensen's inequality}%
%			\index{Jensen, Johan Ludwig William Valdemar!inequality}
%	\[
%		\alpha_1 f(x_1) + \cdots + \alpha_n f(x_n) \geq f(\alpha_1 x_1 + \cdots + \alpha_n x_n)
%	\]
%	\bit
%	\item
%		equality holds \iaoi\ $x_1=\cdots=x_n$
%	\eit
\eit

\begin{myinequality}{Jensen's inequality - for finite sequences}%
		\index{Jensen's inequality!for finite sequences}%
		\index{Jensen, Johan Ludwig William Valdemar!Jensen's inequality!for finite sequences}
	for convex function $f$ and \emph{distinct} $x_i$
		and $0< \alpha_i <1$ with $\alpha_1 + \cdots = \alpha_n=1$%
			\index{Jensen's inequality}%
			\index{inequalities!Jensen's inequality}%
			\index{Jensen, Johan Ludwig William Valdemar!inequality}
	\[
		\alpha_1 f(x_1) + \cdots + \alpha_n f(x_n) \geq f(\alpha_1 x_1 + \cdots + \alpha_n x_n)
	\]
	\bit
	\item
		if $f$ is strictly convex, equality holds \iaoi\ $x_1=\cdots=x_n$
	\eit
\end{myinequality}


\myfoilhead{Jensen's inequality - for random variables}%

\bit
%\item
%	strictly convex function, $f$, and random variable, $X$
%
%\vitem
\item
	discrete random variable interpretation of Jensen's inequality in summation form - assume $\Prob(X=x_i) = \alpha_i$, then
	\[
		\Expect f(X)
		=
		\alpha_1 f(x_1) + \cdots + \alpha_n f(x_n)
		\geq
		f(\alpha_1 x_1 + \cdots + \alpha_n x_n)
		=
		f\left(\Expect X\right)
	\]

	\vitem true for any random variables $X$
\eit

\vfill
\begin{myinequality}{Jensen's inequality - for random variables}%
		\index{Jensen's inequality!for random variables}%
		\index{Jensen, Johan Ludwig William Valdemar!Jensen's inequality!for random variables}
	for random vector $X$ (page~\pageref{page:random-variables} for definition)

	\[
		\Expect f(X) \geq f(\Expect X)
	\]

	if probability density function (PDF) $p_X$ given,

	\[
		\int f(x) p_X(x) dx \geq f\left(\int x p_X(x) dx\right)
	\]
\end{myinequality}
\vfill


\myfoilhead{Proof for $n=3$}

\bit
	\item for any $x,y,z$ and $\alpha,\beta,\gamma>0$ with $\alpha + \beta + \gamma = 1$

	\begin{eqnarray*}
			\alpha f(x) + \beta f(y) + \gamma f(z)
		&=&
			(\alpha+\beta)\left(\frac{\alpha}{\alpha+\beta} f(x) + \frac{\beta}{\alpha + \beta} f(y)\right) + \gamma f(z)
		\\
		&\geq&
			(\alpha+\beta)f\left(\frac{\alpha}{\alpha+\beta} x + \frac{\beta}{\alpha + \beta} y\right) + \gamma f(z)
		\\
		&\geq&
			f\left((\alpha+\beta)\left(\frac{\alpha}{\alpha+\beta} x + \frac{\beta}{\alpha + \beta} y\right) + \gamma z \right)
		\\
		&=&
			f(\alpha x + \beta y + \gamma z )
	\end{eqnarray*}
\eit


\myfoilhead{Proof for all $n$}

\bit
\item
	use mathematical induction
	\bit
	\vitem
		assume that Jensen's inequality holds for $1\leq n\leq m$
	\vitem
		for distinct $x_i$ and $\alpha_i>0$ ($1\leq i\leq m+1$) with $\alpha_1 + \cdots + \alpha_{m+1} = 1$
		\begin{eqnarray*}
				\sum^{m+1}_{i=1} \alpha_i f(x_i)
			&=&
				\left(\sum^m_{j=1} \alpha_j\right) \sum^m_{i=1} \left(\frac{\alpha_i}{\sum^m_{j=1} \alpha_j} f(x_i)\right) + \alpha_{m+1} f(x_{m+1})
			\\
			&\geq&
				\left(\sum^m_{j=1} \alpha_j\right) f\left(\sum^m_{i=1} \left(\frac{\alpha_i}{\sum^m_{j=1} \alpha_j} x_i\right)\right) + \alpha_{m+1} f(x_{m+1})
			\\
			&=&
				\left(\sum^m_{j=1} \alpha_j\right) f\left(\frac{1}{\sum^m_{j=1} \alpha_j}\sum^m_{i=1} {\alpha_i}{} x_i\right) + \alpha_{m+1} f(x_{m+1})
			\\
			&\geq&
				f\left( \sum^m_{i=1} \alpha_i x_i + \alpha_{m+1} x_{m+1}\right)
			=
				f\left( \sum^{m+1}_{i=1} \alpha_i x_i \right)
		\end{eqnarray*}
	\eit
\eit


\myfoilhead{1st and 2nd order conditions for convexity}

\bit
	\item 1st order condition (assuming differentiable $f:\reals\to\reals$)
		- $f$ is strictly convex \iaoi\ for any $x\neq y$%
		\index{convex functions!first order condition}

	\[
		f(y) > f(x) + f'(x)(y-x)
	\]

	\vitem 2nd order condition (assuming twice-differentiable $f:\reals\to\reals$)%
		\index{convex functions!second order condition}
	\bit
		\vitem if $f''(x)>0$, $f$ is strictly convex
		\vitem $f$ is convex \iaoi\ for any $x$

			\[
				f''(x) \geq 0
			\]
	\eit
\eit
\vfill


\myfoilhead{Jensen's inequality examples}

\bit
	\item $f(x)=x^2$ is strictly convex
	\[
		\frac{a^2 + b^2}{2}
		\geq
		\left(\frac{a+b}{2}\right)^2
	\]

	\vitem $f(x)=x^4$ is strictly convex
	\[
		\frac{a^4 + b^4}{2}
		\geq
		\left(\frac{a+b}{2}\right)^4
	\]

	\vitem $f(x)=\exp(x)$ is strictly convex
	\[
		\frac{\exp(a) + \exp(b)}{2}
		\geq
		\exp\left(\frac{a+b}{2}\right)
	\]

	\vvitem equality holds \iaoi\ $a=b$ for all inequalities
\eit


\myfoilhead{1st and 2nd order conditions for convexity - vector version}

\bit
\vitem
	1st order condition (assuming differentiable $f:\reals^n\to\reals$)
		- $f$ is strict convex \iaoi\ for any $x,y$%
			\index{convex functions!first order condition!vector functions}

	\[
		f(y) > f(x) + \nabla f(x)^T (y-x)
	\]
	where $\nabla f(x) \in\reals^{n}$ with $\nabla f(x)_{i} = \partial f(x) / \partial x_i$

\vitem
	2nd order condition (assuming twice-differentiable $f:\reals^n\to\reals$)%
		\index{convex functions!second order condition!vector functions}
	\bit
	\vitem
		if $\nabla^2 f(x) \succ 0$, $f$ is strictly convex

	\vitem
		$f$ is convex \iaoi\ for any $x$

			\[
				\nabla^2 f(x)\succeq 0
			\]
	\eit

	where $\nabla^2 f(x) \in\reals^{n\times n}$
	is Hessian matrix of $f$ evaluated at $x$,
	\ie,
	$\nabla^2 f(x)_{i,j} = \partial^2 f(x) / \partial x_i \partial x_j$
\eit
\vfill


\myfoilhead{Jensen's inequality examples - vector version}

\bit
	\item assume $f:\reals^n\to\reals$

	\vitem $f(x)=\|x\|_2 = \sqrt{\sum x_i^2}$ is strictly convex
	\[
		(\|a\|_2 + 2\|b\|_2 )/3
		\geq
		\left\|(a+2b)/3\right\|_2
	\]
	\bit
		\item equality holds \iaoi\ $a=b\in\reals^n$
	\eit

	\vitem $f(x)=\|x\|_p = \left(\sum |x_i|^p\right)^{1/p}$ ($p>1$) is strictly convex
	\[
		\frac{1}{k}
		\left(\sum_{i=1}^k\|x^{(i)}\|_p \right)
		\geq
		\left\|\frac{1}{k}\sum_{i=1}^k x^{(i)}\right\|_p
	\]
	\bit
		\item equality holds \iaoi\ $x^{(1)}=\cdots=x^{(k)}\in\reals^n$
	\eit
\eit


\myfoilhead{AM $\geq$ GM}

\bit
\item
	for all $a,b>0$

	\[
		\frac{a + b}{2} \geq \sqrt{ab}
	\]
	\bit
	\item
		equality holds if and only if $a=b$
	\eit

\vitem
	below most general form holds
\eit

\begin{myinequality}{AM-GM inequality}
	for any $n$ $a_i>0$ and $\alpha_i>0$ with $\alpha_1+\cdots+\alpha_n=1$

	\[
		\alpha_1 a_1 + \cdots + \alpha_n a_n
		\geq
		{a_1^{\alpha_1} \cdots a_n^{\alpha_n}}
	\]

	where equality holds if and only if $a_1=\cdots=a_n$
\end{myinequality}

\bit
\item
	let's prove these incrementally
	(for rational $\alpha_i$)
\eit
\vfill


\myfoilhead{Proof of AM $\geq$ GM - simplest case}

\bit
	\item use fact that $x^2\geq0$ for any $x\in\reals$
	\vitem for any $a,b>0$
		\begin{eqnarray*}
		&&
			(\sqrt{a}-\sqrt{b})^2 \geq 0
		\\
		&\Leftrightarrow&
			a^2 - 2\sqrt{ab} + b^2 \geq 0
		\\
		&\Leftrightarrow&
			a + b \geq 2\sqrt{ab}
		\\
		&\Leftrightarrow&
			\frac{a + b}{2} \geq \sqrt{ab}
		\end{eqnarray*}
		\bit
			\item equality holds if and only if $a=b$
		\eit
\eit


\myfoilhead{Proof of AM $\geq$ GM - when $n=4$ and $n=8$}

\bit
	\item for any $a,b,c,d>0$
	\[
		\frac{a+b+c+d}{4}
		\geq
			\frac{2\sqrt{ab} + 2\sqrt{cd}}{4}
		=
			\frac{\sqrt{ab} + \sqrt{cd}}{2}
		\geq
			\sqrt{\sqrt{ab} \sqrt{cd}}
		=
			\sqrt[4]{abcd}
	\]
	\bit
		\item equality holds if and only if $a=b$ and $c=d$ and $ab=cd$
		if and only if $a=b=c=d$
	\eit

	\vitem likewise, for $a_1,\ldots,a_8>0$
	\begin{eqnarray*}
			\frac{a_1+\cdots+a_8}{8}
		&\geq&
			\frac{\sqrt{a_1a_2} + \sqrt{a_3a_4} + \sqrt{a_5a_6} + \sqrt{a_7a_8}}{4}
		\\
		&\geq&
			\sqrt[4]{\sqrt{a_1a_2} \sqrt{a_3a_4} \sqrt{a_5a_6} \sqrt{a_7a_8}}
		\\
		&=&
			\sqrt[8]{a_1\cdots a_8}
	\end{eqnarray*}
	\bit
		\item equality holds if and only if $a_1=\cdots=a_8$
	\eit
\eit


\myfoilhead{Proof of AM $\geq$ GM - when $n=2^m$}

\bit
	\item generalized to cases $n=2^m$
	\[
		\left(\sum_{a=1}^{2^m} a_i\right) / 2^m\geq \left({\prod_{a=1}^{2^m} a_i}\right)^{1/2^m}
	\]
	\bit
		\item equality holds if and only if $a_1=\cdots=a_{2^m}$
	\eit

	\vitem can be proved by \emph{mathematical induction}
\eit


\myfoilhead{Proof of AM $\geq$ GM - when $n=3$}

\bit
	\item proof for $n=3$
		\begin{eqnarray*}
		&&
			\frac{a+b+c}{3} = \frac{a + b + c + (a+b+c)/3}{4}
			\geq \sqrt[4]{abc(a+b+c)/3}
		\\
		&\Rightarrow&
			\left(\frac{a+b+c}{3}\right)^4 \geq {abc(a+b+c)/3}
		\\
		&\Leftrightarrow&
			\left(\frac{a+b+c}{3}\right)^3 \geq abc
		\\
		&\Leftrightarrow&
			\frac{a+b+c}{3} \geq \sqrt[3]{abc}
		\end{eqnarray*}
		\bit
			\item equality holds if and only if $a=b=c=(a+b+c)/3$ if and only if $a=b=c$
		\eit
\eit


\myfoilhead{Proof of AM $\geq$ GM - for all integers}

\bit
	\item for any integer $n\neq 2^m$
	\vitem for $m$ such that $2^m>n$
		\begin{eqnarray*}
		&&
			\frac{a_1+\cdots+a_n}{n} = \frac{a_1 + \cdots + a_n + (2^m-n) (a_1+\cdots+a_n) /n}{2^m}
		\\
		&&
			\geq
			\sqrt[2^m]{a_1\cdots a_n \cdot ((a_1 + \cdots + a_n)/n)^{2^m-n}}
		\\
		&\Leftrightarrow&
			\left(\frac{a_1+\cdots+a_n}{n}\right)^{2^m}
			\geq
			{a_1\cdots a_n \cdot \left(\frac{a_1 + \cdots + a_n}{n}\right)^{2^m-n}}
		\\
		&\Leftrightarrow&
			\left(\frac{a_1+\cdots+a_n}{n}\right)^{n}
			\geq
			{a_1\cdots a_n}
		\\
		&\Leftrightarrow&
			\frac{a_1+\cdots+a_n}{n}
			\geq
			\sqrt[n]{a_1\cdots a_n}
		\end{eqnarray*}
		\bit
			\item equality holds if and only if $a_1=\cdots=a_n$
		\eit
\eit


\myfoilhead{Proof of AM $\geq$ GM - rational $\alpha_i$}

\bit
\item
	given $n$ positive rational $\alpha_i$,
	we can find $n$ natural numbers $q_i$
	such that

	\[
		\alpha_i = \frac{q_i}{ N}
	\]
	where $q_1+\cdots+q_n=N$

\vitem
	for any $n$ positive $a_i\in\reals$ and positive $n$ $\alpha_i\in\rationals$ with $\alpha_1+\cdots+\alpha_n=1$

		\[
			\alpha_1 a_1 + \cdots + \alpha_n a_n
			= \frac{q_1 a_1 + \cdots + q_n a_n}{N}
			\geq \sqrt[N]{a_1^{q_1}\cdots a_n^{q_n}}
			= a_1^{\alpha_1}\cdots a_n^{\alpha_n}
		\]

	\bit
	\item
		equality holds if and only if $a_1=\cdots=a_n$
	\eit
\eit


\myfoilhead{Proof of AM $\geq$ GM - real $\alpha_i$}

\bit
\item
	exist $n$ rational sequences $\{ \beta_{i,1}, \beta_{i,2}, \ldots\}$ ($1\leq i\leq n$) such that
	\begin{eqnarray*}
		&&
			\beta_{1,j}+\cdots+\beta_{n,j}=1 \ \forall \ j\geq1
		\\
		&&
			\lim_{j\to\infty} \beta_{i,j} = \alpha_i \ \forall \ 1\leq i\leq n
	\end{eqnarray*}

\vitem
	for all $j$
	\[
		\beta_{1,j} a_1 + \cdots + \beta_{n,j} a_n
			\geq
		a_1^{\beta_{1,j}}\cdots a_n^{\beta_{n,j}}
	\]
	hence
		\begin{eqnarray*}
		&&
			\lim_{j\to\infty} \left(\beta_{1,j} a_1 + \cdots + \beta_{n,j} a_n \right)
				\geq
			\lim_{j\to\infty} a_1^{\beta_{1,j}}\cdots a_n^{\beta_{n,j}}
		\\
		&\Leftrightarrow&
			\alpha_1 a_1 + \cdots + \alpha_n a_n
				\geq
			a_1^{\alpha_1}\cdots a_n^{\alpha_n}
		\end{eqnarray*}
	\vitem \emph{cannot} prove equality condition from above proof method
\eit


\myfoilhead{Proof of AM $\geq$ GM using Jensen's inequality}

\bit
\item
	$(-\log)$ is strictly convex function because

	\[
		\frac{d^2}{dx^2} \left(-\log(x)\right)
		= \frac{d}{dx} \left(-\frac{1}{x} \right)
		= \frac{1}{x^2} > 0
	\]

\vitem
	Jensen's inequality implies for $a_i >0$, $\alpha_i >0$ with $\sum \alpha_i = 1$

	\begin{eqnarray*}
		-\log\left(\prod a_i^{\alpha_i}\right)
		= -\sum \log\left( a_i^{\alpha_i}\right)
		=
		\sum \alpha_i (-\log(a_i)) \geq -\log \left(\sum \alpha_i a_i\right)
	\end{eqnarray*}

\vitem
	$(-\log)$ strictly monotonically decreases, hence $\prod a_i^{\alpha_i} \leq \sum \alpha_i a_i$,
	having just proved

	\[
%		\sum \alpha_i a_i \geq \prod a_i^{\alpha_i}
		\alpha_1 a_1 + \cdots + \alpha_n a_n
			\geq
		a_1^{\alpha_1}\cdots a_n^{\alpha_n}
	\]

	where equality if and only if $a_i$ are equal
	(by Jensen's inequality's equality condition)
\eit


\myfoilhead{Cauchy-Schwarz inequality}

\begin{myinequality}{Cauchy-Schwarz inequality}
	for any $a_i, b_i\in\reals$
		\index{Cauchy-Schwarz inequality}%
		\index{Schwarz, Hermann!Cauchy-Schwarz inequality}%
		\index{Cauchy, Augustin-Louis!Cauchy-Schwarz inequality}%
		\index{Schwarz, Hermann!Cauchy-Schwarz inequality}
	\[
		( a_1^2 + \cdots + a_n^2 )
		( b_1^2 + \cdots + b_n^2 )
			\geq
		(a_1b_1 + \cdots + a_nb_n)^2
	\]
\end{myinequality}

\bit
	\vitem middle school proof
	\begin{eqnarray*}
		&&\sum (t a_i + b_i)^2 \geq 0 \ \forall\ t \in \reals
		\\
		&\Leftrightarrow&
		t^2 \sum a_i^2 + 2t \sum a_ib_i + \sum b_i^2 \geq 0 \ \forall\ t \in \reals
		\\
		&\Leftrightarrow&
		\Delta = \left(\sum a_ib_i \right)^2 - \sum a_i^2 \sum b_i^2 \leq 0
	\end{eqnarray*}
	\bit
		\item equality holds if and only if $\exists t\in\reals$, $t a_i + b_i=0$ for all $1\leq i\leq n$
	\eit
\eit


\myfoilhead{Cauchy-Schwarz inequality - another proof}

\bit
	\item $x\geq0$ for any $x\in\reals$, hence
		\begin{eqnarray*}
			&&
				\sum_i \sum_j (a_ib_j - a_jb_i)^2 \geq0
			\\
			&\Leftrightarrow&
				\sum_i \sum_j (a_i^2b_j^2 - 2a_ia_jb_ib_j + a_j^2b_i^2) \geq0
			\\
			&\Leftrightarrow&
				\sum_i \sum_j a_i^2b_j^2 + \sum_i \sum_j a_j^2b_i^2 -2 \sum_i \sum_j a_ia_jb_ib_j \geq 0
			\\
			&\Leftrightarrow&
				2 \sum_i a_i^2 \sum_j b_j^2 - 2 \sum_i a_ib_i \sum_j a_jb_j \geq 0
			\\
			&\Leftrightarrow&
				\sum_i a_i^2 \sum_j b_j^2 - \left(\sum_i a_ib_i\right)^2 \geq0
		\end{eqnarray*}
	\bit
		\item equality holds if and only if $a_ib_j=a_jb_i$ for all $1\leq i,j\leq n$
	\eit
\eit


\myfoilhead{Cauchy-Schwarz inequality - still another proof}

\bit
	\item for any $x,y\in\reals$ and $\alpha,\beta>0$ with $\alpha + \beta = 1$
	\begin{eqnarray*}
	&&
			(\alpha x - \beta y)^2
		=
			\alpha^2 x^2 + \beta^2 y^2 - 2\alpha \beta xy
	\\
	&&
		=
			\alpha(1-\beta) x^2 + (1-\alpha)\beta y^2 - 2\alpha \beta xy
		\geq
			0
	\\
	&\Leftrightarrow&
			\alpha x^2 + \beta y^2
		\geq
			\alpha \beta x^2 + \alpha \beta y^2 + 2\alpha \beta xy
			= \alpha \beta (x+y)^2
	\\
	&\Leftrightarrow&
			x^2 / \alpha + y^2 / \beta \geq (x+y)^2
	\end{eqnarray*}

	\item plug in $x=a_i$, $y=b_i$, $\alpha = A/(A+B)$, $\beta=B/(A+B)$
	where $A = \sqrt{\sum a_i^2}$, $B = \sqrt{\sum b_i^2}$
	\begin{eqnarray*}
	&&
		\sum (a_i^2 / \alpha + b_i^2 / \beta) \geq \sum (a_i+b_i)^2
	%\\
	\Leftrightarrow
		%\left(\sqrt{\sum a_i^2} + \sqrt{\sum b_i^2} \right)^2 \geq \sum a_i^2 +\sum b_i^2 + 2 \sum a_i b_i
		(A+B)^2 \geq A^2 + B^2 + 2 \sum a_i b_i
	\\
	&\Leftrightarrow&
		AB \geq \sum a_i b_i
	\Leftrightarrow
		A^2B^2 \geq \left(\sum a_i b_i\right)^2
	\Leftrightarrow
		{\sum a_i^2}{\sum b_i^2} \geq \left(\sum a_i b_i \right)^2
	\end{eqnarray*}
\eit



\myfoilhead{Cauchy-Schwarz inequality - proof using determinant}

\bit
	\item almost the same proof as first one - but using $2$-by-$2$ matrix determinant
	\begin{eqnarray*}
		&&\sum (x a_i + y b_i )^2 \geq 0 \ \forall\ x,y \in \reals
	\\
	&\Leftrightarrow&
		x^2 \sum a_i^2 + 2xy \sum a_ib_i + y^2\sum b_i^2 \geq 0 \ \forall \ x, y \in \reals
	\\
	&\Leftrightarrow&
		\begin{my-matrix}{cc}
			x & y
		\end{my-matrix}
		\begin{my-matrix}{cc}
			\sum a_i^2 & \sum a_ib_i
			\\
			\sum a_ib_i & \sum b_i^2
		\end{my-matrix}
		\begin{my-matrix}{c}
			x \\ y
		\end{my-matrix}
		\geq 0
		\ \forall \ x, y \in \reals
	\\
	\\
	&\Leftrightarrow&
		\left|
			\begin{array}{cc}
				\sum a_i^2 & \sum a_ib_i
				\\
				\sum a_ib_i & \sum b_i^2
			\end{array}
		\right|
		\geq 0
%	\\
%	&
	\Leftrightarrow
%	&
		\sum a_i^2 \sum b_i^2 - \left(\sum a_ib_i \right)^2 \geq0
	\end{eqnarray*}
	\bit
		\item equality holds \iaoi\
		$$
			\left(
				\exists x,y\in\reals \mbox{ with } xy\neq0
			\right)
			\left(
				xa_i + yb_i=0\ \
				\forall 1\leq i\leq n
			\right)
		$$
	\eit

	\vitem allows \eemph{beautiful generalization} of Cauchy-Schwarz inequality
\eit


\myfoilhead{Cauchy-Schwarz inequality - generalization}%
	\index{Cauchy-Schwarz inequality!generalization}%
	\index{Schwarz, Hermann!Cauchy-Schwarz inequality!generalization}%
	\index{Cauchy, Augustin-Louis!Cauchy-Schwarz inequality!generalization}
\pagelabel{page:Cauchy-Schwarz inequality - generalization}

\bit
\item
	want to say something like $\sum_{i=1}^n (x a_i + y b_i + z c_i + w d_i + \cdots)^2$
%	\bit
%	\item
%		run out of alphabets \ldots
%	\eit

	\vitem run out of alphabets \ldots\ - use double subscripts

	\begin{eqna}
%	\begin{eqnarray*}
	&&
		\sum_{i=1}^n (x_1 A_{1,i} + x_2 A_{2,i} + \cdots + x_m A_{m,i})^2 \geq 0 \ \forall\ x_i \in \reals
	\\
	&\Leftrightarrow&
		\sum_{i=1}^n (x^T a_i)^2
	=
		\sum_{i=1}^n x^T a_ia_i^T x
	=
		x^T \left(\sum_{i=1}^n a_ia_i^T\right) x \geq 0 \ \forall\ x \in \reals^m
	\\
	&\Leftrightarrow&
		\left|
		\begin{array}{cccc}
			\sum_{i=1}^n A_{1,i}^2 & \sum_{i=1}^n A_{1,i} A_{2,i} & \cdots & \sum_{i=1}^n A_{1,i} A_{m,i}
		\\
			\sum_{i=1}^n A_{1,i}A_{2,i} & \sum_{i=1}^n A_{2,i}^2 & \cdots & \sum_{i=1}^n A_{2,i} A_{m,i}
		\\
			\vdots & \vdots & \ddots & \vdots
		\\
			\sum_{i=1}^n A_{1,i}A_{m,i} & \sum_{i=1}^n A_{2,i}A_{m,i} & \cdots & \sum_{i=1}^n A_{m,i}^2
		\end{array}
		\right|
		\geq 0
%	\end{eqnarray*}
	\end{eqna}

	\vspace{1em}
	\bit
	\item []
		where $a_i = \begin{my-matrix}{ccc} A_{1,i} &\cdots & A_{m,i}\end{my-matrix}^T \in\reals^m$
	\vitem
		equality holds \iaoi\ $\exists x\neq0\in\reals^m$, $x^Ta_i =0$ for all $1\leq i\leq n$\
	\eit
\eit


\myfoilhead{Cauchy-Schwarz inequality - three series of variables}

\bit
\item
	let $m=3$
	\begin{eqnarray*}
	&&
		\begin{my-matrix}{ccc}
			\sum a_{i}^2 & \sum a_{i} b_{i} & \sum a_{i} c_{i}
		\\
			\sum a_{i}b_{i} & \sum b_{i}^2 & \sum b_{i} c_{i}
		\\
			\sum a_{i}c_{i} & \sum b_{i}c_{i} & \sum c_{i}^2
		\end{my-matrix}
		\succeq 0
	\\
	&\Rightarrow&
		\sum a_i^2 \sum b_i^2 \sum c_i^2 + 2 \sum a_ib_i \sum b_ic_i \sum c_ia_i
	\\
	&&
		\geq \sum a_i^2 \left(\sum b_i c_i\right)^2 + \sum b_i^2 \left(\sum a_i c_i\right)^2 + \sum c_i^2 \left(\sum a_i b_i\right)^2
	\end{eqnarray*}
	\bit
	\item
		equality holds if and only if $\exists x,y,z\in\reals$, $xa_i + yb_i + zc_i=0$ for all $1\leq i\leq n$
	\eit

\vitem
	questions for you
	\bit
	\vitem
		what does this mean?
	\vitem
		any real-world applications?
	\eit
\eit


\myfoilhead{Cauchy-Schwarz inequality - extensions}%
	\index{Cauchy-Schwarz inequality!extension}%
	\index{Schwarz, Hermann!Cauchy-Schwarz inequality!extension}%
	\index{Cauchy, Augustin-Louis!Cauchy-Schwarz inequality!extension}

\begin{myinequality}{Cauchy-Schwarz inequality - for complex numbers}%
	\index{Cauchy-Schwarz inequality!for complex numbers}%
	\index{Schwarz, Hermann!Cauchy-Schwarz inequality!for complex numbers}%
	\index{Cauchy, Augustin-Louis!Cauchy-Schwarz inequality!for complex numbers}
	for $a_i, b_i \in\complexes$
	\[
		\sum |a_i|^2 \sum |b_i|^2 \geq \left|\sum a_i b_i \right|^2
	\]
\end{myinequality}

\vspace{-.3cm}
\begin{myinequality}{Cauchy-Schwarz inequality - for infinite sequences}%
	\index{Cauchy-Schwarz inequality!for infinite sequences}%
	\index{Schwarz, Hermann!Cauchy-Schwarz inequality!for infinite sequences}%
	\index{Cauchy, Augustin-Louis!Cauchy-Schwarz inequality!for infinite sequences}
	for two complex infinite sequences
	$\seq{a_i}_{i=1}^\infty$
	and
	$\seq{b_i}_{i=1}^\infty$
	\[
		\sum^\infty_{i=1} |a_i|^2 \sum^\infty_{i=1} |b_i|^2 \geq \left|\sum^\infty_{i=1} a_i b_i \right|^2
	\]
\end{myinequality}

\vspace{-.3cm}
\begin{myinequality}{Cauchy-Schwarz inequality - for complex functions}%
	\index{Cauchy-Schwarz inequality!for complex functions}%
	\index{Schwarz, Hermann!Cauchy-Schwarz inequality!for complex functions}%
	\index{Cauchy, Augustin-Louis!Cauchy-Schwarz inequality!for complex functions}
	for two complex functions $f,g:[0,1] \to \complexes$
	\[
		\int |f|^2 \int |g|^2 \geq \left|\int f g \right|^2
	\]
\end{myinequality}

\bit
	\vitem note that \eemph{all these can be further generalized
	as in page~\pageref{page:Cauchy-Schwarz inequality - generalization}}
\eit


\titlefoil{Number Theory - Queen of Mathematics}{number-theory}

\myfoilhead{Integers}

\bit
\item
	integers ($\integers$)
	-
		$\ldots -2, -1, 0, 1, 2, \ldots$

	\bit
	\vitem
		first defined by Bertrand Russell

	\vitem
		algebraic structure - commutative ring
		\bit
		\vitem [-]
			addition, multiplication defined, but divison \emph{not} defined
		\vitem [-]
			addition, multiplication are associative
		\vitem [-]
			multiplication distributive over addition
		\vitem [-]
			addition, multiplication are commutative
		\eit
	\eit

\vitem
	natural numbers ($\naturals$)
	\bit
	\vitem
		$1, 2, \ldots$
	\eit
\eit


\myfoilhead{Division and prime numbers}

\bit
	\item divisors for $n\in\naturals$
	\[
		\set{d\in\naturals}{ d \mbox{ divides } n}
	\]

	\vitem prime numbers
	\bit
		\item $p$ is primes if $1$ and $p$ are only divisors
	\eit
\eit


\labelfoilhead{Fundamental theorem of arithmetic}

\begin{mytheorem}{fundamental theorem of arithmetic}%
	\index{fundamental theorem of arithmetic}%
	\index{fundamental theorem!of arithmetic}
	integer $n\geq2$ can be factored uniquely into products of primes,
	\ie,
	exist distinct primes, $p_1$, \ldots, $p_k$, and $e_1,\ldots, e_k\in\naturals$
	such that
	$$
		n = p_1^{e_1} p_2^{e_2} \cdots p_k^{e_k}
	$$
\end{mytheorem}

\bit
\vitem
	hence,
	integers are \emph{factorial ring}
	(\definitionname~\ref{definition:factorial ring})
\eit
\vfill


\myfoilhead{Elementary quantities}

\bit
\item
	greatest common divisor (gcd) (of $a$ and $b$)%
		\index{greatest common divisor}%
		\index{greatest common divisor!integers}
	\[
		\gcd(a,b) = \max \set{d}{d\mbox{ divides both }a \mbox{ and } b}
	\]
	\bit
	\vitem
		for definition of gcd
		for general entire rings,
		refer to \definitionname~\ref{definition:greatest common divisor}
	\eit

\vitem
	least common multiple (lcm) (of $a$ and $b$)%
		\index{least common multiple}%
		\index{least common multiple!integers}
	\[
		\mbox{lcm}(a,b) = \min \set{m}{\mbox{both } a \mbox{ and } b \mbox{ divides }m}
	\]

\vitem
	$a$ and $b$ coprime, relatively prime, mutually prime $\Leftrightarrow$ $\gcd(a,b)=1$
\eit
\vfill


\myfoilhead{Are there infinite number of prime numbers?}

\bit
\item
	yes!
\vitem
	proof
	\bit
	\vitem
		assume there only exist finite number of prime numbers, \eg, $p_1 < p_2 < \cdots <p_n$\
	\vitem
		but then, $p_1 \cdot p_2 \cdots p_n + 1$ is prime,
		but which is greater than $p_n$, hence contradiction
	\eit
\eit
\vfill


\myfoilhead{Integers modulo $n$}

\begin{mydefinition}{modulo}
	when $n$ divides $a-b$,
	$a$, said to be \define{equivalent to} $b$ \define{modulo $n$},
	denoted by
	$$
		a \equiv b \Mod{n}
	$$
	read as \define{``$a$ congruent to $b$ mod $n$''}
\end{mydefinition}

\bit
\vitem
	$a\equiv b\Mod{n}$ and $c\equiv d\Mod{n}$ imply
	\bit
	\vitem
		$a+c\equiv b+d \Mod{n}$
	\vitem
		$ac\equiv bd \Mod{n}$
	\eit
\eit

\begin{mydefinition}{congruence class}%
	\index{congruence class}%
	\index{integers!congruence class}%
	\index{congruence class!integers}%
	\index{residue class under modulo}%
	\index{integers!residue class under modulo}%
	\index{residue class under modulo!integers}

	classes determined by modulo relation,
	called \define{congruence} or \define{residue class under modulo}
\end{mydefinition}

\begin{mydefinition}{integers modulo n}%
	\index{integers modulo $n$}%
	\index{integers mod $n$}%
	\index{integers!integers modulo $n$}%
	\index{integers!integers mod $n$}
	set of equivalence classes under modulo,
	denoted by \define{$\integers/n \integers$},
	called \define{integers modulo $n$} or \define{integers mod $n$}
\end{mydefinition}


\myfoilhead{Euler's theorem}

\begin{mydefinition}{Euler's totient function}%
		\index{Euler, Leonhard!Euler's totient function}%
		\index{Euler, Leonhard!phi-function}%
		\index{Euler phi-function}%
		\index{Euler's totient function}
	for $n\in\naturals$,
	$$
		\varphi(n)
		= (p_1-1)p_1^{e_1-1} \cdots (p_k-1)p_k^{e_k-1}
		= n \prod_{\mathrm{prime}\ p\ \mathrm{dividing}\ n} (1-1/p)
	$$
	called \define{Euler's totient function},
	also called \define{Euler $\varphi$-function}%
		\index{Euler $\varphi$-function}%
		\index{Euler, Leonhard!$\varphi$-function}
\end{mydefinition}
\bit
	\item \eg, $\varphi(12) = \varphi(2^2\cdot 3^1) = 1\cdot2^1\cdot 2\cdot3^0 = 4$,
		$\varphi(10) = \varphi(2^1\cdot5^1) = 1\cdot2^0\cdot 4\cdot 5^0 =4$
\eit

\begin{mytheorem}{Euler's theorem - number theory}%
	\index{Euler's theorem}%
	\index{Euler, Leonhard!Euler's theorem}
	for coprime $n$ and $a$
	$$
		a^{\varphi(n)} \equiv 1 \Mod{n}
	$$
\end{mytheorem}
\bit
\item
	\eg, $5^4 \equiv 1 \Mod{12}$ whereas $4^4 \equiv 4 \neq 1 \Mod{12}$
%
%\vitem
%	proof not (extremely) hard, but beyond scope of presentation
\eit

\bit
\vitem
	\eemph{Euler's theorem underlies RSA cryptosystem, which is pervasively used in internet communication}
\eit


\yesnoexec{\showincomplete}{%
\myfoilhead{Number theory \& cryptography - XXX}

\bit
	\item at the core of cryptography lies number theory, \emph{classical} topic of mathematics!
	\item XXX\idxtodo{1 - number theory \& cryptography}
\eit
}{}


\yesnoexec{\trig}{%
\titlefoil{Trigonometric functions}{Trigonometric-functions}

\myfoilhead{Trigonometric functions}

\bit
\item
	real functions relating angle of right-angled triangle to ratios of two side lengths,
	called \define{trigonometric functions}%
		\index{trigonometric functions}
	\bit
	\item
		also called circular functions, angle functions, or goniometric functions%
			\index{trigonometric functions!circular functions}%
			\index{trigonometric functions!angle functions}%
			\index{trigonometric functions!goniometric functions}
	\eit

\vitem
	widely used in sciences related to
		geometry, such as
			navigation, solid mechanics, celestial mechanics, geodesy, \etc\

\vitem
	trigonometric functions most widely used in modern mathematics are
	\bit
	\item
		sine, cosine, tangent functions - denoted by $\sin$, $\cos$, $\tan$ respectively
	\item
		reciprocals - cosecant, secant, cotangent - denoted by $\csc$, $\sec$, $\cot$ respectively
	\eit
\eit


\myfoilhead{Definitions of trigonometric functions}

\bit
\item basic definitions (\figref{definition of trigonometric functions})\
		\index{trigonometric functions!sine}%
		\index{trigonometric functions!cosine}%
		\index{trigonometric functions!tangent}%
		\index{trigonometric functions!cosecant}%
		\index{trigonometric functions!secant}%
		\index{trigonometric functions!cotangent}
%	\begin{eqnarray*}
	\[
	\begin{array}{lll}
%		&
		\sin \theta = a/c = \mbox{opposite}/\mbox{hypotenuse},
		&
		\csc \theta = c/a = 1/\sin \theta
	\\
%		&
		\cos \theta = b/c = \mbox{adjacent}/\mbox{hypotenuse},
		&
		\sec \theta = c/b = 1/\cos \theta
	\\
%		&
		\tan \theta = a/b = \mbox{opposite}/\mbox{adjacent} = \sin \theta / \cos \theta,
		&
		\cot \theta = b/a = 1/\tan \theta
	\end{array}
	\]
%	\end{eqnarray*}
\eit
\vfill

\begin{figure}
\begin{center}
	\mypsfrag{a}{$a$ - opposite}
	\mypsfrag{b}{$\begin{array}{c}b\\\mbox{adjacent}\end{array}$}
	\mypsfrag{c}{hypotenuse - $c$}
	\mypsfrag{A}{$A$}
	\mypsfrag{B}{$B$}
	\mypsfrag{C}{$C$}
	\mypsfrag{theta}{$\theta$}
	\includegraphics[width=.4\textwidth]{figures/right-angled-triangle}
	\idxfig{definition of trigonometric functions}
	\label{fig:definition of trigonometric functions}
\end{center}
\end{figure}


\labelfoilhead{Pythagorean theorem and trigonometric functions}

\bit
\item
	Pythagorean theorem implies $a^2 + b^2 = c^2$, thus $(a/c)^2 + (b/c)^2 = 1$
	(\figref{triangle for pythagorean theorem}),
	hence
	$$
		\sin^2 \theta + \cos^2 \theta = 1
	\Leftrightarrow
		\tan^2 \theta + 1 = \sec^2 \theta
	\Leftrightarrow
		1 + \cot^2 \theta = \csc^2 \theta
	$$
\eit
\vfill

\begin{figure}
\begin{center}
	\mypsfrag{a}{$a$ - opposite}
	\mypsfrag{b}{$\begin{array}{c}b\\\mbox{adjacent}\end{array}$}
	\mypsfrag{c}{hypotenuse - $c$}
	\mypsfrag{A}{$A$}
	\mypsfrag{B}{$B$}
	\mypsfrag{C}{$C$}
	\mypsfrag{theta}{$\theta$}
	\includegraphics[width=.4\textwidth]{figures/right-angled-triangle}
	\idxfig{triangle for pythagorean theorem}
	\label{fig:triangle for pythagorean theorem}
\end{center}
\end{figure}


\myfoilhead{Unit for angles - radians vs degrees}

\bit
\item
	for trigonometric functions, most times use \emph{radians} for units of angles instead of \emph{degrees}

\vitem
	why use radians? - cuz can write following (clean) formula, \eg,
	\bit
	\item
		{length of arc having central angle} $\theta$ { (in radius)} = $\theta r$
	\item
		{area of sector having central angle} $\theta$ { (in radius)} = $\theta r^2 /2$
	\eit

\vitem
	both proportional to each other - very easy to convert
	$$
		d^\circ = \frac{d}{180} \pi \mbox{ radians}
	\Leftrightarrow
		\theta \mbox{ radians} = \left( \frac{180}{\pi} \theta \right)^\circ
	$$

\vitem
	\eg\ (drop ``radians'' when writing)
	\bit
	\item
		$0 = 0^\circ$,
		$\pi/12 = 15^\circ$,
		$2\pi/12 = 30^\circ$,
		$3\pi/12 = 45^\circ$,
		$4\pi/12 = 60^\circ$,
	\item
		$5\pi/12 = 75^\circ$,
		$6\pi/12 = 90^\circ$
		$7\pi/12 = 105^\circ$,
		$8\pi/12 = 120^\circ$,
	\item
		$9\pi/12 = 135^\circ$,
		$10\pi/12 = 150^\circ$,
		$11\pi/12 = 165^\circ$,
		$12\pi/12 = 180^\circ$,
	\eit


\eit


\myfoilhead{Some values of sine function}

\bit
\item
	derive from \figref{triangles for special values of sin}
	\begin{eqnarray*}
		\sin 0 &=& \sin 0^\circ = 0
		\\
		\sin ({\pi}/{6}) &=& \sin 30^\circ = 1/2
		\\
		\sin ({\pi}/{4}) &=& \sin 45^\circ = 1/\sqrt{2}
		\\
		\sin ({\pi}/{3}) &=& \sin 60^\circ = \sqrt{3}/{2}
		\\
		\sin ({\pi}/{2}) &=& \sin 90^\circ = 1
	\end{eqnarray*}
\eit

\begin{figure}
\begin{center}
	\mypsfrag{1}{$1$}
	\mypsfrag{2}{$2$}
	\mypsfrag{sqrt 2}{$\sqrt{2}$}
	\mypsfrag{sqrt 3}{$\sqrt{3}$}
	\mypsfrag{30}{$\pi/6 = 30^\circ$}
	\mypsfrag{45}{$\pi/4 = 45^\circ$}
	\mypsfrag{60}{$\pi/3 = 60^\circ$}
	\includegraphics[width=.9\textwidth]{figures/right-angled-triangle-values}
	\idxfig{triangles for special values of sin}
	\label{fig:triangles for special values of sin}
\end{center}
\end{figure}


\myfoilhead{Some values of cosine function}

\bit
\item
	derive from \figref{triangles for special values of cos}
	\begin{eqnarray*}
		\cos 0 &=& \cos 0^\circ = 1
		\\
		\cos ({\pi}/{6}) &=& \cos 30^\circ = \sqrt{3}/2
		\\
		\cos ({\pi}/{4}) &=& \cos 45^\circ = 1/\sqrt{2}
		\\
		\cos ({\pi}/{3}) &=& \cos 60^\circ = {1}/{2}
		\\
		\cos ({\pi}/{2}) &=& \cos 90^\circ = 0
	\end{eqnarray*}
\eit

\begin{figure}
\begin{center}
	\mypsfrag{1}{$1$}
	\mypsfrag{2}{$2$}
	\mypsfrag{sqrt 2}{$\sqrt{2}$}
	\mypsfrag{sqrt 3}{$\sqrt{3}$}
	\mypsfrag{30}{$\pi/6 = 30^\circ$}
	\mypsfrag{45}{$\pi/4 = 45^\circ$}
	\mypsfrag{60}{$\pi/3 = 60^\circ$}
	\includegraphics[width=.9\textwidth]{figures/right-angled-triangle-values}
	\idxfig{triangles for special values of cos}
	\label{fig:triangles for special values of cos}
\end{center}
\end{figure}


\myfoilhead{More values of trigonometric functions}

\bit
\item
	below are values of trigonometric functions
	for some angles
	\[
	\begin{array}{|c|c|c|c|c|}
		\hline
		\mbox{degrees} & \mbox{radians} & \sin\theta & \cos\theta & \tan\theta
		\\
		\hline
		0 & 0^\circ & 0 & 1 & 0
		\\
	%	\hline
		\pi/12 & 15^\circ & (\sqrt{6} - \sqrt{2})/4 & (\sqrt{6}+\sqrt{2})/4 & 2-\sqrt{3}
		\\
	%	\hline
		\pi/6 & 30^\circ & 1/2 & \sqrt{3}/2 & 1/\sqrt{3}
		\\
	%	\hline
		\pi/4 & 45^\circ & 1/\sqrt{2} & 1/\sqrt{2} & 1
		\\
	%	\hline
		\pi/3 & 60^\circ & \sqrt{3}/2 & 1/2 & \sqrt{3}
		\\
	%	\hline
		5\pi/12 & 75^\circ & (\sqrt{6} + \sqrt{2})/4 & (\sqrt{6}-\sqrt{2})/4 & 2+\sqrt{3}
		\\
	%	\hline
		\pi/2 & 90^\circ & 1 & 0 & \pm \infty\mbox{ (cannot be defined)
		}
		\\
		\hline
	\end{array}
	\]

\vitem
	who can we get values for $\pi/12$ and $5\pi/12$?
	\bit
	\item
		do not worry; we have not learned how to get it yet - but we will!
		\bit
		\item [-] I don't know when, though - depends on Kim :)
		\eit
	\eit
\eit
\vfill


%\myfoilhead{Trigonometric functions as coordiates of points on unit circle}
%
%\ntwocolss{.45\textwidth}{.50\textwidth}{figures/py-tri-basic-psfragable}{trim=0 0 0 0, clip, width=.80\textwidth}{.5in}
%{%
%\mypsfrag{xy}{$(\cos \theta, \sin\theta)$}
%\mypsfrag{theta}{$\theta$}
%\mypsfrag{cos theta}{$\cos\theta$}
%\mypsfrag{sin theta}{$\sin\theta$}
%\mypsfrag{01}{$(0,1)$}
%\mypsfrag{0-1}{$(0,-1)$}
%\mypsfrag{10}{$(1,0)$}
%\mypsfrag{-10}{$(-1,0)$}
%}
%{%
%\bit
%\item
%	consider point on unit circle whose angle with (positive direction) of $x$-axis is $\theta$
%	\bit
%	\item
%		$x$-coordinate of the point is $\cos \theta$
%	\item
%		$y$-coordinate of the point is $\sin \theta$
%	\eit
%
%\vitem
%	following this rule,
%	enables us to calculate trigonometric function values for every angle;
%	even those greater than $\pi/2=90^\circ$ or even negative angles
%
%\vitem
%	in next slides, will derive several formula using this
%
%\eit
%\vfillfi
%}
%\idxfig{trigonometric function values as Cartesian coordinates of points on unit circle}
%
%
\myfoilhead{Trigonometric functions as coordiates of points on unit circle}

\bit
\item
	consider point on unit circle whose angle with (positive direction) of $x$-axis is $\theta$\
		(\figref{trigonometric function values as Cartesian coordinates of points on unit circle})
	\bit
	\item
		$x$-coordinate of the point is $\cos \theta$
	\item
		$y$-coordinate of the point is $\sin \theta$
	\eit

\vitem
	following this rule,
	enables us to calculate trigonometric function values for every angle;
	even those greater than $\pi/2=90^\circ$ or even negative angles

\vitem
	in next slides, will derive several conversion formula using this
\eit

\begin{figure}
\begin{center}
	\mypsfrag{xy}{$(\cos \theta, \sin\theta)$}
	\mypsfrag{theta}{$\theta$}
	\mypsfrag{cos theta}{$\cos\theta$}
	\mypsfrag{sin theta}{$\sin\theta$}
	\mypsfrag{01}{$(0,1)$}
	\mypsfrag{0-1}{$(0,-1)$}
	\mypsfrag{10}{$(1,0)$}
	\mypsfrag{-10}{$(-1,0)$}
	\includegraphics[width=.40\textwidth]{figures/py-tri-basic-psfragable}%
		\idxfig{trigonometric function values as Cartesian coordinates of points on unit circle}%
		\label{fig:trigonometric function values as Cartesian coordinates of points on unit circle}
\end{center}
\end{figure}


\myfoilhead{Conversion formula for $\cos(\pi-\theta)$, $\sin(\pi-\theta)$, $\tan(\pi-\theta)$}

\bit
\item
	derive following formula from \figref{trigonometric function conversion formula for pi minus thera}
	\bit
	\item
		$\cos(\pi-\theta) = - \cos\theta$
	\item
		$\sin(\pi-\theta) = \sin\theta$
	\item
		$\tan(\pi-\theta) = \sin(\pi-\theta)/\cos(\pi-\theta) = -\tan \theta$
	\eit
\eit

\begin{figure}
\begin{center}
	\mypsfrag{xy}{$(\cos \theta, \sin\theta)$}
	\mypsfrag{xy1}{$(\cos (\pi-\theta), \sin(\pi-\theta))$}
	\mypsfrag{theta}{$\theta$}
	\mypsfrag{cos theta}{$\cos\theta$}
	\mypsfrag{cos theta0}{$(\cos\theta,0)$}
	\mypsfrag{-cos theta0}{$(-\cos\theta,0)$}
	\mypsfrag{0sin theta}{$(0, \sin\theta)$}
	\mypsfrag{sin theta}{$\sin\theta$}
	\mypsfrag{pi-theta}{$\pi-\theta$}
	\mypsfrag{01}{$(0,1)$}
	\mypsfrag{0-1}{$(0,-1)$}
	\mypsfrag{10}{$(1,0)$}
	\mypsfrag{-10}{$(-1,0)$}
	\includegraphics[width=0.65\textwidth]{figures/py-pi-minus-theta-psfragable}
		\idxfig{trigonometric function conversion formula for $\pi - \theta$}
		\label{fig:trigonometric function conversion formula for pi minus thera}
\end{center}
\end{figure}


\myfoilhead{Conversion formula for $\cos(-\theta)$, $\sin(-\theta)$, $\tan(-\theta)$}

\bit
\item
	derive following formula from \figref{trigonometric function conversion formula for minus theta}\
	\bit
	\item
		$\cos(-\theta) = \cos\theta$
	\item
		$\sin(-\theta) = -\sin\theta$
	\item
		$\tan(-\theta) = \sin(-\theta)/\cos(-\theta) = -\tan \theta$
	\eit
\eit

\begin{figure}
\begin{center}
	\mypsfrag{xy}{$(\cos \theta, \sin\theta)$}
	\mypsfrag{xy1}{$(\cos (-\theta), \sin(-\theta))$}
	\mypsfrag{theta}{$\theta$}
	\mypsfrag{-theta}{$-\theta$}
	\mypsfrag{cos theta}{$\cos\theta$}
	\mypsfrag{sin theta}{$\sin\theta$}
	\mypsfrag{pi-theta}{$\pi-\theta$}
	\mypsfrag{01}{$(0,1)$}
	\mypsfrag{0-1}{$(0,-1)$}
	\mypsfrag{10}{$(1,0)$}
	\mypsfrag{-10}{$(-1,0)$}
	\includegraphics[width=0.4\textwidth]{figures/py-minus-theta-psfragable}%
		\idxfig{trigonometric function conversion formula for $- \theta$}%
		\label{fig:trigonometric function conversion formula for minus theta}
\end{center}
\end{figure}


\myfoilhead{Conversion formula for $\cos(\theta+\pi/2)$, $\sin(\theta+\pi/2)$, $\tan(\theta+\pi/2)$}

\bit
\item
	derive following formula from \figref{tri conversion rules for theta plus pi over 2}
	\bit
	\item
		$\cos(\theta+\pi/2) = -\sin\theta$
	\item
		$\sin(\theta+\pi/2) = \cos\theta$
	\item
		$\tan(\theta+\pi/2) = \sin(\theta+\pi/2)/\cos(\theta+\pi/2) = -\cos\theta/\sin\theta= -\cot \theta$
	\eit
\eit

\begin{figure}
\begin{center}
	\mypsfrag{xy}{$(\cos \theta, \sin\theta)$}
	\mypsfrag{xy1}{$(\cos(\theta+\pi/2),\sin(\theta+\pi/2))$}
	\mypsfrag{theta}{$\theta$}
	\mypsfrag{theta2}{$\theta+\pi/2$}
	\mypsfrag{0cos theta}{$(0, \cos \theta)$}
	\mypsfrag{-sin theta0}{$(-\sin \theta,0)$}
	\mypsfrag{0sin theta}{$(0, \sin \theta)$}
	\mypsfrag{cos theta0}{$(\cos \theta,0)$}
	\includegraphics[width=0.55\textwidth]{figures/py-theta-plus-half-pi-psfragable}
		\idxfig{trigonometric function conversion formula for $\theta+\pi/2$}
		\label{fig:tri conversion rules for theta plus pi over 2}
\end{center}
\end{figure}


\iffalse
\myfoilhead{Conversion formula for trigonometric functions}

\bit
\item
	can consider many other cases from similar drawings

\vitem
	below table shows some formula

	\[
	\begin{array}{|c|r|r|r|}
		\hline
		\xi & \sin\xi & \cos\xi & \tan\xi
		\\
		\hline
		\theta +\pi/2& \cos\theta & -\sin\theta & -\cot\theta
		\\
		\theta + \pi & -\sin\theta & -\cos\theta & \tan\theta
		\\
		\theta + 3\pi/2 & -\cos\theta & \sin\theta & -\cot\theta
		\\
		\theta + 2\pi & \sin\theta & \cos\theta & \tan\theta
		\\
		-\theta & -\sin\theta & \cos\theta & -\tan\theta
		\\
		\pi/2-\theta & \cos\theta & \sin\theta & \cot\theta
		\\
		\pi-\theta & \sin\theta & -\cos\theta & -\tan\theta
		\\
		3\pi/2-\theta & -\cos\theta & -\sin\theta & \cot\theta
		\\
		2\pi-\theta & -\sin\theta & \cos\theta & -\tan\theta
		\\
		\hline
	\end{array}
	\]

\vitem
	\eemph{actually you can derive every formula using only these three:
		$\sin(-\theta)=-\sin(\theta)$, $\sin(\theta + \pi/2) = \cos(\theta)$, $\sin(\theta+\pi)=-\sin(\theta)$; try to figure out how!}
\eit


\myfoilhead{Evaluation using conversion formula - 1}

\bit
\item
	let's try to calculate trigonometric function values for various angles
	given four values below only - can we?
	\[
		\sin 0 = 0,
		\ \
		\sin(\pi/6) = 1/2,
		\ \
		\sin(\pi/4) = 1/\sqrt{2},
		\ \
		\sin(\pi/3) = \sqrt{3}/2,
	\]

\vitem
	examples
	\[
		\begin{array}{cccl}
			\cos (30^\circ) &=& \cos (\pi/6) &= \cos(\pi/2 - \pi/3) = \sin(\pi/3) = \sqrt{3}/2
			\\
			\tan (30^\circ) &=& \tan (\pi/6) &= \sin(\pi/6) / \cos(\pi/6) = 1/ \sqrt{3}
			\\
			\\
			\cos (60^\circ) &=& \cos (\pi/3) &= \cos(\pi/2 - \pi/6) = \sin(\pi/6) = {1}/2
			\\
			\tan (60^\circ) &=& \tan (\pi/3) &= \sin(\pi/3) / \cos(\pi/3) = \sqrt{3}
			\\
			\\
			\sin (120^\circ) &=& \sin(2\pi/3) &= \sin(\pi - \pi/3) = \sin(\pi/3) = \sqrt{3}/2
			\\
			\cos (120^\circ) &=& \cos(2\pi/3) &= \cos(\pi - \pi/3) = -\cos(\pi/3) = -1/2
			\\
			\tan (120^\circ) &=& \tan (2\pi/3) &= \sin(2\pi/3) / \cos(2\pi/3) = -\sqrt{3}
		\end{array}
	\]
\eit


\myfoilhead{Evaluation using conversion formula - 2}

\bit
\item
	more examples
	\[
		\begin{array}{cccl}
			\sin(-45^\circ ) &=&\sin(-\pi/4) &= - \sin(\pi/4) = - 1/\sqrt{2}
			\\
			\cos(-45^\circ ) &=&\cos(-\pi/4) &= \cos(\pi/4) = 1/\sqrt{2}
			\\
			\tan(-45^\circ) &=& \tan(-\pi/4) &= \sin(-\pi/4) / \cos(-\pi/4) = -1
			\\
			\\
			\sin(210^\circ ) &=&\sin(7\pi/6) &= \sin(\pi/6 + \pi) = -\sin(\pi/6) = - 1/{2}
			\\
			\cos(210^\circ ) &=&\cos(7\pi/6) &= \cos(\pi/6 + \pi) = -\cos(\pi/6) = - \sqrt{3}/{2}
			\\
			\tan(210^\circ) &=& \tan(7\pi/6) &= \sin(7\pi/6) / \cos(7\pi/6) = 1/\sqrt{3}
			\\
			\\
			\sin(315^\circ ) &=&\sin(7\pi/4) &= \sin(2\pi - \pi/4) = -\sin(\pi/4) = - 1/\sqrt{2}
			\\
			\cos(315^\circ ) &=&\cos(7\pi/4) &= \cos(2\pi - \pi/4) = \cos(\pi/4) = 1/\sqrt{2}
			\\
			\tan(315^\circ) &=& \tan(7\pi/4) &= \sin(7\pi/4) / \cos(7\pi/4) = -1
		\end{array}
	\]
\eit


\myfoilhead{Figuring out conversion formula using graphs without memorizing them}

\bit
\item
	\bit
	\item
	\eit

\vitem
	\bit
	\item
	\eit
\eit


\myfoilhead{Law of sines}

\begin{mylaw}{law of sines}
	$$
		\frac{a}{\sin A}
			=
		\frac{b}{\sin B}
			=
		\frac{c}{\sin C}
	$$
\end{mylaw}

\vfill
\begin{figure}
\begin{center}
	\mypsfrag{A}{$A$}
	\mypsfrag{B}{$B$}
	\mypsfrag{C}{$C$}
	\mypsfrag{a}{$a$}
	\mypsfrag{b}{$b$}
	\mypsfrag{c}{$c$}
	\includegraphics[width=.30\textwidth]{figures/py-triangle-psfragable}%
		\idxfig{triangle}
		\label{fig:triangle}
\end{center}
\end{figure}

\vfill
\begin{proof}
	From \figref{triangle}, the height of triangle when $a$ is chosen as base of the triangle is $c\sin B = b\sin C$,
	hence $b/\sin B = c/\sin C$.
	Applying this to another side gives us $a/\sin A = b/\sin B = c/\sin C$.
	\eemph{Please make sure yourself whether this is true for obtuse triangles, too!}
\end{proof}


\myfoilhead{Law of cosines}

\begin{mylaw}{law of cosines}
	\begin{eqnarray*}
		a^2 &=& b^2 + c^2 - 2bc \cos A
		\\
		b^2 &=& c^2 + a^2 - 2ca \cos B
		\\
		c^2 &=& a^2 + b^2 - 2ab \cos C
	\end{eqnarray*}
\end{mylaw}

\vfill
\begin{proof}
	From \figref{triangle}, applying Pythagorean theorem to side, $b$,
	yields
	\begin{eqnarray*}
		b^2
			& = &
		(c\sin B)^2 + (a-c\cos B)^2
		\\
			&=&
		c^2 (\sin^2 B + \cos^2 B) - 2ac \cos B + a^2
			=
		a^2 + c^2 - 2ac \cos B
	\end{eqnarray*}
	where $\sin^2 B + \cos^2 B=1$ shown \foilref{Pythagorean theorem and trigonometric functions}\ is used.
	Similar equations arise by choosing the side of length $a$ or the side of length $c$
	to apply Pythagorean theorem.
	\eemph{Please make sure yourself whether this is true for obtuse triangles, too!}
\end{proof}


\myfoilhead{Quiz for Beth - triangulation}

\bit
\item
	suppose you want to know the distance between the target point, $X$, and line, $\overline{AB}$ (\figref{triangulation}).
	however you are on the other side of the river,
	which is so wide that you cannot cross.
	but you can measure two angles $\alpha$ and $\beta$ and distance, $d$, between $A$ and $B$.
\vitem
	do you think you can calculate (exactly) distance between $X$ and $\overline{AB}$,
	\ie, length of dotted line in \figref{triangulation}?
	if so, how?
	(hint: would ``law of sines'' be helpful?)
\eit

\begin{figure}
\begin{center}
	\mypsfrag{a}{$d$}
	\mypsfrag{b}{}
	\mypsfrag{c}{}
	\mypsfrag{A}{$X$ (target)}
	\mypsfrag{B}{$A$}
	\mypsfrag{C}{$B$}
	\mypsfrag{alpha}{$\alpha$}
	\mypsfrag{beta}{$\beta$}
	\includegraphics[width=.5\textwidth]{figures/py-triangulation-psfragable}%
		\idxfig{triangulation}
		\label{fig:triangulation}
\end{center}
\end{figure}


\myfoilhead{Trigonometric identities}

\begin{myformula}{trigonometric identities}
	\begin{eqnarray*}
		\sin(\alpha + \beta) &=& \sin(\alpha)\cos(\beta) + \cos(\alpha)\sin(\beta)
	\\
		\cos(\alpha + \beta) &=& \cos(\alpha)\cos(\beta) - \sin(\alpha)\sin(\beta)
	\\
		\tan(\alpha + \beta) &=& (\tan\alpha + \tan \beta) / (1-\tan\alpha\tan\beta)
	\\
		\sin 2\theta
	&=&
					2\sin\theta\cos\theta
	\\
		\cos 2\theta
	&=&
					\cos^2\theta- \sin^2\theta = 2\cos^2\theta - 1 = 1-2\sin^2\theta
	\\
		\tan2\theta &=& 2\tan\theta / (1-\tan^2\theta)
	\\
		\cos \alpha + \cos \beta
	&=&
					2\cos \left(\frac{\alpha + \beta}{2}\right) \cos \left(\frac{\alpha - \beta}{2}\right)
	\\
		\sin \alpha + \sin \beta
	&=&
					2\sin \left(\frac{\alpha + \beta}{2}\right) \cos \left(\frac{\alpha - \beta}{2}\right)
	\end{eqnarray*}
\end{myformula}

-- note all other formula can be drived from first one
\fi
}{%
}
}{}


\yesnoexec{\aalgebra}{
\TITLEFOIL{Abstract Algebra}{Abstract Algebra}
\nocite{Lang:99}
\nocite{DF:99}

\newcommand\fieldextensions[1]{%
\setlength{\unitlength}{#1}%
\begin{picture}(1,4)%
\thicklines
	\put(.5,0){\makebox(0,0)[c]{$k$}}%
		\put(.5,.5){\line(0,1){1}}%
	\put(.5,2){\makebox(0,0)[c]{$F$}}%
		\put(.5,2.5){\line(0,1){1}}%
	\put(.5,4){\makebox(0,0)[c]{$E$}}%
\end{picture}
}

\newcommand\plifting[1]{%
\setlength{\unitlength}{#1}%
\begin{picture}(2,3)%
\thicklines
  \put(1.05,.3){\line(1,2){.9}}%
	  \put(1,.1){\makebox(0,0)[ct]{$k$}}%
	  \put(2.1,2.2){\makebox(0,0)[lc]{$F$}}%
  \put(.9,.25){\line(-2,1){.8}}%
	  \put(-.1,.7){\makebox(0,0)[rc]{$E$}}%
  \put(.05,.8){\line(1,2){.9}}%
  \put(1.9,2.25){\line(-2,1){.8}}%
	  \put(1,2.8){\makebox(0,0)[cb]{$EF$}}%
\end{picture}%
}

\newcommand\pliftingsmallest[1]{%
\setlength{\unitlength}{#1}%
\begin{picture}(2,3)%
\thicklines
  \put(1.05,.3){\line(1,2){.9}}%
	  \put(1,.1){\makebox(0,0)[t]{$k$}}%
	  \put(2.1,2.2){\makebox(0,0)[l]{$F$}}%
  \put(.9,.25){\line(-2,1){.8}}%
	  \put(-.1,.7){\makebox(0,0)[r]{$E=k(\alpha_1, \ldots, \alpha_n)$}}%
  \put(.05,.8){\line(1,2){.9}}%
  \put(1.9,2.25){\line(-2,1){.8}}%
	  \put(1,2.8){\makebox(0,0)[b]{$EF = F(\alpha_1,\ldots,\alpha_n)$}}%
\end{picture}%
}

\newcommand\galoisf[1]{%
\setlength{\unitlength}{#1}%
\begin{picture}(3,3)%
\thicklines
  \put(1.4,1.2){\line(2,1){1.2}}%
  \put(.4,2.2){\line(2,1){1.2}}%
  \put(.8,1.2){\line(-1,1){.6}}%
  \put(2.8,2.2){\line(-1,1){.6}}%
  \put(1,.2){\line(0,1){.6}}%
	  \put(1,0){\makebox(0,0){$k$}}%
	  \put(1,1){\makebox(0,0){$K\cap F$}}%
	  \put(0,2){\makebox(0,0){$K$}}%
	  \put(.4,1.4){\makebox(0,0)[r]{Galois}}%
	  \put(3,2){\makebox(0,0){$F$}}%
	  \put(2,3){\makebox(0,0){$KF$}}%
	  \put(2.6,2.6){\makebox(0,0)[l]{Galois}}%
\end{picture}%
}

\newcommand\galoiss[1]{%
\setlength{\unitlength}{#1}%
\begin{picture}(2,3)%
\thicklines
  \put(1,.2){\line(0,1){.6}}%
  \put(1.2,1.2){\line(1,1){.6}}%
  \put(.8,1.2){\line(-1,1){.6}}%
  \put(.2,2.2){\line(1,1){.6}}%
  \put(1.8,2.2){\line(-1,1){.6}}%
	  \put(1,0){\makebox(0,0){$k$}}%
	  \put(1,1){\makebox(0,0){$K_1\cap K_2$}}%
	  \put(0,2){\makebox(0,0){$K_1$}}%
	  \put(2,2){\makebox(0,0){$K_2$}}%
	  \put(1,3){\makebox(0,0){$K_1K_2$}}%
\end{picture}%
}

\newcommand\embeddingextensionf[1]{%
\setlength{\unitlength}{#1}%
\begin{picture}(1.2,1.2)%
\thicklines
	\put(.3,.1){\vector(1,0){.6}}%
	\put(.1,.3){\vector(0,1){.6}}%
	\put(1.1,.3){\vector(0,1){.6}}%
	\put(.3,1.1){\vector(1,0){.6}}%
%
	\put(.1,.1){\makebox(0,0){$F$}}%
	\put(1.1,.1){\makebox(0,0){$L$}}%
	\put(.1,1.1){\makebox(0,0){$E$}}%
	\put(1.1,1.1){\makebox(0,0){$L$}}%
%
	\put(.6,0){\makebox(0,0)[t]{$\sigma$}}%
	\put(0,.6){\makebox(0,0)[r]{inc}}%
	\put(1.2,.6){\makebox(0,0)[l]{id}}%
	\put(.6,1.2){\makebox(0,0)[b]{$\tau$}}%
\end{picture}%
}

\newcommand\embeddingextensions[1]{%
\setlength{\unitlength}{#1}%
\begin{picture}(1,1.2)%
\thicklines
	\put(.2,1.1){\vector(1,0){.6}}%
	\put(.4,.27321){\vector(-1,2){.3}}%
	\put(.6,.27321){\vector(1,2){.3}}%
%
	\put(.5,.1){\makebox(0,0){$F$}}%
	\put(1,1.1){\makebox(0,0){$L$}}%
	\put(0,1.2){\makebox(0,0){$E$}}%
%
	\put(.1,.6){\makebox(0,0)[r]{inc}}%
	\put(.9,.6){\makebox(0,0)[l]{id}}%
	\put(.5,1.2){\makebox(0,0)[b]{$\tau$}}%
\end{picture}%
}

\newcommand\embeddingextensiont[1]{%
\setlength{\unitlength}{#1}%
\begin{picture}(1,1)%
\thicklines
%	\put(.1,.69282){\vector(2,-3){.3}}%
	\put(.4,.2732){\vector(-2,3){.3}}%
	\put(.2,.966){\vector(1,0){.6}}%
	\put(.6,.2732){\vector(2,3){.3}}%
%
	\put(0,.966){\makebox(0,0){$A$}}%
	\put(1,.966){\makebox(0,0){$A'$}}%
	\put(.5,.1){\makebox(0,0){$A/\ideal{a}$}}%
%
	\put(.5, 1.066){\makebox(0,0)[b]{$\tau$}}%
	\put(.1, .533){\makebox(0,0)[r]{inc}}%
	\put(.9, .533){\makebox(0,0)[l]{id}}%
\end{picture}%
}

\newcommand\factorring[1]{%
\setlength{\unitlength}{#1}%
\begin{picture}(1,1)%
\thicklines
	\put(.1,.69282){\vector(2,-3){.3}}%
	\put(.2,.866){\vector(1,0){.6}}%
	\put(.6,.1732){\vector(2,3){.3}}%
%
	\put(0,.866){\makebox(0,0){$A$}}%
	\put(1,.866){\makebox(0,0){$A'$}}%
	\put(.5,0){\makebox(0,0){$A/\ideal{a}$}}%
%
	\put(.5, .966){\makebox(0,0)[b]{$g$}}%
	\put(.1, .433){\makebox(0,0)[r]{$f$}}%
	\put(.9, .433){\makebox(0,0)[l]{$g^\ast$}}%
\end{picture}%
}

\newcommand\commutativediagram[1]{%
\setlength{\unitlength}{#1}%
\begin{picture}(1.2,1.2)%
\thicklines
	\put(.3,.1){\vector(1,0){.6}}%
	\put(.1,.3){\vector(0,1){.6}}%
	\put(.3,1.1){\vector(1,0){.6}}%
	\put(1.1,.3){\vector(0,1){.6}}%
%
	\put(.2,.1){\makebox(0,0)[r]{$f^{-1}(H')$}}
	\put(.1,1.1){\makebox(0,0){$G$}}
	\put(1.1,1.1){\makebox(0,0){$G'$}}
	\put(1.1,.1){\makebox(0,0){$H'$}}
\end{picture}%
}

\newcommand\largecommutativediagram[8]{%
\setlength{\unitlength}{#1}%
%\fbox{
\begin{picture}(4.2,1)%
\thicklines
	\put(.4,.1){\vector(1,0){.4}}%
	\put(1.4,.1){\vector(1,0){.4}}%
	\put(2.4,.1){\vector(1,0){.4}}%
	\put(3.4,.1){\vector(1,0){.4}}%
%
	\put(.4,.9){\vector(1,0){.4}}%
	\put(1.4,.9){\vector(1,0){.4}}%
	\put(2.4,.9){\vector(1,0){.4}}%
	\put(3.4,.9){\vector(1,0){.4}}%
%
	\put(1.1,.7){\vector(0,-1){.4}}%
	\put(2.1,.7){\vector(0,-1){.4}}%
	\put(3.1,.7){\vector(0,-1){.4}}%
%
	\put(.1,.9){\makebox(0,0){$0$}}%
	\put(1.1,.9){\makebox(0,0){#2}}%
	\put(2.1,.9){\makebox(0,0){#3}}%
	\put(3.1,.9){\makebox(0,0){#4}}%
	\put(4.1,.9){\makebox(0,0){$0$}}%
%
	\put(.1,.1){\makebox(0,0){$0$}}%
	\put(1.1,.1){\makebox(0,0){#5}}%
	\put(2.1,.1){\makebox(0,0){#6}}%
	\put(3.1,.1){\makebox(0,0){#7}}%
	\put(4.1,.1){\makebox(0,0){0}}%
%
	\ifthenelse{\equal{#8}{1}}
	{
		\put(1.6,.95){\makebox(0,0)[b]{$f$}}%
		\put(2.6,.95){\makebox(0,0)[b]{$g$}}%
	}
	{
		\ifthenelse{\equal{#8}{2}}
		{
			\put(1.2,.5){\makebox(0,0)[l]{can}}%
			\put(2.2,.5){\makebox(0,0)[l]{can}}%
			\put(3.2,.5){\makebox(0,0)[l]{id}}%
		}
		{
			\put(2.2,.5){\makebox(0,0)[l]{$f$}}%
			\put(3.2,.5){\makebox(0,0)[l]{$\bar{f}$}}%
		}
	}
\end{picture}%
%}
}

\newcommand\butterfly[1]{%
\setlength{\unitlength}{#1}%
\begin{picture}(10,8)(-5,-2)%
%\thicklines
	\put(0,0){\circle*{.2}}
		\put(0,0){\line(0,1){2}}
		\put(0,2){\circle*{.2}}
%
		\put(0,0){\line(-1,-1){1}}
			\put(-1,-1){\makebox(0,-.3)[t]{$u\cap V$}}%
	\put(-1,-1){\circle*{.2}}
		\put(-1,-1){\line(-2,1){2}}
		\put(-3,0){\makebox(-.3,0)[r]{$u$}}
	\put(-3,0){\circle*{.2}}
		\put(-3,0){\line(1,1){1}}
		\put(-2,1){\makebox(-.4,0)[r]{$u(U\cap v)$}}
	\put(-2,1){\circle*{.2}}
		\put(-2,1){\line(2,-1){2}}
		\put(-2,1){\line(0,1){2}}
		\put(-2,3){\makebox(-.4,0)[r]{$u(U\cap V)$}}
	\put(-2,3){\circle*{.2}}
		\put(-2,3){\line(2,-1){2}}
		\put(-2,3){\line(0,1){1.5}}
		\put(-2,4.9){\makebox(0,0)[b]{$U$}}
	\put(-2,4.5){\circle*{.2}}
%
		\put(0,0){\line(1,-1){1}}
			\put(1,-1){\makebox(0,-.3)[t]{$U\cap v$}}%
	\put(1,-1){\circle*{.2}}
		\put(1,-1){\line(2,1){2}}
		\put(3.3,0){\makebox(0,0)[l]{$v$}}
	\put(3,0){\circle*{.2}}
		\put(3,0){\line(-1,1){1}}
%		\put(2,1){\makebox(.3,0)[l]{$(u\cap V)v$}}
		\put(2.4,1){\makebox(0,0)[l]{$(u\cap V)v$}}
	\put(2,1){\circle*{.2}}
		\put(2,1){\line(-2,-1){2}}
		\put(2,1){\line(0,1){2}}
%		\put(2,3){\makebox(.2,0)[l]{$(U\cap V)v$}}
		\put(2.3,3){\makebox(0,0)[l]{$(U\cap V)v$}}
	\put(2,3){\circle*{.2}}
		\put(2,3){\line(-2,-1){2}}
		\put(2,3){\line(0,1){1.5}}
		\put(2,4.9){\makebox(0,0)[b]{$V$}}
	\put(2,4.5){\circle*{.2}}
\end{picture}
}



\titlefoil{Why Abstract Algebra?}{Why Abstract Algebra?}

\myfoilhead{Why abstract algebra?}%
	\index{abstract algebra}%
	\index{abstract algebra!why}

\bit
\item
	it's fun!

\vvitem
	can understand \emph{instrict structures} of algebraic objects

\vitem
	allow us to solve \emph{extremely practical problems}
	(depending on your definition of practicality)
	\bit
	\item
		\eg, can prove why root formulas for polynomials of order $n\geq 5$ do not exist
	\eit

\vitem
	prepare us for pursuing further math topics such as
	\bit
	\vitem
		differential geometry
	\vitem
		algebraic geometry
	\vitem
		analysis
	\vitem
		representation theory
	\vitem
		algebraic number theory
	\eit
%
%\vvitem and ... it's really fun; sheer examination and pondoring upon ideas make us happy and thrilled!
\eit
\vfill


\myfoilhead{Some history}%
	\index{abstract algebra!history}

\bit
\item
	by the way, historically, often the case that application of an idea presented
	before extracting and presenting the idea on its own right

\vitem
	\eg,
	Galois used ``quotient group'' only implicitly in his 1830's investigation,
	and it had to wait until 1889 to be explicitly presented as ``abstract quotient group''
	by H\"{o}lder
\eit
\vfill


\titlefoil{Groups}{Groups}

\myfoilhead{Monoids}

\begin{mydefinition}{law of composition}
	mapping $S\times S \to S$ for set $S$,
	called \define{law of composition (of $S$ to itself)}%
		\index{law of composition!group}%
		\index{group!law of composition}
	\ibit
	\item [-]
		when $(\forall x, y, z \in S)((xy)z = x(yz))$,
		composition is said to be \define{associative}%
			\index{associativity!group}%
			\index{group!associativity}
	\vitem [-]
		$e\in S$ such that $(\forall x\in S)(ex = xe = x)$,
		called \define{unit element} - always unique
			\index{unit element!group}%
			\index{group!unit element}

		\begin{proof}
			for any two unit elements $e$ and $f$,
			$
				e = ef = f,
			$
			hence, $e=f$
		\end{proof}
	\eit
\end{mydefinition}

\vfill

\begin{mydefinition}{monoids}%
	\index{group!monoid}%
	\index{monoid!group}

	set $M$ with composition which is associative and having unit element,
	called \define{monoid}
	(so in particular, $M$ is not empty)
	\ibit
	\item [-]
		monoid $M$ with
		$
			\left(
				\forall x, y \in M
			\right)
			\left(
				xy = yx
			\right)
		$,
		called \define{commutative or abelian} monoid%
			\index{monoid!commutative}%
			\index{monoid!abelian}%
			\index{abelian monoid}%
			\index{Abel, Niels Henrik!abelian monoid}%
			\index{commutative monoid}

	\vitem [-]
		subset $H\subset M$ which has \emph{the} unit element $e$ and is itself monoid,
		called \define{submonoid}%
			\index{submonoid}%
			\index{submonoid!monoid}
	\eit
\end{mydefinition}


\myfoilhead{Groups}

\begin{mydefinition}{group}
	monoid $G$ with
	$$
		\left(
			\forall x \in G
		\right)
		\left(
			\exists y \in G
		\right)
		\left(
			xy = yx = e
		\right)
	$$
	called \define{group}
	\ibit
	\item [-]
		for $x\in G$, $y\in G$ with $xy=yx=e$,
		called \define{inverse of $x$}%
			\index{inverse!group}

	\vitem [-]
		group derived from commutative monoid,
		called \define{abelian group} or \define{commutative group}%
			\index{Abel, Niels Henrik!abelian group}%
			\index{abelian group}%
			\index{commutative group}%
			\index{group!abelian}%
			\index{group!commutative}\

	\vitem [-]
		group $G$ with $|G|<\infty$,
		called \define{finite group}%
			\index{finite group}%
			\index{group!finite}

	\vitem [-]
		(similarly as submonoid)
		$H\subset G$ that has unit element and is itself group,
		called \define{subgroup}
			\index{subgroup}%
			\index{subgroup!group}

	\vitem [-]
		subgroup consisting only of unit element, called \define{trivial}%
			\index{trivial subgroup}%
			\index{subgroup!trivial}
	\eit
\end{mydefinition}


\myfoilhead{Cyclic groups, generators, and direct products}

\begin{mydefinition}{cyclic groups}%
	\index{group!cyclic}
	group $G$ with
	$$
		\left(
			\exists a\in G
		\right)
		\left(
			\forall x \in G
		\right)
		\left(
			\exists n\in \naturals
		\right)
		\left(
			x = a^n
		\right)
	$$
	called \define{cyclic group}%
		\index{group!cyclic group},
	such $a\in G$ called \define{cyclic generator}%
		\index{cyclic generator!group}%
		\index{group!cyclic generator}
\end{mydefinition}

\begin{mydefinition}{generators}
	for group $G$, $S\subset G$
	with
	$$
		\left(
			\forall x \in G
		\right)
		\left(
			x \mbox{ is arbitrary product of elements or inverse elements of } S
		\right)
	$$
	called \define{set of generators for $G$},
	said to \define{generate $G$},
	denoted by $G=\generates{S}$%
		\index{generators!group}%
		\index{group!generators}
\end{mydefinition}

\begin{mydefinition}{direct products}
	for two groups $G_1$ and $G_2$,
	group $G_1\times G_2$
	with
	$$
		\left(
			\forall (x_1,x_2), (y_1,y_2) \in G_1 \times G_2
		\right)
		\left(
			(x_1,x_2)(y_1,y_2)
			= (x_1y_1, x_2,y_2) \in G_1 \times G_2
		\right)
	$$
	whose unit element defined by $(e_1,e_2)$
	where $e_1$ and $e_2$ are unit elements of $G_1$ and $G_2$ respectively,
	called \define{direct product of $G_1$ and $G_2$}%
		\index{direct products!group}
		\index{group!direct products}
\end{mydefinition}


\myfoilhead{Homeomorphism and isomorphism}

\begin{mydefinition}{homeomorphism}
	for monoids $M$ and $M'$,
	mapping $f:M\to M'$ with $f(e)=e'$
	$$
		\left(
			x,y \in M
		\right)
		\left(
			f(xy) = f(x)f(y)
		\right)
	$$
	where $e$ and $e'$ are unit elements of $M$ and $M'$ respectively,
	called \define{monoid-homeomorphism} or simple \define{homeomorphism}%
		\index{monoid-homeomorphism}%
		\index{homeomorphism!monoid}%
		\index{monoid!homeomorphism}%

	\shrinkspacewithintheoremslike
	\ibit
	\iitem
		\define{group homeomorphism} $f:G\to G'$ is similarly monoid-homeomorphism%
			\index{homeomorphism!group}%
			\index{group!homeomorphism}%
	\iitem
		homeomorphism $f:G\to G'$ where exists $g:G\to G'$ such that $f\circ g:G'\to G'$ and $g\circ f:G\to G$
		are identity mappings,
		called \define{isomorphism},%
			\index{isomorphism!group}%
			\index{isomorphism!monoid}%
			\index{group!isomorphism}%
			\index{monoid!isomorphism}
		sometimes denoted by \define{$G\isomorph G'$}

	\iitem
		homeomorphism of $G$ into itself, called \define{endomorphism}
			\index{endomorphism!group}%
			\index{endomorphism!monoid}%
			\index{group!endomorphism}%
			\index{monoid!endomorphism}

	\iitem
		isomorphism of $G$ onto itself, called \define{automorphism}
			\index{automorphism!group}%
			\index{automorphism!monoid}%
			\index{group!automorphism}%
			\index{monoid!automorphism}
	\eit
\end{mydefinition}

\bit
\item
	set of all automorphisms of $G$ is itself group,
	denoted by \define{\aut{G}}\
\eit


\myfoilhead{Kernel, image, and embedding of homeomorphism}

\begin{mydefinition}{kernel of homeomorphism}%
	\index{kernel!group homeomorphism}%
	\index{homeomorphism!group!kernel}
	for group-homeomorphism $f:G\to G'$ where $e'$ is unit element of $G'$,
	$f^{-1}(\{e'\})$, which is subgroup of $G$,
	called \define{kernel of $f$},
	denoted by $\Ker{f}$\
\end{mydefinition}

\begin{mydefinition}{embedding of homeomorphism}%
	\index{embedding!group homeomorphism}%
	\index{homeomorphism!group!embedding}
	homeomorphism $f:G\to G'$ establishing isomorphism between $G$ and $f(G)\subset G'$,
	called \define{embedding}
\end{mydefinition}


\begin{myproposition}{group homeomorphism and isomorphism}\ \\\vspace{-2em}

	\ibit
	\iitem
		for group-homeomorphism $f:G\to G'$, $f(G)\subset G'$ is subgroup of $G'$
	\iitem
		homeomorphism whose kernel is trivial is injective,%
			\index{injective!homeomorphism!group}
			\index{homeomorphism!group!injective}
			\index{group!homeomorphism!injective}
		often denoted by special arrow
		$$
			f:G \injhomeo G'
		$$
	\iitem
		surjective homeomorphism whose kernel is trivial is \emph{isomorphism}
	\iitem
		for group $G$, its generators $S$, and another group $G'$,
		map $f:S\to G'$ has at most one extension to homeomorphism of $G$ into $G'$

	\eit
\end{myproposition}


\myfoilhead{Orthogonal subgroups}

\begin{myproposition}{orthogonal subgroups}%
		\index{orthogonal subgroup!group}%
		\index{group!orthogonal subgroup}
	for group $G$ and two subgroups $H$ and $K\subset G$
	with $HK = G$, $H\cap K = \{e\}$, and $ \left( x\in H, y\in K \right) \left( xy=yx \right) $,
	$$
		f: H\times K \to G
	$$
	with $(x,y)\mapsto xy$
	is \emph{isomorphism}

%	\ibit
%	\iitem
		can generalize to finite number of subgroups, $H_1$, \ldots, $H_n$ such that
		$$
			H_1 \cdots H_n = G
		$$
		and
		$$
			H_{k+1} \cap (H_1\cdots H_k) = \{e\}
		$$

		in which case, $G$ is isomorphic to $H_1\cdots H_n$
%		\eit
\end{myproposition}


\myfoilhead{Cosets of groups}

\begin{mydefinition}{cosets of groups}%
		\index{coset!group}%
		\index{group!coset}
	for group $G$ and subgroup $H\subset G$,
	$aH$ for some $a\in G$,
	called \define{left coset of $H$ in $G$},%
		\index{left coset!group}%
		\index{group!left coset}
	and element in $aH$, called
	\define{coset representation of $aH$}%
		\index{coset representation!group}%
		\index{group!coset representation}\
	- can define \define{right cosets} similarly
		\index{right coset!group}%
		\index{group!right coset}
\end{mydefinition}


\begin{myproposition}{cosets of groups}
	for group $G$ and subgroup $H\subset G$,
	\ibit
	\iitem
		for $a\in G$,
		$x\mapsto ax$ induces bijection of $H$ onto $aH$,
		hence all left cosets have same cardinality

	\iitem
		$aH \cap bH \neq \emptyset$ for $a,b\in G$ implies $aH=bH$

	\iitem
		hence, $G$ is disjoint union of left cosets of $H$

	\iitem
		same statements can be made for right cosets
	\eit
\end{myproposition}

\begin{mydefinition}{index and order of group}%
		\index{index!group}%
		\index{order!group}%
		\index{group!index}%
		\index{group!order}
	number of left cosets of $H$ in $G$,
	called \define{index of $H$ in $G$},
	denoted by $(G:H)$\
	- index of trivial subgroups,
	called \define{order of $G$},
	denoted by $(G:1)$\
\end{mydefinition}


\myfoilhead{Indices and orders of groups}

\begin{myproposition}{indices and orders}
	for group $G$ and two subgroups $H$ and $K\subset G$ with $K\subset H$,
	$$
		(G:H) (H:K) = (G:K)
	$$
	when $K$ is trivial, we have
	$$
		(G:H) (H:1) = (G:1)
	$$
	\proofref{relation among coset indices}
\end{myproposition}
hence, if $(G:1)<\infty$, both $(G:H)$ and $(H:1)$ divide $(G:1)$


\myfoilhead{Normal subgroup}

\begin{mydefinition}{normal subgroups}%
		\index{normal subgroup!group}%
		\index{group!normal subgroup}

	subgroup $H\subset G$ of group $G$ with
	$$
		\left(
			\forall x \in G
		\right)
		\left(
			x H = H x
		\right)
		\Leftrightarrow
		\left(
			\forall x \in G
		\right)
		\left(
			xHx^{-1}= H
		\right)
	$$
	called \define{normal subgroup of $G$},
	in which case
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		set of cosets \set{xH}{x\in G}\ with law of composition defined by
		$
			(xH)(yH) = (xy)H,
		$
		forms group with unit element $H$,
		denoted by \define{$G/H$},
		called \define{factor group of $G$ by $H$},%
			\index{factor group!group}%
			\index{group!factor group}
		read \define{$G$ modulo $H$}
		or \define{$G$ mod $H$}

	\iitem
		$x \mapsto xH$ induces homeomorphism of $X$ onto \set{xH}{x\in G},
		called \define{canonical map}%
			\index{canonical maps!group}%
			\index{group!canonical maps},
		kernel of which is $H$
	\eit
\end{mydefinition}

\begin{myproposition}{normal subgroups and factor groups}
\ \
\shrinkspacewithintheoremslike
	\ibit
	\iitem kernel of (every) homeomorphism of $G$ is normal subgroups of $G$
	\iitem for family of normal subgroups of $G$, \seq{N_\lambda},
		$
			\bigcap N_\lambda
		$
		is also normal subgroup
	\iitem
		every subgroup of abelian group is normal
	\iitem
		factor group of abelian group is abelian
	\iitem
		factor group of cyclic group is cyclic
	\eit
\end{myproposition}


\myfoilhead{Normalizers and centralizers}

\begin{mydefinition}{normalizers and centralizers}
	for subset $S\subset G$ of group $G$,
	$$
		\set{x\in G}{xSx^{-1} = S}
	$$
	is subgroup, called \define{normalizer of $S$,}%
		\index{normalizers!group}%
		\index{group!normalizers}
	and also called \define{centralizer of $a$} when $S=\{a\}$ is singletone;

	$$
		\set{x\in G}{(\forall y\in S)(xyx^{-1} = y)}
	$$

	called \define{centralizer of $S$,}%
		\index{centralizers!group}%
		\index{group!centralizers}
	and centralizer of $G$ itself, called \define{center of $G$}%
		\index{center!of group}%
		\index{group!center}
\end{mydefinition}

\bit
\vitem \eg,
	$A \mapsto \det A$ of multiplicative group of square matrices in $\reals^{n\times n}$
	into $\reals\sim\{0\}$
%	onto $\reals$,
%		onto $\reals \sim \{0\}$
	is homeomorphism,
	kernel of which called \define{special linear group,}%
		\index{special linear group!group}%
		\index{group!special linear group}
	and (of course) is normal
\eit
\vfill


\myfoilhead{Normalizers and congruence}

\begin{myproposition}{normalizers of groups}
	subgroup $H\subset G$ of group $G$
	is normal subgroup of its normalizer $N_H$\
	\bit
	\item
		subgroup $H\subset G$ of group $G$
		is normal subgroup of its normalizer $N_H$
	\item
		subgroup $K\subset G$ with $H\subset K$ where $H$ is normal in $K$
		is contained in $N_H$
	\item
		for subgroup $K\subset N_H$,
		$KH$ is group
		and $H$ is normal in $KH$

	\item
		normalizer of $H$ is largest subgroup of $G$ in which $H$ is normal
	\eit
\end{myproposition}

\vfill
\begin{mydefinition}{congruence with respect to normal subgroup}%
		\index{congruence with respect to normal subgroup!group}%
		\index{group!congruence with respect to normal subgroup}
		for normal subgroup $H\subset G$ of group $G$,
		we write
		$$
			x \equiv y \Mod{H}
		$$
		if $xH=yH$,
		read \define{$x$ and $y$ are congruent modulo $H$}
		- this notation used mostly for additive groups
\end{mydefinition}
\vfill


\myfoilhead{Exact sequences of homeomorphisms}

\begin{mydefinition}{exact sequences of homeomorphisms}%
		\index{exact sequences of homeomorphisms!group}%
		\index{group!exact sequences of homeomorphisms}
	below sequence of homeomorphisms with $\Img f = \Ker g$
	$$
		G' \overset{f}{\longrightarrow}
		G \overset{g}{\longrightarrow}
		G''
	$$
	said to be \define{exact}\

	below sequence of homeomorphisms
	with $\Img f_i = \Ker f_{i+1}$
	$$
		G_1 \overset{f_1}{\longrightarrow}
		G_2 \overset{f_2}{\longrightarrow}
		G_3 \longrightarrow
		\cdots
		\overset{f_{n-1}}{\longrightarrow}
		G_n
	$$
	said to be \define{exact}\
\end{mydefinition}

\bit
\vitem
	for normal subgroup $H\subset G$ of group $G$,
	sequence
	$
		H \overset{j}{\to}
		G \overset{\varphi}{\to}
		G/H
	$
	is exact
	where $j$ is inclusion and $\varphi$

\item
	$
		0 \overset{}{\to}
		G' \overset{f}{\to}
		G \overset{g}{\to}
		G'' \overset{}{\to}
		0
	$
	is exact
	\iaoi\
	$f$ injective, $g$ surjective, and $\Img f = \Ker g$

\item
if $H=\Ker g$ above,
	$
		0 \overset{}{\to}
		H \overset{}{\to}
		G \overset{}{\to}
		G/H \overset{}{\to}
		0
	$

\item
	more precisely, exists commutative diagram as in \figref{commutative diagram for canonical map},
	in which vertical mappings are isomorphisms and rows are \emph{exact}
	\begin{figure}
	\begin{center}
		\largecommutativediagram{6em}%
		{$G'$}{$G$}{$G''$}%
		{$H$}{$G$}{$G/H$}%
		{1}
%		\mypsfrag{0}{$0$}
%		\mypsfrag{G}{$G$}
%		\mypsfrag{Gp}{$G'$}
%		\mypsfrag{Gq}{$G''$}
%		\mypsfrag{G/Ha}{$G/H$}
%		\mypsfrag{H}{$H$}
%		\mypsfrag{f}{$f$}
%		\mypsfrag{g}{$g$}
%		\includegraphics[width=.95\textwidth]{figures/py-normal-commutative-diagram-psfragable}%
			\idxfig{commutative diagram for canonical map}
			\label{fig:commutative diagram for canonical map}
	\end{center}
	\end{figure}
\eit


\myfoilhead{Canonical homeomorphism examples}

all homeomorphisms described below called \define{canonical}

\bit
\item
	for two groups $G$ \& $G'$ and homeomorphism $f:G\to G'$ whose kernel is $H$,
	exists unique homeomorphism $f_*: G/H \to G'$ with $$f=f_*\circ \varphi$$
	where $\varphi:G\to G/H$ is canonical map,
	and $f_*$ is injective
	\bit
	\item
		$f_*$ can be defined by $xH\mapsto f(x)$
	\item
		\define{$f_*$ said to be induced by $f$}
	\item
		$f_*$ induces isomorphism $\lambda: G/H \to \Img f$
	\item
		below sequence summarizes above statements
		$$
			G \overset{\varphi}{\to}
			G/H \overset{\lambda}{\to}
			\Img f \overset{j}{\to}
			G
		$$
		where $j$ is inclusion
	\eit

\item
	for group $G$,
	subgroup $H\subset G$,
	and
	homeomorphism $f:G\to G'$ whose kernel contains $H$,
	intersection of all normal subgroups containing $H$, $N$,
	which is the smallest normal subgroup containing $H$,
	is contained in $\Ker f$,
	\ie,
	$N\subset \Ker f$,
	and exists unique homeomorphism, $f_*:G/N\to G'$
	such that $$f = f_* \circ \varphi$$
	where $\varphi:G\to G/H$ is canonical map
	\bit
	\item
		$f_*$ can be defined by $xN\mapsto f(x)$
	\item
		\define{$f_*$ said to be induced by $f$}
	\eit

\item
	for subgroups of $G$, $H$ and $K$ with $K\subset H$,
	$xK \mapsto xH$ induces homeomorphism of $G/K$ into $G/H$,
	whose kernel is \set{xK}{x\in H},
	thus \eemph{canonical isomorphism}%
		\index{canonical isomorphisms!group}%
		\index{group!canonical isomorphisms}\
	$$
		(G/K)/(H/K) \isomorph (G/K)
	$$
	this can be shown in \figref{commutative diagram for canonical isomorphism}
	where rows are exact

	\begin{figure}
	\begin{center}
		\largecommutativediagram{6em}%
		{$H$}{$G$}{$G/H$}%
		{$H/K$}{$G/K$}{$G/H$}%
		{2}
%		\mypsfrag{0}{$0$}
%		\mypsfrag{H}{$H$}
%		\mypsfrag{G}{$G$}
%		\mypsfrag{G/Ha}{$G/H$}
%		\mypsfrag{H/Ka}{$H/K$}
%		\mypsfrag{G/Ka}{$G/K$}
%		\mypsfrag{can}{can}
%		\mypsfrag{id}{id}
%		\includegraphics[width=.95\textwidth]{figures/py-normal-commutative-diagram-2-psfragable}%
			\idxfig{commutative diagram for canonical isomorphism}
			\label{fig:commutative diagram for canonical isomorphism}
	\end{center}
	\end{figure}

\item
	for subgroup $H\subset G$ and $K\subset G$ with $H$ contained in normalizer of $K$,
	$H\cap K$ is normal subgroup of $H$,
	$HK=KH$ is subgroup of $G$,
	exists surjective homeomorphism
	$$
		H \to HK / K
	$$
	with $x \mapsto xK$,
	whose kernel is $H\cap K$,
	hence \eemph{canonical isomorphism}%
		\index{canonical isomorphisms!group}%
		\index{group!canonical isomorphisms}\
	$$
		H/(H\cap K) \isomorph HK/K
	$$

\item
	for group homeomorphism $f:G\to G'$, normal subgroup of $G'$, $H'$,
		$$H=f^{-1}(H')\subset G$$
		as shown in \figref{commutative diagram 1},%
		\begin{figure}
		\begin{center}
			\commutativediagram{5em}
%			\mypsfrag{G}{$G$}
%			\mypsfrag{Gp}{$G'$}
%			\mypsfrag{Hp}{$H'$}
%			\mypsfrag{f-1Hpaaaa}{$f^{-1}(H')$}
%			\includegraphics[width=.3\textwidth]{figures/py-normal-commutative-diagram-3-psfragable}%
				\idxfig{commutative diagram}
				\label{fig:commutative diagram 1}
		\end{center}
		\end{figure}
		$H$ is normal in $G$
		and kernel of homeomorphism
		$$
			G \overset{f}{\to} G'\overset{\varphi}{\to} G'/H'
		$$
		is $H$ where $\varphi$ is canonical map,
		hence we have injective homeomorphism
		$$
			\bar{f}:G/H \to G'/H'
		$$
		again called \eemph{canonical homeomorphism},
		giving commutative diagram
		in \figref{commutative diagram for canonical homeomorphism};
		if $f$ is surjective, $\bar{f}$ is isomorphism

	\begin{figure}
	\begin{center}
		\largecommutativediagram{6em}%
		{$H$}{$G$}{$G/H$}%
		{$H'$}{$G'$}{$G'/H'$}%
		{3}
%		\mypsfrag{0}{$0$}
%		\mypsfrag{H}{$H$}
%		\mypsfrag{G}{$G$}
%		\mypsfrag{G/Ha}{$G/H$}
%		\mypsfrag{Hp}{$H'$}
%		\mypsfrag{Gp}{$G'$}
%		\mypsfrag{Gp/Hp}{$G'/H'$}
%		\mypsfrag{f}{$f$}
%		\mypsfrag{g}{$\bar{f}$}
%		\includegraphics[width=.95\textwidth]{figures/py-normal-commutative-diagram-4-psfragable}%
			\idxfig{commutative diagram for canonical homeomorphism}
			\label{fig:commutative diagram for canonical homeomorphism}
	\end{center}
	\end{figure}
\eit


\myfoilhead{Towers}

\begin{mydefinition}{towers of groups}%
		\index{towers!group}%
		\index{group!towers}
	for group $G$,
	sequence of subgroups
	$$
		G = G_0
		\supset G_1
		\supset G_2
		\supset \cdots
		\supset G_m
	$$
	called \define{tower of subgroups}
	\bit
	\item
		said to be \define{normal} if every $G_{i+1}$ is normal in $G_i$
	\item
		said to be \define{abelian} if normal and every factor group $G_i/G_{i+1}$ is abelian
	\item
		said to be \define{cyclic} if normal and every factor group $G_i/G_{i+1}$ is cyclic
	\eit
		\index{group!towers!normal}%
		\index{group!towers!abelian}%
		\index{group!towers!cyclic}%
		\index{normal group!towers}%
		\index{abelian group!towers}%
		\index{cyclic group!towers}%
		\index{towers!normal}%
		\index{towers!abelian}%
		\index{towers!cyclic}
\end{mydefinition}

\begin{myproposition}{towers inded by homeomorphism}%
	\index{towers!inded by homeomorphism}
	for group homeomorphism $f:G\to G'$ and normal tower
	$$
		G' = G'_0
		\supset G'_1
		\supset G'_2
		\supset \cdots
		\supset G'_m
	$$
	tower
	$$
		f^{-1}(G') = f^{-1}(G'_0)
		\supset f^{-1}(G'_1)
		\supset f^{-1}(G'_2)
		\supset \cdots
		\supset f^{-1}(G'_m)
	$$
	is
	\bit
	\item
		normal if $G'_i$ form normal tower
	\item
		abelian if $G'_i$ form abelian tower
	\item
		cyclic if $G'_i$ form cyclic tower
	\eit
	because every homeomorphism
	$$
		G_i / G_{i+1}
		\to
		G'_i / G'_{i+1}
	$$
	is injective
\end{myproposition}


\myfoilhead{Refinement of towers and solvability of groups}

\begin{mydefinition}{refinement of towers}%
		\index{refinement of towers!group}%
		\index{group!refinement of towers}
		\index{towers!refinement}
	for tower of subgroups,
	tower obtained by inserting finite number of subgroups,
	called \define{refinement of tower}
\end{mydefinition}

\vfill
\begin{mydefinition}{solvable groups}%
		\index{solvable group!group}%
		\index{group!solvable group}
	group having an abelian tower whose last element is trivial subgroup,
	said to be \define{solvable}%
		\idximportant{group having an abelian tower whose last element is trivial subgroup,\
			said to be \define{solvable}}
\end{mydefinition}

\vfill
\begin{myproposition}{finite solvable groups}\ \\
	\shrinkspacewithintheoremsliket
	\ibit
	\iitem
		abelian tower of finite group admits cyclic refinement
	\iitem
		finite solvable group admits cyclic tower, whose last element is trivial subgroup
	\eit
\end{myproposition}

\vfill
\begin{mytheorem}{Feit-Thompson theorem}%
		\index{Feit, Walter!Feit-Thompson theorem}%
		\index{Thompson, John Griggs!Feit-Thompson theorem}
	group whose order is prime power is solvable
\end{mytheorem}

\vfill
\begin{mytheorem}{solvability condition in terms of normal subgroups}
	for group $G$ and its normal subgroup $H$,
	$G$ is solvable \iaoi\
	both $H$ and $G/H$ are solvable
\end{mytheorem}
\vfill


\myfoilhead{Commutators and commutator subgroups}

\begin{mydefinition}{commutator}%
		\index{commutator!group}%
		\index{group!commutator}
	for group $G$,
	$xyx^{-1}y^{-1}$ for $x,y\in G$,
	called \define{commutator}
\end{mydefinition}

\begin{mydefinition}{commutator subgroups}%
		\index{commutator subgroup!group}%
		\index{group!commutator subgroup}
	subgroup generated by commutators of group $G$,
	called \define{commutator subgroup},
	denoted by \define{$G^C$},
	\ie\
	$$
		G^C = \generates{\set{xyx^{-1}y^{-1}}{x,y\in G}}
	$$
\end{mydefinition}

\bit
\item
	$G^C$ is normal in $G$
\vitem
	$G/G^C$ is commutative
\vitem
	$G^C$ is contained in kernel of every homeomorphism of $G$ into commutative group

\vitem [-]
	\proofref{normality and commutativity of commutator subgroups} of above statements
\eit

\bit
\item
	\eemph{commutator group is at the heart of solvability and non-solvability problems!}
\eit
\vfill


\myfoilhead{Simple groups}

\begin{mydefinition}{simple groups}%
		\index{group!simple}
	non-trivial group having no normal subgroup other than itself and trivial subgroup,
	said to be \define{simple}
\end{mydefinition}

\vfill
\begin{myproposition}{simple groups}
	abelian group is simple
		\iaoi\
	cycle of prime order
\end{myproposition}
\vfill


\myfoilhead{Butterfly lemma}

\begin{mylemma}{butterfly lemma - Zassenhaus}%
	\index{butterfly lemma!group}%
	\index{group!butterfly lemma}%
	\index{Zassenhaus, Hans!butterfly lemma}
	for subgroups $U$ and $V$ of a group
	and normal subgroups $u$ and $v$ of $U$ and $V$ respectively,
	$$
		u(U\cap v) \mbox{ is normal in } u(U\cap V)
	$$
	$$
		(u\cap V)v \mbox{ is normal in } (U\cap V)v
	$$
	and factor groups are isomorphic,
	\ie,
	$$
		u(U\cap V) / u(U\cap v)
			\isomorph\
		(U\cap V)v / (u\cap V)v
	$$
	these shown in \figref{butterfly lemma}
\end{mylemma}

\bit
\item
	indeed
	$$
		(U\cap V)/((u\cap V)(U\cap v))
			\isomorph\
		u(U\cap V) / u(U\cap v)
			\isomorph\
		(U\cap V)v / (u\cap V)v
	$$
\eit

%\setlength{\fboxsep}{0pt}
\begin{figure}
\begin{center}
%\fbox{
\butterfly{1cm}
%}
%	\mypsfrag{U}{$U$}
%	\mypsfrag{V}{$V$}
%	\mypsfrag{ua}{$u$}
%	\mypsfrag{v}{$v$}
%	\mypsfrag{Ucapv}{$U\cap v$}
%	\mypsfrag{ucapVv}{$(u\cap V)v$}
%	\mypsfrag{UcapVv}{$(U\cap V)v$}
%
%	\mypsfrag{ucapV}{$u\cap V$}
%	\mypsfrag{uUcapvaaaaaaaaaa}{$u(U\cap v)$}
%	\mypsfrag{uUcapVaaaaaaaaaa}{$u(U\cap V)$}
%
%	\mypsfrag{UcapV}{$U\cap V$}
%
%%	\includegraphics[width=.5\textwidth]{figures/py-butterfly-lemma-psfragable}%
%	\includegraphics[width=.3\textwidth]{figures/py-butterfly-lemma-psfragable}%
		\idxfig{butterfly lemma}
		\label{fig:butterfly lemma}
\end{center}
\end{figure}


\myfoilhead{Equivalent towers}

\begin{mydefinition}{equivalent towers}%
		\index{equivalent towers!group}%
		\index{group!equivalent towers}%
		\index{group!towers!equivalent}%
		\index{towers!equivalent}
	for two normal towers
	of same height
	starting from same group
	ending with trivial subgroup
	$$
		G = G_1
		\supset G_2
		\supset G_3
		\supset \cdots
		\supset G_{n+1} = \{e\}
	$$
	$$
		G = H_1
		\supset H_2
		\supset H_3
		\supset \cdots
		\supset H_{n+1} = \{e\}
	$$
	with
	$$
		G_i/G_{i+1}\isomorph H_{\pi(i)+1}/H_{\pi(i)}
	$$

	for some permutation $\pi\in\perm{\{1,\ldots,n\}}$,
	\ie,
	sequences of factor groups are same
	up to isomorphisms and permutation of indices,
	said to be \define{equivalent}
\end{mydefinition}


\myfoilhead{Schreier and Jordan-H\"{o}lder theorems}

\begin{mytheorem}{Schreier theorem}%
		\index{Schreier theorem!group}%
		\index{Schreier, Otto!Schreier theorem}%
		\index{group!Schreier theorem}
	two normal towers starting from same group and ending with trivial subgroup
	have equivalent refinement
\end{mytheorem}


\vfill
\begin{theorem}[Jordan-H\"{o}lder theorem]%
		\index{Jordan-H\"{o}lder theorem}%
		\index{Jordan, Marie Ennemond Camile!Jordan-H\"{o}lder theorem}%
		\index{H\"{o}lder, Ludwig Otto!Jordan-H\"{o}lder theorem}%
		\index{group!Jordan-H\"{o}lder theorem}
	all normal towers starting from same group and ending with trivial subgroup
	where each factor group is non-trivial and simple
	are equivalent
\end{theorem}

\vfill


\myfoilhead{Cyclic groups}

\begin{mydefinition}{exponent of groups and group elements}%
		\index{exponent!group}%
		\index{group!exponent}
	for group $G$,
	$n\in\naturals$ with $a^n=e$ for $a\in G$,
	called \define{exponent of $a$};
	$n\in\naturals$ with $x^n=e$ for every $x\in G$,
	called \define{exponent of $G$}
\end{mydefinition}

\vfill
\begin{mydefinition}{period of group elements}%
		\index{period!group!elements}%
		\index{group!period!elements}
	for group $G$ and $a\in G$,
	smallest $n\in\naturals$ with $a^n=e$,
	called \define{period of $a$}
\end{mydefinition}

\vfill
\begin{myproposition}{period of elements of finite groups}
	for finite group $G$ of order $n>1$,
	period of every non-unit element $a$ ($\neq e$) devided $n$;
	if $n$ is prime number,
	$G$ is cyclic and period of every generator is $n$
\end{myproposition}

\vfill
\begin{myproposition}{subgroups of cyclic groups}
	every subgroup of cyclic group is cyclic
	and image of every homeomorphism of cyclic group is cyclic
\end{myproposition}

\vfill


\myfoilhead{Properties of cyclic groups}

\begin{myproposition}{properties of cyclic groups}\ \\\vspace{-2em}
	\ibit
	\iitem
		infinity cyclic group has exactly two generators; if $a$ is one, $a^{-1}$ is the other

	\iitem
		for cyclic group $G$ of order $n$ and generator $x$,
		set of generators of $G$ is
		$$
			\set{x^m}{m \mbox{ is relatively prime to }n}
		$$

	\iitem
		for cyclic group $G$ and two generators $a$ and $b$,
		exists automorphism of $G$ mapping $a$ onto $b$;
		conversely, every automorphism maps $a$ to some generator

	\iitem
		for cyclic group $G$ of order $n$ and $d\in\naturals$ dividing $n$,
		exists unique subgroup of order $d$

	\iitem
		for cyclic groups $G_1$ and $G_2$ of orders $n$ and $m$ respectively
		with $n$ and $m$ relatively prime,
		$G_1\times G_2$ is cyclic group

	\iitem
		for non-cyclic finite abelian group $G$,
		exists subgroup isomorphic to $C\times C$
		with $C$ cyclic with prime order
	\eit
\end{myproposition}


\myfoilhead{Symmetric groups and permutations}

\begin{mydefinition}{symmetric groups and permutations}%
	\index{permutations!group}
	\index{group!permutations}
	\index{symmetric group!group}
	\index{group!symmetric}
	for nonempty set $S$, group $G$ of bijective functions of $S$ onto itself
	with law of composition being function composition,
	called \define{symmetric group of $S$}, denoted by \define{\perm{S}};
	elements in \perm{S} called \define{permutations of $S$};
	element swapping two disjoint elements in $S$ leaving every others left,
	called \define{transposition}%
		\index{transpositions!symmetric group}%
		\index{symmetric group!transposition}
		\index{transpositions!permutations}%
		\index{permutations!transposition}
\end{mydefinition}

\begin{myproposition}{sign homeomorphism of finite symmetric groups}
	for finite symmetric group $S_n$,
	exits unique homeomorphism $\epsilon: S_n \to\{-1,1\}$
	mapping every transposition, $\tau$, to $-1$,
	\ie, $\epsilon(\tau)=-1$
\end{myproposition}
\index{sign homeomorphism!of finite symmetric group}%
\index{homeomorphism!sign homeomorphism!of finite symmetric group}

\begin{mydefinition}{alternating groups}%
	element of finite symmetric group $\sigma$ with $\epsilon(\sigma)=1$,
	called \define{even},
	element $\sigma$ with $\epsilon(\sigma)=-1$,
	called \define{odd};
		\index{even!finite symmetric group}%
		\index{group!symmetric!even}%
		\index{odd!finite symmetric group}%
		\index{group!symmetric!odd}
	kernel of $\epsilon$, called \define{alternating group}, denoted by \define{$A_n$}%
		\index{alternating group!finite symmetric group}%
		\index{group!symmetric!alternating}
\end{mydefinition}

\begin{mytheorem}{solvability of finite symmetric groups}
	symmetric group $S_n$ with $n\geq 5$
	is \emph{not} solvable
\end{mytheorem}

\begin{mytheorem}{simplicity of alternating groups}
	alternating group $A_n$ with $n\geq 5$ is simple
\end{mytheorem}


\myfoilhead{Operations of group on set}

\begin{mydefinition}{operations of group on set}%
		\index{operation!group}%
		\index{group!operation}
	for group $G$ and set $S$,
	homeomorphism
	$$
		\pi:G \to \perm{S}
	$$
	called \define{operation of $G$ on $S$} or \define{action of $G$ on $S$}%
		\index{action!group}%
		\index{group!action}
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		$S$, called \define{$G$-set}%
		\index{$G$-set!group}%
		\index{group!$G$-set}

	\iitem
		denote $\pi(x)$ for $x\in G$
%		, \ie, permutation associated with $x$,
%		denote $\pi(x)$ for $x\in G$, \ie, permutation associated with $x$,
		by \define{$\pi_x$},
		hence homeomorphism denoted by \define{$x\mapsto \pi_x$}
	\eit
\end{mydefinition}

\bit
\item
	obtain mapping from such operation, $G\times S \to S$, with $(x,s)\mapsto \pi_x(s)$

\item
	often abbreviate $\pi_x(s)$ by $xs$, with which the following two properties satisfied
	\bit
	\item
		$ \left( \forall x,y\in G, s\in S \right) \left( x(ys) = (xy)s \right) $
	\item
		$ \left( \forall s\in S \right) \left( es = s \right) $
	\eit

\item
	conversely, for mapping $G\times S\to S$ with $(x,s)\mapsto xs$ satisfying above two properties,
	$s\mapsto xs$ is permutation for $x\in G$,
	hence $\pi_x$ is homeomorphism of $G$ into \perm{S}\

\item
	thus, operation of $G$ on $S$ can be defined as mapping $S\times G\to S$ satisfying above two properties\
\eit


\myfoilhead{Conjugation}

\begin{mydefinition}{conjugation of groups}%
		\index{conjugation!group}%
		\index{group!conjugation}
	for group $G$ and map $\gamma_x:G\to G$ with $\gamma_x(y) = xyx^{-1}$,
	homeomorphism
	$$
		G \to \aut{G} \mbox{ defined by } x\mapsto \gamma_x
	$$
	called \define{conjugation},
	which is operation of $G$ on itself\
\end{mydefinition}

\bit
\item
	$\gamma_x$, called \define{inner}%
		\index{inner!group}%
		\index{group!inner}

\item
	kernel of conjugation is \emph{center of $G$}

\item
	to avoid confusion, instead of writing $xy$ for $\gamma_x(y)$, write
	$$
		\gamma_x(y) = xyx^{-1} = \prescript{x}{}{y}
		\mbox{ and }
		\gamma_{x^{-1}}(y) = x^{-1}yx = {y}^x
	$$

\item
	for subset $A\subset G$,
	map $(x,A) \mapsto xAx^{-1}$
	is operation of $G$ on set of subsets of $G$

\item
	similarly for subgroups of $G$

\item
	two subsets of $G$, $A$ and $B$ with $B= x A x^{-1}$ for some $x\in G$,
	said to be \define{conjugate}%
		\index{conjugate!group}%
		\index{group!conjugate}
\eit


\myfoilhead{Translation}

\begin{mydefinition}{translation}%
		\index{translation!group}%
		\index{group!translation}
	operation of $G$ on itself defined by map
	$$
		(x,y) \mapsto xy
	$$
	called \define{translation},
	denoted by \define{$T_x:G \to G$}
	with $T_x(y) = xy$
\end{mydefinition}

\bit
\vitem
	for subgroup $H\subset G$,
	$T_x(H) = xH$ is left coset
	\bit
	\vitem
		denote set of left cosets also by $G/H$ even if $H$ is not normal
	\vitem
		denote set of right cosets also by $H\backslash G$
	\eit

\vitem examples of translation
	\ibit
	\vitem
		$G=GL(V)$, group of linear automorphism of vector space with field $F$,
		for which, map $(A,v)\mapsto Av$ for $A\in G$ and $v\in V$
		defines operation of $G$ on $V$
		\bit
		\iitem
			$G$ is subgroup of group of permutations, \perm{V}\
		\eit
	\vitem
		for $V=F^n$, $G$ is group of nonsingular $n$-by-$n$ matrices
	\eit
\eit
\vfill


\myfoilhead{Isotropy}

\begin{mydefinition}{isotropy}%
		\index{isotropy!group}%
		\index{group!isotropy}
	for operation of group $G$ on set $S$
	$$
		\set{x\in G}{xs = s}
	$$
	called \define{isotropy of $G$},
	denoted by \define{$G_s$},
	which is subgroup of $G$\
\end{mydefinition}

\bit
\vitem
	for conjugation operation of group $G$,
	$G_s$ is normalizer of $s\in G$
\vitem
	isotropy groups are conjugate,
	\eg, for $s,s'\in S$ and $y\in G$ with $ys=s'$,
	$$
		G_{s'} = yG_s y^{-1}
	$$
\vitem
	by definition, kernel of operation of $G$ on $S$ is
	$$
		K = \bigcap_{s\in S} G_s \subset G
	$$
\vitem
	operation with trivial kernel, said to be \define{faithful}%
		\index{group!operation!faithful}%
		\index{operation!group!faithful}
\vitem
	$s\in G$ with $G_s = G$, called \define{fixed point}%
		\index{group!operation!fixed points}%
		\index{operation!group!fixed points}%
		\index{fixed points!group!operation}
\eit


\myfoilhead{Orbits of operation}

\begin{mydefinition}{orbits of operation}%
		\index{operation!group!orbits}%
		\index{group!operation!orbits}%
		\index{orbits!group!operation}
	for operation of group $G$ on set $S$,
	\set{xs}{x\in G},
	called \define{orbit of $s$ under $G$},
	denoted by \define{$Gs$}\
\end{mydefinition}

\bit
\item
	for $x,y\in G$ in same coset of $G_s$, $xs = ys$, \ie\
	$
		\left(
			\exists z\in G
		\right)
		\left(
			x,y \in zG_s
		\right)
		\Leftrightarrow
		xs = ys
	$
\item hence, mapping $G/G_s \to S$ with $x \mapsto x G_s$
	is morphism of $G$-sets, thus
\eit

\begin{myproposition}{}
	for group $G$, operating on set $S$ and $s\in S$,
	order of orbit $Gs$ is equal to index $(G:G_s)$
\end{myproposition}

\vfill
\begin{myproposition}{}
	for subgroup $H$ of group $G$,
	number of conjugate subgroups to $H$
	is index of normalizer of $H$ in $G$\
\end{myproposition}

\vfill
\begin{mydefinition}{transitive operation}%
		\index{group!operation!transitive}%
		\index{operation!group!transitive}%
		\index{transitive!group!operation}
	operation with one orbit, said to be \define{transitive}
\end{mydefinition}

\vfill


\myfoilhead{Orbit decomposition and class formula}
\bit
\item
	orbits are disjoint
	$$
		S = \coprod_{\lambda \in \Lambda} Gs_\lambda
	$$
	where $s_\lambda$ are elements of distinct orbits
\eit

\begin{myformula}{orbit decomposition formula}%
	\index{orbit decomposition formula!group}
	\index{group!orbit decomposition formula}
	for group $G$ operating on set $S$,
	index set $\Lambda$ whose elements represent distinct orbits\
	$$
		|S| = \sum_{\lambda \in \Lambda} (G:G_\lambda)
	$$
\end{myformula}

\begin{myformula}{class formula}%
	\index{class formula!group}%
	\index{group!class formula}
	for group $G$ and set $C\subset G$ whose elements represent distinct conjugacy classes\
	$$
		(G:1) = \sum_{x\in C} (G:G_x)
	$$
\end{myformula}


\myfoilhead{Sylow subgroups}

\begin{mydefinition}{sylow subgroups}%
		\index{sylow subgroup!group}%
		\index{group!sylow subgroup}
	for prime number $p$,
	finite group with order $p^n$ for some $n\geq0$, called \define{$p$-group};
	subgroup $H\subset G$ of finite group $G$ with order $p^n$ for some $n\geq0$, called \define{$p$-subgroup};
	subgroup of order $p^n$ where $p^n$ is highest power of $p$ dividing order of $G$,
	called \define{$p$-Sylow subgroup}%
		\index{$p$-group!group}%
		\index{group!$p$-group}%
		\index{$p$-subgroup!group}%
		\index{group!$p$-subgroup}
\end{mydefinition}

\begin{mylemma}{}
	finite abelian group of order divided by prime number $p$
	has subgroup of order $p$
\end{mylemma}

\begin{mytheorem}{$p$-Sylow subgroups of finite groups}
	finite group of order divided by prime number $p$
	has $p$-Sylow subgroup
\end{mytheorem}

\begin{mylemma}{number of fixed points of group operations}
	for $p$-group $H$, operating on finite set $S$\
	\theoremslikepostvspace
	\ibit
	\iitem
		number of fixed points of $H$ is congruent to size of $S$ modulo $p$,
		\ie\
%
%		\begin{eqn}
		$$
%		$
			\mbox{\# fixed points of }H \equiv |S| \Mod{p}
%		$
		$$
%		\end{eqn}

	\iitem
		if $H$ has exaxctly one fixed point,
%		$$
		$
			|S| \equiv 1\Mod{p}
		$
%		$$
	\iitem
		if $p$ divides $|S|$,
%		$$
		$
			|S| \equiv 0\Mod{p}
		$
%		$$
	\eit
\end{mylemma}


\myfoilhead{Sylow subgroups and solvability}

\begin{mytheorem}{solvability of finite $p$-groups}
	finite $p$-group is solvable;
	if it is non-trivial,
	it has non-trivial center
\end{mytheorem}

\begin{mycorollary}{}
	for non-trivial $p$-group,
	exists sequence of subgroups\
	$$
		\{e\} = G_0
		\subset G_1
		\subset G_2
		\subset \cdots
		\subset G_n
		=G
	$$
	where $G_i$ is normal in $G$
	and $G_{i+1}/G_i$ is cyclic group of order $p$\
\end{mycorollary}

\begin{mylemma}{normality of subgroups of order $p$}
	for finite group $G$ and smallest prime number dividing order of $G$ $p$,
	every subgroup of index $p$ is normal
\end{mylemma}

\begin{myproposition}{solvability of groups of order $pq$}
	group of order $pq$ with $p$ and $q$ being distinct prime numbers,
	is solvable
\end{myproposition}

\bit
\vitem now can prove following
	\bit
	\vitem
		group of order, $35$, is solvable
		- implied by \propositionname~\ref{proposition:finite solvable groups}
				and \propositionname~\ref{proposition:properties of cyclic groups}
	\vitem
		group of order less than $60$ is solvable
	\eit
\eit


\yesnoexec{\showincomplete}{%
\myfoilhead{Direct sums and free abelian groups}
}{}

\yesnoexec{\showincomplete}{%
\myfoilhead{Finitely generated abelian groups}
}{}


\yesnoexec{\showincomplete}{%
\myfoilhead{The dual group}
}{}


\yesnoexec{\showincomplete}{%
\myfoilhead{Inverse limit and completion}
}{}


\yesnoexec{\showincomplete}{%
\myfoilhead{Categories and functors}
}{}


\yesnoexec{\showincomplete}{%
\myfoilhead{Free groups}
}{}


\yesnoexec{\showincomplete}{%
\myfoilhead{Dihedral groups}

\begin{mydefinition}{dihedral groups}%
	\index{group!dihedral group}
	for $n\geq3$,
	rigid movement of regular $n$-gon
	using symmetries and rotations (in arbitrary orders for arbitrary number of times)
	such that result fits into original regular $n$-gon,
	called \define{dihedral group},
	denoted by \define{$D_{2n}$}\
\end{mydefinition}

\bit
\item XXX
\eit
}{}


\titlefoil{Rings}{Rings}

\myfoilhead{Rings}

\begin{mydefinition}{ring}
	set $A$ together with two laws of composition called multiplication and addition
	which are written as product and sum respectively, satisfying following conditions,
	called \define{ring}%
		\index{addition!ring}%
		\index{ring!addition}%
		\index{multiplication!ring}%
		\index{ring!multiplication}
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		$A$ is commutative group \wrt\ addition
			- unit element denoted by $0$\
%			(refer to \definitionname~\ref{definition:groups})

	\iitem
		$A$ is monoid \wrt\ multiplication
			- unit element denoted by $1$\
%			(refer to \definitionname~\ref{definition:monoids})

	\iitem
		multiplication is distributive
		over addition,
		\ie\
%
%		\begin{eqn}
		$$
			\left(
				\forall x, y, z \in A
			\right)
			\left(
				(x+y)z = xz + yz
				\mbox{ \& }
				z(x+y) = zx + zy
			\right)
		$$
%		\end{eqn}

	\item []
		do not assume $1\neq 0$
	\eit
\end{mydefinition}

\shrinkspacewithintheoremslike
\bit
\item
	can prove, \eg,
	\bit
	\item
		$\left( \forall x \in A \right) \left( 0x = 0 \right)$
		because
		$0x + x = 0x + 1x = (0+1)x = 1x = x$

	\item
		if $1=0$, $A=\{0\}$
		because
		$x = 1x = 0x = 0$

	\item
		$\left( \forall x,y\in A \right) \left( (-x)y = -(xy) \right)$
		because
		$xy + (-x)y = (x+ -x)y = 0y = 0$
%
%	\item
%		$\left( \forall x,y\in A \right) \left( (-x)(-y) = xy \right)$
%		$\because (-x)(-y) + (-(xy)) = (-x)(-y) + (-x)y = (-x)(-y + y) = (-x)0 = 0$
	\eit
\eit

\begin{mydefinition}{subring}%
		\index{subring!ring}%
		\index{ring!subring}
	subset of ring which itself is ring with same additive and multiplicative laws of composition,
	called \define{subring}\
\end{mydefinition}


\myfoilhead{More on ring}

\begin{mydefinition}{multiplicative group of invertible elements of ring}%
	\index{multiplicative group of invertible elements of ring}%
	\index{ring!multiplicative group of invertible elements of ring}%
	\index{group of units!ring}%
	\index{group of invertible elements!ring}%
	\index{ring!group of units}%
	\index{ring!group of invertible elements}%
	\index{ring!units}%
	\index{units!ring}\
	subset $U$ of ring $A$
	such that every element of $U$ has both left and right inverses,
	called \define{group of units of $A$} or \define{group of invertible elements of $A$},
	sometimes denoted by \define{$A^\ast$}%
\end{mydefinition}

\begin{mydefinition}{division ring}%
		\index{ring!division ring}
	ring with $1\neq0$ and every nonzero element being invertible,
	called \define{division ring}\
\end{mydefinition}

\begin{mydefinition}{commutative ring}%
		\index{ring!commutative}
	ring $A$ with $\left( \forall x,y \in A \right) \left( xy= yx \right)$,
	called \define{commutative ring}\
\end{mydefinition}

\vfill
\begin{mydefinition}{center of ring}%
		\index{center!of ring}%
		\index{ring!center of}
	subset $C\subset A$ of ring $A$ such that
	$$
		C= \set{a\in A}{\forall x \in A, xa = ax}
	$$
	is \emph{subring},
	and
	is called \define{center of ring $A$}
\end{mydefinition}


\myfoilhead{Fields}

\begin{mydefinition}{field}
	commutative division ring, called \define{field}
\end{mydefinition}


\myfoilhead{General distributivity}

\bit
\item
	general distributivity
	- for ring $A$, $\seq{x_i}_{i=1}^n\subset A$ and $\seq{y_i}_{i=1}^n\subset A$
	$$
		\left(
			\sum x_i
		\right)
		\left(
			\sum y_j
		\right)
		=
		\sum_i \sum_j x_iy_j
	$$
\eit


\myfoilhead{Ring examples}

\bit
\item
	for set $S$ and ring $A$,
	\cemph{set of all mappings of $S$ into $A$ $\Map(S,A)$}
	whose addition and multiplication are defined as below,
	is \emph{ring}
	\proofref{set of functions into ring is ring}
	\begin{eqnarray*}
		&
		\left(
			\forall f,g\in \Map(S,A)
		\right)
		\left(
			\forall x\in S
		\right)
		\left(
			(f+g)(x) = f(x)+g(x)
		\right)
		&
		\\
		&
		\left(
			\forall f,g\in \Map(S,A)
		\right)
		\left(
			\forall x\in S
		\right)
		\left(
			(fg)(x) = f(x)g(x)
		\right)
		&
	\end{eqnarray*}
	\bit
	\iitem
		additive and multiplicative unit elements of $\Map(S,A)$
		are constant maps whose values are
		additive and multiplicative unit elements of $A$
		respectively
	\eit
	\bit
	\vitem
		$\Map(S,A)$ is commutative \iaoi\ $A$ is commutative
	\vitem
		for set $S$, $\Map(S,\reals)$
		(page~\pageref{page:Notations})
		is a commutative ring
	\eit

\vitem
	for abelian group $M$,
	\cemph{set $\End(M)$ of group homeomorphisms of $M$ into itself}
	is \emph{ring} with normal addition and mapping composition as multiplication
	\proofref{set of group endomorphisms is ring}
	\bit
	\iitem
		additive and multiplicative unit elements of $\End(M)$
		are constant map whose value is the unit element of $M$
		and identity mapping
		respectively
	\eit
	\bit
	\vitem
		not commutative in general
	\eit

\vitem
	for ring $A$,
	\cemph{set $A[X]$ of polynomials over $A$}
	is \emph{ring},
	(\definitionname~\ref{definition:polynomial})

\vitem
	for field $K$,
	$K^{n\times n}$,
	\ie,
	set of $n$-by-$n$ matrices with components in $K$,
	is \emph{ring}
	\bit
	\vitem
		$\left(K^{n\times n}\right)^\ast$,
		\ie,
		multiplicative group of units of $K^{n\times n}$,
		consists
		of non-singular matrices,
		\ie,
		those whose determinants are nonzero
	\eit
\eit
\vfill


\myfoilhead{Group ring}

\begin{mydefinition}{group ring}%
		\index{group ring!ring}%
		\index{ring!group ring}
	for group $G$ and field $K$,
	set of all formal linear combinations
	$
		\sum_{x\in G} a_x x
	$
	with $a_x\in K$ where $a_x$ are zero except finite number of them
	where addition is defined normally
	and multiplication is defined as
	$$
		\left(
			\sum_{x\in G} a_x x
		\right)
		\left(
			\sum_{y\in G} b_y y
		\right)
		=
		\sum_{z\in G}
		\left(
			\sum_{xy=z} a_xb_y xy
		\right)
	$$
	called \define{group ring},
	denoted by \define{$K[G]$}
%	-
%	$\sum_{xy=z} a_xb_y$ above
%	defines what is called \define{convolution product}
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		$\sum_{xy=z} a_xb_y$ above
		defines what is called \define{convolution product}
	\eit
\end{mydefinition}


\myfoilhead{Convolution product}

\begin{mydefinition}{convolution product}%
		\index{convolution product!ring}%
		\index{ring!convolution product}
	for two functions $f,g$ on group $G$,
	\define{convolution (product)},
	denoted by \define{$f\ast g$},
	defined by
	$$
		(f\ast g)(z) =
		\sum_{xy=z} f(x)f(y)
	$$
	as function on group $G$
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		one may restrict this definition to functions which are $0$ except at finite number of elements
	\eit
\end{mydefinition}

\bit
\item
	for $f,g\in L^1(\reals)$, can define \emph{convolution product} $f\ast g$ by
	$$
		(f\ast g) (x) = \int_{\reals} f(x-y)g(y)dy
	$$
	\bit
	\vitem
		satisfies all axioms of ring except that there is not unit element
	\vitem
		commutative (essentially because $\reals$ is commutative)
	\eit
\vitem
	more generally,
	for locally compact group $G$ wiht Haar measure $\mu$,
	can define \emph{convolution product}
	by
	$$
		(f\ast g) (x) = \int_{G} f(xy^{-1})g(y)d\mu(y)
	$$
\eit


\myfoilhead{Ideals of ring}

\begin{mydefinition}{ideal}
	subset \ideal{a}\ of ring $A$ which is subgroup of additive group of $A$
	with $A\ideal{a}\subset \ideal{a}$,
	called \define{left ideal};%
		\index{left ideal!of ring}%
		\index{ring!ideal!left ideal}%
		\index{ideal!of ring!left}
	indeed, $A\ideal{a} = \ideal{a}$ because $A$ has $1$;
	\define{right ideal} can be similarly defined, \ie, $\ideal{a} A = \ideal{a}$;%
		\index{right ideal!of ring}%
		\index{ring!ideal!right ideal}\
		\index{ideal!of ring!right}\
	subset which is both left and right ideal, called \define{two-sided ideal}
	or simply \define{ideal}%
		\index{two-sided ideal!of ring}%
		\index{ring!ideal!two-sided ideal}%
		\index{ideal!of ring!two-sided}%
		\index{ideal!of ring}%
		\index{ring!ideal}\
\end{mydefinition}

\bit
\item
	for ring $A$,
	$(0)$ are $A$ itself area ideals
\eit

\begin{mydefinition}{principal ideal}%
		\index{principal ideal}%
		\index{ring!principal ideal}\
	for ring $A$ and $a\in A$, left ideal $Aa$, called \define{principal left ideal}
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		$a$, said to be generator of $\ideal{a}=Aa$ (over $A$)
	\eit
\end{mydefinition}

\begin{mydefinition}{principal two-sided ideal}%
		\index{principal two-sided ideal}%
		\index{ring!principal two-sided ideal}\
	$AaA$, called \define{principal two-sided ideal}
	where
	$$
		AaA
		=
		\bigcup_{i=1}^\infty \bigsetl{\sum_{i=1}^n x_i a y_i}{x_i,y_i\in A}
	$$
\end{mydefinition}

\begin{mylemma}{ideals of field}
\index{mylemma}{ideals of field}
only ideals of field
are the field itself and zero ideal
\end{mylemma}


\myfoilhead{Principle rings}

\begin{mydefinition}{principal ring}%
		\index{ring!principal}\
	commutative ring of which every ideal is principal and $1\neq0$,
	called \define{principal ring}
\end{mydefinition}

\bit
\vitem
	$\integers$ (set of integers)
	is \emph{principal} ring
	\proofref{nonzero ideals of integers are principal}
	\pagelabel{page:nonzero ideals of integers are principal}

\vitem
	$k[X]$ (ring of polynomials) for field $k$
	is \emph{principal} ring

\vitem
	ring of algebraic integers in number field $K$
	is \emph{not} necessarily principal
	\bit
	\vitem
		let \ideal{p}\ be prime ideal,
		let $R_\ideal{p}$
		be ring of all elements $a/b$
		with $a,b\in R$ and $b\not\in\ideal{p}$,
		then
		$R_\ideal{p}$ is principal,
		with one prime ideal $\ideal{m}_\ideal{p}$
		consisting of all elements $a/b$ as above
		but with $a\in\ideal{p}$
	\eit

\vitem
	let $A$
	be set of entire functions on complex plane,
	then $A$ is commutative ring,
	and every finitely generated ideal is \emph{principal}
	\bit
	\vitem
		given discrete set of complex numbers $\{z_i\}$
		and nonnegative integers $\{m_i\}$,
		exists entire function $f$
		having zeros at $z_i$ of multiplicity $m_i$
		and \emph{no} other zeros
	\vitem
		every principal ideal is of form $Af$ for some such $f$
	\vitem
		group of units $A^\ast$ in $A$
		consists of functions having no zeros
	\eit

\eit


\myfoilhead{Ideals as both additive and multiplicative monoids}

\bit
\item
	ideals form additive monoid
	\bit
	\vitem
		for left ideals \ideal{a}, \ideal{b}, \ideal{c}\ of ring $A$,
		$\ideal{a}+\ideal{b}$ is left ideal,
		$(\ideal{a}+\ideal{b})+\ideal{c} =\ideal{a}+(\ideal{b}+\ideal{c})$,
		hence form additive monoid with $(0)$ as the unit elemenet
	\vitem
		similarly
		for right ideals \& two-sided ideals
	\eit

\vitem
	ideals form multiplicative monoid
	\bit
	\vitem
		for left ideals \ideal{a}, \ideal{b}, \ideal{c}\ of ring $A$,
		define $\ideal{a}\ideal{b}$ as
		$$
			\ideal{a}\ideal{b}
			=
			\bigcup_{i=1}^\infty \bigsetl{\sum_{i=1}^n x_i y_i}{x_i \in \ideal{a},y_i\in \ideal{b}}
		$$
		then $\ideal{a}\ideal{b}$ is also left ideal,
		$(\ideal{a}\ideal{b})\ideal{c} =\ideal{a}(\ideal{b}\ideal{c})$,
		hence form multiplicative monoid with $A$ itself as the unit elemenet;
		for this reason,
		this unit element $A$, \ie, the ring itself, often written as $(1)$
	\vitem
		similarly
		for right ideals \& two-sided ideals
	\eit

\vitem
	ideal multiplication is also distributive over addition

\vitem
	however, set of ideals does \cemph{not} form ring
	(because the additive monoid is \emph{not} group)
\eit


\yesnoexec{\showincomplete}{
\myfoilhead{Dedekind ring}%
	\index{Dedekind ring}%
	\index{ring!Dedekind ring}

\bit
	\item XXX Lang p88\idxtodo{2 - Dedekind}
\eit
}{}


\myfoilhead{Generators of ideal}

\begin{mydefinition}{generators of ideal}%
		\index{generators of ideal!of ring}%
		\index{ring!generators of ideal}%
		\index{ideal!of ring!generators of}\
	for ring $A$ and $a_1,\ldots,a_n\subset A$, set of elements of $A$ of form
	$$
		\sum_{i=1}^n x_i a_i
	$$
	with $x_i \in A$, is left ideal,
	denoted by \define{$(a_1,\ldots,a_n)$},
	called \define{generators} of the left ideal;%
	\index{generated by!ring!ideal}%
	\index{ring!generated by ideal}\
	similarly for right ideals
\end{mydefinition}

\bit
\item
	above equal to smallest ideals containing $a_i$, \ie,
	intersection of all ideals containing $a_i$
	$$
		\cap_{a_1,\ldots, a_n\in\ideal{a}} \ideal{a}
	$$
	\proofref{ideal generated by elements of ring}
	-
	just like set ($\sigma$-)algebras in set theory \foilref{Algebras generated by subsets}
\eit


\myfoilhead{Entire rings}

\begin{mydefinition}{zero divisor}%
		\index{zero divisor!ring}%
		\index{ring!zero divisor}\
	for ring $A$,
	$x,y\in A$ with $x\neq0$, $y\neq0$, and $xy=0$,
	said to be \define{zero divisors}\
\end{mydefinition}

\vfill
\begin{mydefinition}{entire ring}%
		\index{entire ring}%
		\index{ring!entire}\
	commutative ring with no zero divisors for which $1\neq0$,
	said to be \define{entire};
	entire ring, sometimes called \define{integral domain}%
		\index{integral domain}%
		\index{entire ring!integral domain}
\end{mydefinition}

\vfill
\begin{mylemma}{every field is entire ring}
	every field is entire ring
\end{mylemma}
\vfill


\myfoilhead{Ring-homeomorphism}

\begin{mydefinition}{ring-homeomorphism}%
		\index{ring-homeomorphism}%
		\index{ring!ring-homeomorphism}%
		\index{homeomorphism!ring-homeomorphism}\
	mapping of ring into ring $f:A\to B$
	such that
	$f$ is monoid-homeomorphism for both additive and multiplicative structure on $A$ and $B$,
	\ie,
	$$
		\left(
			\forall
			a, b \in A
		\right)
		\left(
			f(a+b) = f(a) + f(b) \mbox{ \& } f(ab) = f(a)f(b)
		\right)
	$$
	and
	$$
		f(1)=1 \mbox{ \& } f(0)=0
	$$

	called \define{ring-homeomorphism};
	\define{kernel}, defined to be kernel of $f$
	viewed as additive homeomorphism%
		\index{kernel!ring-homeomorphism}%
		\index{ring-homeomorphism!kernel}%
		\index{ring!ring-homeomorphism!kernel}
\end{mydefinition}

\bit
\vitem
	\emph{kernel of ring-homeomorphism} $f:A\to B$
	is ideal of $A$
	\proofref{kernel of ring-homeomorphism is ideal}
\vitem
	conversely, for ideal \ideal{a}, can construct factor ring $A/\ideal{a}$
\vitem
	simply say ``homeomorphism'' if reference to ring is clear
\eit

\begin{myproposition}{injectivity of field homeomorphism}%
	\index{field homeomorphism}%
	\index{field homeomorphism!injectivity}%
	\index{ring homeomorphism!field homeomorphism}\
	ring-homeomorphism from field into field
	is injective
	(due to \lemmaname~\ref{lemma:ideals of field})
\end{myproposition}
\vfill


\myfoilhead{Factor ring and canonical map}

\begin{mydefinition}{factor ring and residue class}%
	\index{factor ring!ring}%
	\index{ring!factor ring}%
	\index{residue class!ring}%
	\index{ring!residue class}\
	for ring $A$ and an ideal $\ideal{a} \subset A$,
	set of cosets $x+\ideal{a}$ for $x\in A$
	combined with \emph{addition} defined by viewing $A$ and \ideal{a}\ as additive groups,
	\emph{multiplication} defined by
	$
		(x+\ideal{a})
		(y+\ideal{a})
		=
		xy+\ideal{a},
	$
	which satisfy all requirements for ring,
	called \define{factor ring} or \define{residue class ring},
	denoted by \define{$A/\ideal{a}$};
	cosets in $A/\ideal{a}$,
	called \define{residue classes modulo \ideal{a}},
	and each coset $x+\ideal{a}$
	called \define{residue class of $x$ modulo \ideal{a}}\
\end{mydefinition}

\bit
\item
	for ring $A$ and ideal \ideal{a}
	\bit
	\item
		for subset $S\subset \ideal{a}$, write $S \equiv 0 \Mod{\ideal{a}}$
	\vitem
		for $x,y\in A$, if $x-y\in\ideal{a}$, write $x \equiv y \Mod{\ideal{a}}$
	\vitem
		if $\ideal{a} = (a)$ for $a\in A$,
		for $x,y\in A$, if $x-y\in\ideal{a}$, write $x \equiv y \Mod{{a}}$
	\eit
\eit

\begin{mydefinition}{canonical map of ring}%
		\index{canonical map!ring}%
		\index{ring!canonical map}\
	ring-homeomorphism of ring $A$ into factor ring $A/\ideal{a}$
	$$
		A \to A/\ideal{a}
	$$
	called \define{canonical map of $A$ into $A/\ideal{a}$}
\end{mydefinition}


\myfoilhead{Factor ring induced ring-homeomorphism}

\begin{myproposition}{factor ring induced ring-homeomorphism}%
		\index{ring!factor ring induced ring-homeomorphism}\
	for ring-homeomorphism $g:A\to A'$
	whose kernel contains ideal \ideal{a},
	exists unique ring-homeomorphism $g_\ast:A/\ideal{a} \to A'$
	making diagram in \figref{factor-ring-induced-ring-homeomorphism}
	commutative,
	\ie,
	$g^\ast \circ f = g$
	where $f$ is the ring canonical map
	$f:A\to A/\ideal{a}$
\end{myproposition}

\vfill
\begin{figure}
\begin{center}
%\fbox{
	\factorring{6em}
%}
\end{center}
\idxfig{factor-ring-induced-ring-homeomorphism}
\label{fig:factor-ring-induced-ring-homeomorphism}
\end{figure}

%\vfill
%\begin{figure}
%	$$\diagtrirud{A}{g}{A'}{g_\ast}{A/\ideal{a}}{f}$$
%	\idxfig{factor-ring-induced-ring-homeomorphism}
%	\label{fig:factor-ring-induced-ring-homeomorphism}
%\end{figure}

\bit
\vitem
	the
	ring canonical map $f:A\to A/\ideal{a}$
	is universal in category of homeomorphisms
	whose kernel contains \ideal{a}\
\eit
\vfill


\myfoilhead{Prime ideal and maximal ideal}

\begin{mydefinition}{prime ideal}%
		\index{ring!prime ideal}%
		\index{prime ideal!of ring}
		\index{ideal!of ring!prime}

	for commutative ring $A$,
	ideal $\ideal{p}\neq A$ with $A/\ideal{p}$ entire,
	called \define{prime ideal} or just \define{prime};
\end{mydefinition}

\bit
\item
	equivalently,
	ideal $\ideal{p}\neq A$ is \define{prime}
	\iaoi\
	$
		\left(
			\forall x,y \in A
		\right)
		\left(
			xy \in \ideal{p}
			\Rightarrow
			x \in \ideal{p}
			\mbox{ or }
			y \in \ideal{p}
		\right)
	$
\eit

\begin{mydefinition}{maximal ideal}%
		\index{ring!maximal ideal}%
		\index{maximal ideal!of ring}
		\index{ideal!of ring!maximal}

	for commutative ring $A$,
	ideal $\ideal{m}\neq A$ such that
	$$
		\left(
			\forall \mbox{ ideal } \ideal{a} \subset A
		\right)
		\left(
			\ideal{m} \subset \ideal{a} \Rightarrow \ideal{a} = A
		\right)
	$$
	called \define{maximal ideal}
\end{mydefinition}

\begin{mylemma}{properties of prime and maximal ideals}
	for commutative ring $A$%
	\index{maximal ideal!properties}%
	\index{prime ideal!properties}%
	\index{ring!maximal ideal!properties}%
	\index{ring!prime ideal!properties}%
	\index{ring!ideal!maximal}%
	\index{ring!ideal!prime}

\shrinkspacewithintheoremslike
\ibit
\iitem
	every maximal ideal is {prime}
\iitem
	every ideal is contained in some maximal ideal
\iitem
	ideal $\{0\}$ is prime \iaoi\ $A$ is entire
\iitem
	ideal \ideal{m} is maximal \iaoi\ $A/\ideal{m}$ is field
\iitem
	inverse image of prime ideal of commutative ring homeomorphism
	is prime
\eit
\end{mylemma}


\myfoilhead{Embedding of ring}

\begin{mydefinition}{ring-isomorphism}
	bijective ring-homeomorphism (\definitionname~\ref{definition:ring-homeomorphism}) is isomorphism
\end{mydefinition}

\shrinkspacewithintheoremslikehalf
\bit
\item
	indeed,
	for bijective ring-isomorphism $f:A\to B$,
	exists set-theoretic inverse $g:B\to A$ of $f$,
	which is ring-homeomorphism
\eit

\begin{mylemma}{image of ring-homeomorphism is subring}
	image $f(A)$ of ring-homeomorphism $f:A\to B$
	is subring of $B$
	\proofref{image of ring-homeomorphism is subring}
\end{mylemma}

\begin{mydefinition}{embedding of ring}%
		\index{embedding!ring}%
		\index{ring!embedding}\
	ring-isomorphism between $A$ and its image,
	established by injective ring-homeomorphism $f:A\to B$,
	called \define{embedding of ring}
\end{mydefinition}

\begin{mydefinition}{induced injective ring-homeomorphism}%
		\index{ring!induced injective ring-homeomorphism}\
	for ring-homeomorphism $f:A\to A'$
	and ideal $\ideal{a}'$ of $A'$,
	injective ring-homeomorphism
	$$
		A/f^{-1}(\ideal{a}') \to A'/\ideal{a}'
	$$
	called \define{induced injective ring-homeomorphism}
\end{mydefinition}


\myfoilhead{Characteristic of ring}

\bit
\item
	for ring $A$,
	consider ring-homeomorphism

	$$
		\lambda:\integers \to A
	$$

	such that
	$$
		\lambda(n) = ne
	$$

	where $e$ is multiplicative unit element of $A$
	\bit
	\vitem
		kernel of $\lambda$ is ideal $(n)$
		for some $n\geq0$,
		\ie,
		ideal generated by some nonnegative integer $n$
	\vitem
		hence, canonical injective ring-homeomorphism $\integers/n\integers \to A$,
		which is ring-isomorphism between $\integers/n\integers$ and subring of $A$
	\vitem
		when $n\integers$ is prime ideal,
		exist two cases; either $n=0$ or $n=p$ for prime number $p$
	\eit
\eit

\begin{mydefinition}{characteristic of ring}%
		\index{ring!characteristic}%
		\index{characteristic!ring}\
	ring $A$ with $\{0\}$ as prime ideal kernel above,
	said to have \define{characteristic $0$};
	if prime ideal kernel is $p\integers$ for prime number $p$,
	$A$,
	said to have \define{characteristic $p$},
	in which case,
	$A$ contains (isomorphic image of) $\integers/p\integers$ as subring,
	abbreviated by \define{\primefield{p}}\
\end{mydefinition}


\myfoilhead{Prime fields and prime rings}

\bit
\item
	field $K$ has characteristic $0$ or $p$ for prime number $p$%
		\index{field!characteristic}%
		\index{characteristic!field}
\vitem
	$K$ contains as subfield (isomorphic image of)%
	\index{field!isomorphic image of $\rationals$ or \primefield{p}}
	\bit
	\vitem
		$\rationals$ if characteristic is $0$
	\vitem
		\primefield{p} if characteristic is $p$
	\eit
\eit

\vfill
\begin{mydefinition}{prime field}%
	\index{prime field}%
	\index{field!prime}\
	in above cases,
	both $\rationals$ and \primefield{p},
	called \define{prime field (contained in $K$)};
	since prime field is smallest subfield of $K$
	containing $1$ having no automorphism other than identity,
	identify it with $\rationals$ or \primefield{p}\
	for each case
\end{mydefinition}

\begin{mydefinition}{prime ring}%
	\index{prime ring}%
	\index{ring!prime}\
	in above cases,
	\define{prime ring (contained in $K$)}
	means either integers $\integers$ if $K$ has characteristic $0$
	or \primefield{p}\ if $K$ has characteristic $p$\
\end{mydefinition}
\vfill


\myfoilhead{$\integers/n\integers$}%
	\index{$\integers/n\integers$}

\bit
\item
	$\integers$ is ring%
	\index{ring!integer}

\vitem
	every ideal of \integers\ is principal,
	\ie, either $\{0\}$ or $n\integers$ for some $n\in\naturals$
	(refer to page~\pageref{page:nonzero ideals of integers are principal})

\vitem
	ideal of \integers\ is prime \iaoi\ is $p\integers$ for some prime number $p\in\naturals$%
		\index{ring!of integers modulo $n$}%
		\index{ring!of integers modulo $n$!prime}
	\bit
	\vitem
		$p\integers$ is maximal ideal
	\eit
\eit

\vfill
\begin{mydefinition}{ring of integers modulo $n$}%
		\index{ring!of integers modulo $n$}%
		\index{modulo!ring of integers modulo $n$}
	$\integers/n\integers$, called \define{ring of integers modulo $n$};
	abbreviated as \define{$\mbox{mod }n$}\
\end{mydefinition}

\bit
\vitem
	$\integers/p\integers$ for prime $p$
	is \emph{field} and denoted by \define{\primefield{p}}%
		\index{prime!field}%
		\index{field!prime}
\eit
\vfill


\myfoilhead{Euler phi-function}

\begin{mydefinition}{Euler phi-function}%
		\index{Euler $\varphi$-function}%
		\index{Euler's totient function}%
		\index{Euler phi-function}%
		\index{Euler, Leonhard!$\varphi$-function}%
		\index{Euler, Leonhard!Euler's totient function}%
		\index{Euler, Leonhard!phi-function}
	for $n>1$,
	order of \hyperref[definition:divison rings]{divison ring}\ of $\integers/n\integers$,
	called \define{Euler phi-function},
	denoted by \define{$\varphi(n)$};
	if prime factorization of $n$ is
	$$
		n = p_1^{e_1} \cdots p_r^{e_r}
	$$
	with distinct $p_i$ and $e_i\geq1$
	$$
		\varphi(n)
		= p_1^{e_1-1} (p_1 - 1)
		\cdots
		p_r^{e_r-1} (p_r - 1)
	$$
\end{mydefinition}

\vfill
\begin{mytheorem}{Euler's theorem}%
		\index{Euler's theorem}%
		\index{Euler, Leonhard!Euler's theorem}
	for $x$ prime to $n$\
	$$
		x^{\varphi(n)} \equiv 1 \Mod{n}
	$$
\end{mytheorem}
\vfill


\myfoilhead{Chinese remainder theorem}

\begin{mytheorem}{Chinese remainder theorem}%
		\index{ring!Chinese remainder theorem}
	for ring $A$
	and
	$n$ ideals $\ideal{a}_1$, \ldots $\ideal{a}_n$ ($n\geq2$)
	with $\ideal{a}_i + \ideal{a}_j=A$ for all $i \neq j$
	$$
		\left(
			\forall
			x_1,\ldots, x_n \in A
		\right)
		\left(
			\exists x \in A
		\right)
		\left(
			\forall 1\leq i\leq n
		\right)
		\left(
			x \equiv x_i
			\Mod{\ideal{a}_i}
		\right)
	$$
\end{mytheorem}

\begin{mycorollary}{isomorphism induced by Chinese remainder theorem}%
		\index{ring!isomorphism induced by Chinese remainder theorem}
		\index{ring!Chinese remainder theorem!isomorphism induced by}
	for ring $A$,
	$n$ ideals $\ideal{a}_1$, \ldots $\ideal{a}_n$ ($n\geq2$)
	with $\ideal{a}_i + \ideal{a}_j=A$ for all $i \neq j$,
	and
	map of $A$ into product induced by canonical maps of $A$ onto $A/\ideal{a}_i$
	for each factor,
	\ie,
	$$
		f: A
		\to
		\prod A/\ideal{a}_i
	$$

	$f$ is surjective
	and
	$\Ker f = \bigcap \ideal{a}_i$,
	hence, exists isomorphism
	$$
		A/\cap \ideal{a}_i \isomorph \prod A/\ideal{a}_i
	$$
\end{mycorollary}


\myfoilhead{Isomorphism of endomorphisms of cyclic groups}

\begin{mytheorem}{isomorphism of endomorphisms of cyclic groups}%
		\index{ring!isomorphism of endomorphisms of cyclic groups}
	for cyclic group $A$ of order $n$,
	endomorphisms of $A$ into $A$
	with $x\mapsto kx$ for $k\in\integers$
	induce
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		ring isomorphism
		$$
			\integers/n\integers \isomorph \End(A)
		$$
	\iitem
		group isomorphism
		$$
			(\integers/n\integers)^\ast \isomorph \Aut(A)
		$$
	\eit
	where
	$(\integers/n\integers)^\ast$
	denotes group of units of
	$\integers/n\integers$
	(\definitionname~\ref{definition:multiplicative group of invertible elements of ring})
\end{mytheorem}

\bit
\vitem
	\eg,
%	for group of $n$-th roots of unity in \complexes, $\mu_n$,
	for group of $n$-th roots of unity in \complexes,
	all automorphisms are given by
	$$
		\xi \mapsto \xi^k
	$$
	for $k\in(\integers/n\integers)^\ast$
\eit


\myfoilhead{Irreducibility and factorial rings}

\begin{mydefinition}{irreducible ring element}%
		\index{irreducible element!ring!entire}%
		\index{ring!entire!irreducible element}%
	for entire ring $A$,
	non-unit non-zero element $a\in A$ with
	$$
		\left(
			\forall b, c\in A
		\right)
		\left(
			a = bc \Rightarrow b \mbox{ or } c \mbox{ is unit}
		\right)
	$$

	said to be \define{irreducible}
\end{mydefinition}

\vfill
\begin{mydefinition}{unique factorization into irreducible elements}%
		\index{unique factorization!ring!entire}
		\index{ring!entire!unique factorization}
	for entire ring $A$,
	element $a\in A$
	for which,
	exists unit $u$ and irreducible elements, $p_1$, \ldots, $p_r$ in $A$ such that
	$$
		a = u \prod p_i
	$$

	and this expression is unique up to permutation and multiplications by units,
	said to have \define{unique factorization into irreducible elements}
\end{mydefinition}

\vfill
\begin{mydefinition}{factorial ring}%
		\index{ring!entire!factorial}%
		\index{ring!factorial}%
	entire ring with every non-zero element has unique factorial into irreducible elements,
	called \define{factorial ring} or \define{unique factorization ring}
\end{mydefinition}


\myfoilhead{Greatest common divisor}

\begin{mydefinition}{devision of entire ring elements}%
		\index{ring!devision of elements}%
		\index{ring!entire!devision of elements}%
	for entire ring $A$ and nonzero elements $a,b\in A$,
	\define{$a$ said to divide $b$}
	if exists $c\in A$ such that $ac=b$,
	denoted by \define{$a|b$}
\end{mydefinition}

\begin{mydefinition}{greatest common divisor}%
		\index{greatest common divisor!ring}%
		\index{ring!greatest common divisor}
	for entire ring $A$ and $a,b\in A$,
	$d\in A$ which divides $a$ and $b$ and satisfies
%	$$
%		\left(
%			\forall c \in A \mbox{ with } c|a \mbox{ \& } c|b
%		\right)
%		\left(
%			c | d
%		\right)
%	$$
	$$
		\left(
			\forall c \in A
		\right)
		\left(
			c|a \mbox{ \& } c|b
			\Rightarrow
			c | d
		\right)
	$$
	called \define{greatest common divisor (g.c.d.) of $a$ and $b$}
\end{mydefinition}

\begin{myproposition}{existence of greatest common divisor of principal entire rings}%
		\index{greatest common divisor!principal entire ring}%
		\index{ring!greatest common divisor of principal entire ring}%
	for principal entire ring $A$
	and nonzero $a,b\in A$,
	$c\in A$ with $(a,b) = (c)$
	is g.c.d. of $a$ and $b$
\end{myproposition}

\begin{mytheorem}{principal entire ring is factorial}
	principal entire ring is factorial
\end{mytheorem}



\titlefoil{Polynomials}{Polynomials}%
	\index{polynomial}

\myfoilhead{Why (ring of) polynomials?}%
	\index{polynomial!ring}%
	\index{ring!polynomial}%
	\index{polynomial ring!ring}%
	\index{ring!polynomial ring}

\bit
\item
	lays ground work for polynomials in general

\vitem
	needs polynomials over arbitrary rings for diverse purposes
	\bit
	\vitem
		polynomials over finite field
		which cannot be identified with polynomial functions in that field%
		\index{polynomial!over field field}
	\vitem
		polynomials with integer coefficients;%
		\index{polynomial!with integer coefficients}
		reduce them mod $p$ for prime $p$
	\vitem
		polynomials over arbitrary commutative rings%
		\index{polynomial!over arbitrary commutative ring}

	\vitem
		rings of polynomial differential operators%
		\index{ring!of polynomial differential operators}
		for
		algebraic geometry \& analysis
	\eit
\vvitem
	\eg, ring learning with errors (RLWE)
		for cryptographic algorithms
\eit
\vfill


\myfoilhead{Ring of polynomials}

\bit
\item
	exist many ways to define polynomials over commutative ring;
%\item
	here's one
\eit

\begin{mydefinition}{polynomial}%
	\index{polynomial ring!polynomial}%
	\index{polynomial!polynomial ring}%
	\index{ring!polynomial}%
	for ring $A$,
	set of functions from monoid $S = \set{X^r}{r\in\integers, r\geq0}$
	into $A$
	which are equal to $0$
	except finite number of elements of $S$,
	called \define{polynomials over $A$},
	denoted by \define{$A[X]$}
\end{mydefinition}

\bit
\item
	for every $a\in A$,
	define function which has value $a$ on $X^n$, and value $0$ for every other element of $S$,
	by \define{$aX^r$}
\item
	then, \emph{a} polynomial can be uniquely written as
	$$
		f(X) = a_0X^0 + \cdots + a_nX^n
	$$
	for some $n\in\integers_+$,
	$a_i\in A$

\vitem
	$a_i$, called \define{coefficients of $f$}
\eit
\vfill


\myfoilhead{Polynomial functions}

\begin{mydefinition}{polynomial function}%
		\index{polynomia ringl!polynomial function}%
	for two rings $A$ and $B$ with $A\subset B$
	and $f\in A[X]$ with $f(X) = a_0 + a_1 X + \cdots + a_nX^n$,
	map $f_B: B\to B$ defined by
	$$
		f_B(x) = a_0 + a_1 x + \cdots + a_n x^n
	$$
	called \define{polynomial function associated with $f(X)$}
\end{mydefinition}

\begin{mydefinition}{evaluation homeomorphism}%
		\index{polynomial ring!evaluation homeomorphism}%
		\index{polynomial ring!substitution homeomorphism}%
	for two rings $A$ and $B$ with $A\subset B$ and $b\in B$,
	ring homeomorphism from $A[X]$ into $B$
	with association, $\ev_b:f\mapsto f(b)$,
	called \define{evaluation homeomorphism},
	said to be obtained by \define{substituting $b$ for $X$ in $f$}
\end{mydefinition}

\bit
\item
	hence, for $x\in B$, subring \define{$A[x]$} of $B$
	generated by $x$ over $A$ is
	ring of all polynomial values $f(x)$ for $f\in A[X]$
\eit

\begin{mydefinition}{variables and transcendentality}%
		\index{polynomial ring!variable}%
		\index{polynomial ring!transcendental}%
	for two rings $A$ and $B$ with $A\subset B$,
	if $x\in B$ makes evaluation homeomorphism $\ev_x:f\mapsto f(x)$ isomorphic,
	$x$, said to be \define{transcendental over $A$}
	or \define{variable over $A$}
\end{mydefinition}

\bit
\item
	in particular, \emph{$X$ is variable over $A$}
\eit


\myfoilhead{Polynomial examples}

\bit
\item
	consider $\alpha=\sqrt{2}$ and $\bigset{a+b\alpha}{a,b\in\integers}$,
	subring of $\integers[\alpha]\subset \reals$
	generated by $\alpha$.
	\bit
	\vitem
		$\alpha$ is \emph{not} transcendental
		because $f(\alpha)=0$ for $f(X)=X^2-1$
	\vitem
		hence kernel of evaluation map of $\integers[X]$ into $\integers[\alpha]$
		is not injective,
		hence not isomorphism
	\vitem
		indeed
		$$
			\integers[\alpha] = \bigset{a+b\alpha}{a,b\in\integers}
		$$
	\eit

\vvitem
	consider \primefield{p}\ for prime number $p$
	\bit
	\vitem
		$f(X) = X^p - X\in \primefield{p}[X]$ is not zero polynomial,
		but because $x^{p-1} \equiv 1$ for every nonzero $x\in\primefield{p}$
		by \theoremname~\ref{theorem:Euler's theorem}\ (Euler's theorem),
		$x^p\equiv x$ for every $x\in\primefield{p}$,
		thus for polynomial function, $f_{\primefield{p}}$,
		$f_{\primefield{p}}(x)=0$ for every $x$ in \primefield{p}
	\vitem
		\ie, \cemph{non-zero polynomial induces zero polynomial function}
	\eit
\eit


\myfoilhead{Reduction map}%
	\index{ring!reduction map}%
	\index{reduction map!ring}

\bit
\item
	for homeomorphism $\varphi:A\to B$ of commutative rings,
	exists associated homeomorphisms
	of polynomial rings $A[X]\to B[X]$
	such that
	$$
		f(X) = \sum a_i X^i
		\mapsto
		\sum \varphi(a_i) X^i
		= (\varphi f)(X)
	$$
\eit

\begin{mydefinition}{reduction map}%
		\index{polynomial ring!reduction map}%
		\index{reduction map!polynomial ring}%
	above ring homeomorphism $f\mapsto \varphi f$,
	called \define{reduction map}
\end{mydefinition}

\bit
\item
	\eg, for complex conjugate $\varphi: \complexes \to \complexes$,
		homeomorphism of $\complexes[X]$ into itself
		can be obtained by reduction map $f \mapsto \varphi f$,
		which is complex conjugate of polynomials with complex coefficients
\eit

\begin{mydefinition}{reduction of $f$ modulo $p$}%
		\index{polynomial ring!reduction of $f$ modulo $p$}%
		\index{reduction map!reduction of $f$ modulo $p$}
	for prime ideal \ideal{p} of ring $A$
	and
	surjective canonical map $\varphi: A \to A/\ideal{p}$,
	reduction map $\varphi f$ for $f\in A[X]$,
	sometimes called \define{reduction of $f$ modulo \ideal{p}}\
\end{mydefinition}


\myfoilhead{Basic properties of polynomials in one variable}

\begin{mytheorem}{Euclidean algorithm}%
		\index{Euclidean algorithm!polynomial ring}%
		\index{polynomial ring!Euclidean algorithm}\
	for set of all polynomials in one variable of nonnegative degrees $A[X]$
	with commutative ring $A$\
	\begin{eqnarray*}
		\lefteqn{
		\left(
			\forall f,g \in A[X]
			\mbox{ with leading coefficients of } g \mbox{ unit in }A
		\right)
		}
		\\
		&&
		\left(
			\exists q, r \in A[X]
			\mbox{ with } \deg r < \deg g
		\right)
		\left(
			f = qg + r
		\right)
	\end{eqnarray*}
\end{mytheorem}

\vfill
\begin{mytheorem}{principality of polynomial ring}%
		\index{polynomial ring!principality}\
	polynomial ring in one variable $k[X]$ with field $k$
	is principal
\end{mytheorem}

\vfill
\begin{mycorollary}{factoriality of polynomial ring}%
		\index{polynomial ring!factoriality}\
	polynomial ring in one variable $k[X]$ with field $k$
	is factorial
\end{mycorollary}

\vfill


\myfoilhead{Constant, monic, and irreducible polynomials}

\begin{mydefinition}{constant and monic polynomials}%
		\index{constant polynomial}%
		\index{monic polynomial}%
		\index{polynomial!constant}%
		\index{polynomial!monic}\
	$k \in k[X]$ with field $k$,
	called \define{constant polynomial};
	$f(x) \in k[X]$ with leading coefficient $1$,
	called \define{monic polynomial}
\end{mydefinition}

\vfill
\begin{mydefinition}{irreducible polynomials}%
		\index{irreducible polynomial}%
		\index{polynomial!irreducible}%
		\index{polynomial ring!irreducible polynomial}\
	polynomial $f(x)\in k[X]$ such that
	$$
		\left(
			\forall g(X), h(X) \in k[X]
		\right)
		\left(
			f(X) = g(X)h(X) \Rightarrow g(X) \in k \mbox{ or } h(X) \in k
		\right)
	$$
	said to be \define{irreducible}
\end{mydefinition}
\vfill


\myfoilhead{Roots or zeros of polynomials}

\begin{mydefinition}{root of polynomial}%
		\index{root!polynomial}%
		\index{polynomial!root}%
		\index{zero!polynomial}%
		\index{polynomial!zero}
	for commutative ring $B$, its subring $A\subset B$,
	and $f(x)\in A[X]$ in one variable,
	$b\in B$ satisfying
	$$
		f(b) = 0
	$$
	called \define{root} or \define{zero} of $f$
\end{mydefinition}

\vfill
\begin{mytheorem}{number of roots of polynomial}%
		\index{polynomial!root of polynomial}\
	for field $k$,
	polynomial $f\in k[X]$ in one variable of degree $n\geq 0$
	has at most $n$ roots in $k$;
	if $a$ is root of $f$ in $k$,
	$X-a$ divides $f(X)$
\end{mytheorem}
\vfill


\myfoilhead{Induction of zero functions}

\begin{mycorollary}{induction of zero function in one variable}%
		\index{polynomial!induction of zero function in one variable}\
	for field $k$ and infinite subset $T\subset k$,
	if polynomial $f\in k[X]$ in one variable over $k$
	satisfies
	$$
		\left(
			\forall a \in k
		\right)
		\left(
			f(a) =0
		\right)
	$$
	then $f(0)=0$,
	\ie,
	$f$ induces zero function
\end{mycorollary}

\begin{mycorollary}{induction of zero function in multiple variables}%
		\index{polynomial!induction of zero function in multiple variables}\
	for field $k$ and $n$ infinite subsets of $k$, $\seq{S_i}_{i=1}^n$,
	if polynomial in $n$ variables over field $k$
	satisfies
	$$
		\left(
			\forall a_i\in S_i \mbox{ for } 1\leq i \leq n
		\right)
		\left(
			f(a_1,\ldots,a_n)=0
		\right)
	$$
	then
	$f=0$,
	\ie,
	$f$ induces zero function
\end{mycorollary}

\begin{mycorollary}{induction of zero functions in multiple variables - infinite fields}%
		\index{polynomial!induction of zero function!in multiple variables}\
	if polynomial in $n$ variables over infinite field $k$
	induces zero function in $k^{(n)}$,
	$f=0$
\end{mycorollary}

\begin{mycorollary}{induction of zero functions in multiple variables - finite fields}%
		\index{polynomial!induction of zero function in multiple variables!finite field}\
	if polynomial in $n$ variables over finite field $k$ of order $q$,
	degree of which in each variable is less than $q$,
	induces zero function in $k^{(n)}$,
	$f=0$
\end{mycorollary}


\myfoilhead{Reduced polynomials and uniqueness}

\bit
\item
	for field $k$ with $q$ elements,
	polynomial in $n$ variables over $k$
	can be expressed as
	$$
		f(X_1,\ldots,X_n) = \sum a_i X_1^{\nu_{i,1}} \cdots X_n^{\nu_{i,n}}
	$$
	for finite sequence, \seqscr{a_i}{i=1}{m}, and
	\seqscr{\nu_{i,1}}{i=1}{m},
	\ldots,
	\seqscr{\nu_{i,n}}{i=1}{m}
	where $a_i\in k$ and $\nu_{i,j} \geq 0$

\vitem
	because $X_i^q=X_i$ for any $X_i$,
	any $\nu_{i,j}\geq q$ can be (repeatedly) replaced by $\nu_{i,j}-(q-1)$,
	hence
	$f$ can be rewritten as
	$$
		f(X_1,\ldots,X_n) = \sum a_i X_1^{\mu_{i,1}} \cdots X_n^{\mu_{i,n}}
	$$
	where $0\leq \mu_{i,j} < q$ for all $i,j$
\eit

\begin{mydefinition}{reduced polynomials}%
	\index{polynomial!reduced}
	above polynomial, called \define{reduced polynomial},
	denoted by \define{$f^\ast$}
\end{mydefinition}

\begin{mycorollary}{uniqueness of reduced polynomials}%
		\index{reduced polynomial!uniqueness}
	for field $k$ with $q$ elements,
	reduced polynomial is unique
	(by \corollaryname~\ref{corollary:induction of zero functions in multiple variables - finite fields})
\end{mycorollary}
\vfill


\myfoilhead{Multiplicative subgroups and $n$-th roots of unity}

\begin{mydefinition}{multiplicative subgroup of field}%
		\index{multiplicative subgroup of field}%
		\index{field!multiplicative subgroup of field}\
	for field $k$,
	subgroup of group $k^\ast=k\sim \{0\}$,
	called \define{multiplicative subgroup of $k$}
\end{mydefinition}

\vfill
\begin{mytheorem}{finite multiplicative subgroup of field is cyclic}\
	finite multiplicative subgroup of field is cyclic
\end{mytheorem}

\vfill
\begin{mycorollary}{multiplicative subgroup of finite field is cyclic}\
	multiplicative subgroup of finite field is cyclic
\end{mycorollary}

\vfill
\begin{mydefinition}{primitive $n$-th root of unity}%
		\index{primitive $n$-th root of unity}%
		\index{polynomial!primitive $n$-th roots of unity}%
		\index{primitive $n$-th root of unity!polynomial}\
	generator for group of $n$-th roots of unity,
	called \define{primitive $n$-th root of unity};
	group of roots of unity, denoted by \define{$\mu$};
	group of roots of unity in field $k$, denoted by \define{$\mu(k)$}
\end{mydefinition}
\vfill


\myfoilhead{Algebraic closedness}

\begin{mydefinition}{algebraically closed}%
		\index{field!algebraically closed extension}%
		\index{polynomial!algebraically closed}%
		\index{algebraically closed!field}%
		\index{algebraic closedness!field}%
		\index{field!algebraic closedness}
	field $k$, for which every polynomial in $k[X]$ of positive degree has root in $k$,
	said to be \define{algebraically closed}
\end{mydefinition}

\bit
\vitem
	\eg, complex numbers are algebraically closed

\vitem
	every field is contained in some algebraically closed field
	(\theoremname~\ref{theorem:existence of algebraically closed field extensions})

\vitem
	for algebraically closed field $k$\
	\bit
	\item
		(of course) every irreducible polynomial in $k[X]$ is of degree $1$
	\item
		unique factorization of polynomial of nonnegative degree can be written in form
		$$
			f(X) = c \prod_{i=1}^{r} (X-\alpha_i)^{m_i}
		$$
		with nonzero $c\in k$, distinct roots, $\alpha_1,\ldots,\alpha_r \in k$,
		and
		$m_1,\ldots,m_r \in \naturals$
	\eit
\eit


\myfoilhead{Derivatives of polynomials}

\begin{mydefinition}{derivative of polynomial over commutative ring}%
		\index{derivative!polynomial}%
		\index{polynomial!derivative}\
	for polynomial $f(X) = a_nX^n + \cdots + a_1 X + a_0 \in A[X]$ with commutative ring $A$,
	map $D:A[X] \to A[X]$
	defined by
	$$
		Df(X) = na_n X^{n-1} + \cdots + a_1
	$$
	called \define{derivative of polynomial},
	denoted by $f'(X)$;
\end{mydefinition}

\bit
\vitem
	for $f,g\in A[X]$ with commutative ring $A$, and $a\in A$\
	$$
		(f+g)' = f' + g'
		\quad
		\mbox{\&}
		\quad
		(fg)' = f'g + fg'
		\quad
		\mbox{\&}
		\quad
		(af)' = af'
	$$
\eit
\vfill


\myfoilhead{Multiple roots and multiplicity}

\bit
\item
	nonzero polynomial $f(X)\in k[X]$ in one variable over field $k$
	having $a\in k$ as root\
	can be written of form
	$$
		f(X) = (X-a)^m g(X)
	$$
	with some polynomial $g(X)\in A[X]$
	relatively prime to $(X-a)$ (hence, $g(a)\neq0$)
\eit

\begin{mydefinition}{multiplicity and multiple roots}%
		\index{polynomial!multiplicity}%
		\index{multiplicity!polynomial}%
		\index{polynomial!multiple roots}%
		\index{multiple roots!polynomial}\
	above, $m$, called \define{multiplicity of $a$ in $f$};
	$a$, said to be \define{multiple root of $f$}
	if $m>1$
\end{mydefinition}

\vfill
\begin{myproposition}{necessary and sufficient condition for multiple roots}%
		\index{necessary and sufficient condition for multiple roots}%
		\index{polynomial!multiple roots!necessary and sufficient condition for multiple roots}%
		\index{multiple roots!necessary and sufficient condition for multiple roots!polynomial}\
	for polynomial $f$ of one variable over field $k$,
	$a\in k$ is multiple root of $f$
	\iaoi\
	$f(a)=0$ and $f'(a)=0$
\end{myproposition}

\vfill
\begin{myproposition}{derivative of polynomial}%
		\index{derivative!of polynomial}%
		\index{polynomial!derivative}\
	for polynomial $f\in K[X]$ over field $K$ of positive degree,
	$f'\neq0$ if $K$ has characteristic $0$;
	if $K$ has characteristic $p>0$,
	$f'=0$\
	\iaoi\
	$$
		f(X) = \sum_{\nu=1}^n a_\nu X^\nu
	$$
	where $p$ divides each integer $\nu$ whenever $a_\nu\neq0$
\end{myproposition}
\vfill


\myfoilhead{Frobenius endomorphism}%
	\index{field!having characteristic $p$}

\bit
\item
	homeomorphism of $K$ into itself $x\mapsto x^p$
	has trivial kernel, hence injective
\vitem
	hence,
	iterating $r\geq 1$ times yields endomorphism, $x\mapsto x^{p^r}$
\eit

\vfill
\begin{mydefinition}{Frobenius endomorphism}%
		\index{Frobenius endomorphism!polynomial}%
		\index{polynomial!Frobenius endomorphisms}%
		\index{Frobenius, Ferdinand Georg!Frobenius endomorphism!polynomial}\
	for field $K$, prime number $p$, and $r\geq1$,
	endomorphism of $K$ into itself, $x\mapsto x^{p^r}$,
	called \define{Frobenius endomorphism}\
\end{mydefinition}
\vfill


\myfoilhead{Roots with multiplicity $p^r$ in fields having characteristic $p$}%
	\index{field!having characteristic $p$}

\bit
\item
	for field $K$ having characteristic $p$\\
	\bit
	\vitem
		$p | {p \choose \nu}$ for all $0< \nu < p$ because $p$ is prime,
		hence,
		for every $a,b\in K$
		$$
			(a+b)^p = a^p + b^p
		$$

	\vitem
		applying this resurvely $r$ times yields
		$$
			(a+b)^{p^r}
			= (a^p + b^p)^{p^{r-1}}
			= (a^{p^2} + b^{p^2})^{p^{r-2}}
			= \cdots
			= a^{p^r} + b^{p^r}
		$$
		hence
		$$
			(X-a)^{p^r} = X^{p^r} - a^{p^r}
		$$

	\vitem
		if $a,c\in K$ satisfy $a^{p^r} = c$
		$$
			X^{p^r} - c
			= X^{p^r} - a^{p^r}
			= (X-a)^{p^r}
		$$
		hence, polynomial $X^{p^r}-c$ has precisely one root $a$ of multiplicity $p^r$!
	\eit
\eit


\yesnoexec{\showincomplete}{%
\myfoilhead{Polynomials over a factorial ring}
}{}


\yesnoexec{\showincomplete}{%
\myfoilhead{Criteria for irreducibility}
}{}


\yesnoexec{\showincomplete}{%
\myfoilhead{Hilbert's theorem}
}{}


\yesnoexec{\showincomplete}{%
\myfoilhead{Partial fractions}
}{}


\yesnoexec{\showincomplete}{%
\myfoilhead{Symmetric polynomials}
}{}


\yesnoexec{\showincomplete}{%
\myfoilhead{Mason's theorem and the $abc$ conjecture}
}{}


\yesnoexec{\showincomplete}{%
\myfoilhead{The resultant}
}{}


\yesnoexec{\showincomplete}{%
\myfoilhead{Power series}
}{}


\titlefoil{Algebraic Extension}{Algebraic Extension}%
	\index{algebraic extension}\
	\index{field!algebraic extension}\

\myfoilhead{Algebraic extension}%
	\index{field!algebraic extension}

\bit
\item
	will show
	\bit
	\vitem
		for polynomial over field,
		always exists some extension of \emph{that} field
		where the polynomial has root

	\vitem
		existence of algebraic closure
		for every field
	\eit
\eit
\vfillt


\myfoilhead{Extension of field}

\begin{mydefinition}{extension of field}%
		\index{field!extension of field}
	for field $E$ and its subfield $F\subset E$,
	$E$
	said to be \define{extension field of $F$},
	(sometimes) denoted by \define{$E/F$}
	(which should \emph{not} confused with \emph{factor group})
	\shrinkspacewithintheoremslike
	\ibit
	\iitem can view $E$ as \define{vector space} over $F$%
		\index{vector space!as field extension}
	\iitem if dimension of the vector space is finite,
		extension called \define{finite extension of $F$}%
			\index{extension!field!finite}%
			\index{extension!field}%
			\index{field!extension}%
			\index{field!extension!finite}\
	\iitem if infinite,
		called \define{infinite extension of $F$}%
			\index{extension!field!infinite}%
			\index{extension!field}%
			\index{field!extension}%
			\index{field!extension!infinite}\
	\eit
\end{mydefinition}


\myfoilhead{Algebraic over field}

\begin{mydefinition}{algebraic over field}%
		\index{algebraic!over field}%
		\index{field!algebraic over field}\
	for field $E$ and its subfield $F\subset E$,
	$\alpha\in E$ satisfying\\
	$$
		\left(
			\exists a_0,\ldots, a_n
			\mbox{ with not all } a_i \mbox{ zero}
		\right)
		\left(
			a_0 + a_1\alpha + \cdots + a_n \alpha^n=0
		\right)
	$$

	said to be \define{algebraic over $F$}
	\shrinkspacewithintheoremslike
	\ibit
	\iitem
		for algebraic $\alpha\neq0$,
		can always find such equation like above that $a_0\neq0$
	\eit
\end{mydefinition}

\bit
\vitem
	equivalent statements to \definitionname~\ref{definition:algebraic over field}
	\bit
	\vitem
%		for field $E$ and its subfield $F\subset E$,
%		$\alpha\in E$ is algebraic over $F$\
%		\iaoi\
		exists homeomorphism $\varphi: F[X] \to E$ such that%
			\index{algebraic!over field}\
		$$
			\left(\forall x\in F\right) \left(\varphi(x) = x\right)
			\mbox{ \& }
			\varphi(X) = \alpha
			\mbox{ \& }
			\Ker \varphi \neq \{0\}
		$$
	\vitem
%		which is equivalent to say,
		exists evaluation homeomorphism $\ev_\alpha: F[X] \to E$
		with nonzero kernel
		(refer to \definitionname~\ref{definition:evaluation homeomorphism} for definition of evaluation homeomorphism)
	\eit

\vitem
	in which case,
	$\Ker \varphi$ is principal ideal
	(by \theoremname~\ref{theorem:principality of polynomial ring}),
	hence generated by single element,
	thus exists nonzero $p(X) \in F[X]$ (with normalized leading coefficient being $1$)\
	so that
	$$
		F[X] / (p(X)) \isomorph F[\alpha]
	$$

\vitem
	$F[\alpha]$ \emph{entire} (\lemmaname~\ref{lemma:every field is entire ring}),
	hence $p(X)$ irreducible (refer to \definitionname~\ref{definition:prime ideal})
\eit

\vfill
\begin{mydefinition}{THE irreducible polynomial}%
		\index{THE irreducible polynomial}%
		\index{field!THE irreducible polynomial}%
		\index{algebraic!over field!THE irreducible polynomial}
	normalized $p(X)$ (\ie, with leading coefficient being $1$)
	uniquely determined by $\alpha$,
	called \define{THE irreducible polynomial of $\alpha$ over $F$},
	denoted by \define{$\Irr(\alpha, F, X)$}\
\end{mydefinition}
\vfill


\myfoilhead{Algebraic extensions}

\begin{mydefinition}{algebraic extension}%
		\index{algebraic extension}%
		\index{extension!algebraic}%
		\index{field!algebraic extension}%
		\index{field!extension!algebraic}
	for field $F$,
	its extension field every element of which is algebraic over $F$,
	said to be \define{algebraic extension of $F$}
\end{mydefinition}

\vfill
\begin{myproposition}{algebraicness of finite field extensions}%
		\index{algebraic extension!finite}%
		\index{extension!algebraic!finite}%
		\index{field!algebraic extension!finite}%
		\index{field!extension!algebraic!finite}
	for field $F$,
	every finite extension field of $F$
	is algebraic over $F$
\end{myproposition}

\bit
\item
	converse is \emph{not} true,
	\eg,
	subfield of complex numbers
	consisting of algebraic numbers over \rationals\
	is infinite extension of \rationals\
\eit
\vfill


\myfoilhead{Dimension of extensions}%
	\index{dimension!algebraic extension}%
	\index{algebraic!extension!dimension}

\begin{mydefinition}{dimension of extension}%
		\index{dimension!field!algebraic extension}%
		\index{field!dimension!extension}
	for field $F$ and its extension field $E$,
	dimension of $E$ as vector space over $F$,
	called \define{dimension of $E$ over $F$},
	denoted by \define{\dimext{E}{F}}\
\end{mydefinition}

\vfill
\begin{myproposition}{dimension of finite extension}%
		\index{field!dimension of finite extension}
	for field $k$ and its extension fields $F$ and $E$
	with $k\subset F\subset E$\

	\begin{center}
		$
			\dimext{E}{k}
			=
			\dimext{E}{F}
			\dimext{F}{k}
		$
	\end{center}
	\shrinkspacewithintheoremslikehalf\
	\ibit
	\iitem
		if \seqscr{x_i}{i\in I}{}\ is basis for $F$ over $k$,
		and \seqscr{y_j}{j\in J}{}\ is basis for $E$ over $F$,
		\seqscr{x_iy_j}{(i,j)\in I\times J}{}\
		is basis for $E$ over $k$
	\eit
\end{myproposition}

\vfill
\begin{mycorollary}{finite dimension of extension}%
	\index{field!dimension of extension!finiteness}
	for field $k$ and its extension fields $F$ \& $E$
	with $k\subset F\subset E$,
%	extension of $E$ over $k$ is finite
	$E/k$ is finite
	\iaoi\
	both
%	extension of $F$ over $k$
	$F/k$
	and
%	extension of $E$ over $F$
	$E/F$
	are finite
\end{mycorollary}
\vfill


\myfoilhead{Generation of field extensions}

\begin{mydefinition}{generation of field extensions}%
		\index{field!generation of extension}%
		\index{field!extension!generation}%
		\index{field!extension!finitely generated}
	for field $k$, its extension field $E$, and $\alpha_1,\ldots, \alpha_n \in E$,
	smallest subfield containing $k$ and $\alpha_1$, \ldots, $\alpha_n$,
	said to be \define{finitely generated over $k$ by $\alpha_1$, \ldots, $\alpha_n$},
	denoted by \define{$k(\alpha_1,\ldots, \alpha_n)$}
\end{mydefinition}

\bit
\item
	$k(\alpha_1,\ldots, \alpha_n)$ consists of all quotients
	$f(\alpha_1,\ldots,\alpha_n)/g(\alpha_1,\ldots, \alpha_n)$
	where $f,g\in k[X]$ and $g(\alpha_1,\ldots, \alpha_n)\neq0$,
	\ie\
	\begin{eqnarray*}
		\lefteqn{
		k(\alpha_1,\ldots,\alpha_n)
		}
		\\
		&=& \bigset{f(\alpha_1,\ldots, \alpha_n)/g(\alpha_1,\ldots,\alpha_n)}{f,g\in f[X], g(\alpha_1,\ldots,\alpha_n)\neq0}
	\end{eqnarray*}

\item
	\emph{any} field extension $E$ over $k$
	is union of smallest subfields containing $\alpha_1,\ldots, \alpha_n$
	where $\alpha_1,\ldots, \alpha_n$ range over finite set of elements of $E$,
	\ie\
	$$
		E =
			\bigcup_{n\in\naturals}
			\bigcup_{\alpha_1, \ldots, \alpha_n \in E}
			k(\alpha_1,\ldots,\alpha_n)
	$$
\eit

\begin{myproposition}{finite extension is finitely generated}
		\index{field!extension!finite}
	every finite extension of field is finitely generated
\end{myproposition}


\myfoilhead{Tower of fields}

\begin{mydefinition}{tower of fields}%
		\index{field!tower of fields}
	sequence of extension fields
	$$
		F_1
		\subset
		F_2
		\subset
		\cdots
		\subset
		F_n
	$$
	called \define{tower of fields}
\end{mydefinition}

\vfill
\begin{mydefinition}{finite tower of fields}%
		\index{finite tower of fields}%
		\index{field!finite tower of fields}
	tower of fields,
	said to be \define{finite}
	\iaoi\
	each step of extensions is finite
\end{mydefinition}
\vfill


\myfoilhead{Algebraicness of finitely generated subfields}

\begin{myproposition}{algebraicness of finitely generated subfield by single element}%
		\index{field!algebraicness!finitely generated subfield by single element}
	for field $k$, its extension field $E$, and $\alpha\in E$ being algebraic over $k$
	$$k(\alpha) = k[\alpha]$$
	and
	$$
		[k(\alpha):k] = \deg \Irr(\alpha, k, X)
	$$

	hence $k(\alpha)$ is finite extension of $k$,
	thus \emph{algebraic extension over $k$}\
	(by \propositionname~\ref{proposition:algebraicness of finite field extensions})
\end{myproposition}

\vfill
\begin{mylemma}{a fortiori algebraicness}%
		\index{a fortiori algebraicness}%
		\index{field!a fortiori algebraicness}%
		\index{field!algebraicness!a fortiori}
	for field $k$,
	its extension field $F$,
	and $\alpha\in E$ being algebraic over $k$
	where $k(\alpha)$ and $F$ are subfields of common field,
	$\alpha$ is algebraic over $F$
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		indeed, $\Irr(\alpha,k,X)$
		has \emph{a fortiori}
		coefficients in $F$
	\eit
\end{mylemma}


\myfoilhead{}

\bit
\item
	assume tower of fields
	$$
		k \subset k(\alpha_1) \subset k(\alpha_1, \alpha_2) \subset \cdots \subset k(\alpha_1,\ldots, \alpha_n)
	$$
	where $\alpha_i$ is algebraic over $k$

\vitem
	then, $\alpha_{i+1}$ is algebraic over $k(\alpha_1,\ldots,\alpha_i)$ (by \lemmaname~\ref{lemma:a fortiori algebraicness})
\eit

\vfill
\begin{myproposition}{algebraicness of finitely generated subfields by multiple elements}%
		\index{field!algebraicness!finitely generated subfield by multiple elements}
	for field $k$
	and
	$\alpha_1$, \ldots, $\alpha_n$ being algebraic over $k$,
	$E=k(\alpha_1,\ldots,\alpha_n)$
	is finitely algebraic over $k$
	(due to \propositionname~\ref{proposition:algebraicness of finitely generated subfield by single element},
%	\corollaryname~\ref{corollary:finite dimension of extension},
	\propositionname~\ref{proposition:dimension of finite extension},
	and
	\propositionname~\ref{proposition:algebraicness of finite field extensions}).
	Indeed, $E = k[\alpha_1, \ldots, \alpha_n]$ and
%	\begin{eqnarray*}
%		\lefteqn{
%			[k(\alpha_1,\ldots,\alpha_n):k]
%		}
%		\\
%		&=&
%			\Irr(\alpha_1,k,X) \Irr(\alpha_2,k(\alpha_1),X) \cdots \Irr(\alpha_n, k(\alpha_1,\ldots,\alpha_{n-1}), X)
%	\end{eqnarray*}
	\begin{eqnarray*}
			[k(\alpha_1,\ldots,\alpha_n):k] &=& \deg \Irr(\alpha_1,k,X) \deg \Irr(\alpha_2,k(\alpha_1),X)
		\\
		&&
			\cdots \deg \Irr(\alpha_n, k(\alpha_1,\ldots,\alpha_{n-1}), X),
	\end{eqnarray*}
	\proofref{algebraicness of smallest subfields}
\end{myproposition}


\myfoilhead{Compositum of subfields and lifting}

\begin{mydefinition}{compositum of subfields}%
		\index{compositum!field}%
		\index{field!compositum}
	for field $k$ and its extension fields $E$ and $F$, which are subfields of common field $L$,
	smallest subfield of $L$ containing both $E$ and $F$,
	called \define{compositum of $E$ and $F$ in $L$},
	denoted by \define{$EF$}
	\shrinkspacewithintheoremslike\
	\ibit
	\item [!]
		cannot define compositum if $E$ and $F$ are not embedded in common field $L$
	\eit
\end{mydefinition}

\bit
\item
	could define \define{compositum of set of subfields of $L$}
	as smallest subfield containing subfields in the set
\eit

\vfill
\begin{mylemma}{}
	extension $E$ of $k$ is
	compositum of all its finitely generated subfields over $k$,\
	\ie,
	$
		E =
			\bigcup_{n\in\naturals}
			\bigcup_{\alpha_1, \ldots, \alpha_n \in E}
			k(\alpha_1,\ldots,\alpha_n)
	$
\end{mylemma}
\vfill


\myfoilhead{Lifting}

\begin{mydefinition}{lifting}%
		\index{field!lifting}%
		\index{lifting!field}%
		\index{field!translation}%
		\index{translation!field}
	extension $EF$ of $F$,
	called \define{translation of $E$ to $F$} or \define{lifting of $E$ to $F$}
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		often draw diagram as in \figref{translation or lifting of fields}
	\eit
\end{mydefinition}

\begin{figure}
\begin{center}
\plifting{1.5cm}
\end{center}
	\idxfig{translation or lifting of fields}
	\label{fig:translation or lifting of fields}
\end{figure}



\myfoilhead{Finite generation of compositum}

\begin{mylemma}{finite generation of compositum}%
		\index{field!compositum!finite generation}%
		\index{compositum!finite generation!field}
	for field $k$,
	its extension field $F$,
	and
	$E = k(\alpha_1,\ldots,\alpha_n)$
	where both $E$ and $F$ are contained in common field $L$,
	$$
		EF = F(\alpha_1, \ldots, \alpha_n)
	$$
	\ie,
	compositum $EF$ is finitely generated over $F$ \proofref{finite generation of compositum}
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		refer to diagra in \figref{lifting of smallest fields}
	\eit
\end{mylemma}

\begin{figure}
\begin{center}
\pliftingsmallest{1.5cm}
\end{center}
	\idxfig{lifting or smallest fields}
	\label{fig:lifting of smallest fields}
\end{figure}


\myfoilhead{Distinguished classes}

\begin{mydefinition}{distinguished class of field extensions}%
		\index{distinguished class!field!extension}%
		\index{field!extension!distinguished class}
	for field $k$,
%	for field $k$ and its extension field $E$,
	class \classk{C} of extension fields satisfying
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		for tower of fields $k\subset F\subset E$,
		extension $k\subset E$ is in \classk{C}\
		\iaoi\
		both $k\subset F$ and $F\subset E$ are in \classk{C}\

	\iitem
		if $k\subset E$ is in \classk{C}, $F$ is any extension of $k$,
		and both $E$ and $F$ are subfields of common field,
		then $F\subset EF$ is in \classk{C}
	\eit
	\shrinkspacewithintheoremslike
	\noindent
	said to be \define{distinguished};
	\figref{translation or lifting of fields}\ illustrates these two properties,
	which imply the following property
	\shrinkspacewithintheoremslikehalf\
	\ibit
	\iitem
		if $k\subset F$ and $k\subset E$ are in \classk{C}\
		and both $E$ and $F$ are subfields of common field,
		$k\subset EF$ is in \classk{C}\
	\eit
\end{mydefinition}

\begin{figure}
\begin{center}
%	\fbox{
	\fieldextensions{1cm}
%	}
	\hspace{2cm}
%	\fbox{
	\plifting{1.3636363636363635cm}
%	}
	\idxfig{lattice diagram of fields}
	\label{fig:lattice diagram of fields}
\end{center}
\end{figure}


\myfoilhead{Both algebraic and finite extensions are distinguished}

\begin{myproposition}{algebraic and finite extensions are distinguished}%
		\index{field!algebraic extension!distinguished}%
		\index{field!finite extension!distinguished}
	class of algebraic extensions is distinguished,
	so is class of finite extensions
\end{myproposition}

\bit
\item
	true that finitely generated extensions form distinguished class (not necessarily algebraic extensions or finite extensions)
\eit


\myfoilhead{Field embedding and embedding extension}

\begin{mydefinition}{field embedding}%
		\index{field!embedding}%
		\index{embedding!field}
	for two fields $F$ and $L$,
	injective homeomorphism $\sigma:F\to L$,
	called \define{embedding of $F$ into $L$};
	then (of course) $\sigma$ induces isomorphism of $F$ with its image $\sigma F$\footnote{
		Here $\sigma F$ is sometimes written as \define{$F^\sigma$}.
	}
\end{mydefinition}

\begin{mydefinition}{field embedding extension}%
		\index{field!embedding!extension}%
		\index{embedding!extension!field}
	for field embedding $\sigma:F\to L$,
	field extension $F\subset E$,
	and
	embedding $\tau:E\to L$
	whose restriction to $F$ being equal to $\sigma$,
	said to \define{be over $\sigma$} or \define{extend $\sigma$;}
	if $\sigma$ is identity,
	embedding $\tau$, called \define{embedding of $E$ over $F$};
	diagrams in \figref{field embedding extensions}\ show these embedding extensions

%	\shrinkspacewithintheoremslike\
%	\bit
%	\item
%	\eit
\end{mydefinition}

\vfill
\begin{figure}
\begin{center}
%\fbox{
	\embeddingextensionf{4em}
%}
	\hspace{3cm}
%\fbox{
%	\embeddingextensions{5em}
%}
%	\hspace{3cm}
%\fbox{
	\embeddingextensiont{4.6188em}
%}
\end{center}
\idxfig{embedding extension}
\label{fig:field embedding extensions}
\end{figure}

%\vfill
%\begin{figure}
%	$$
%		\diagsquare{E}{\tau}{L}{\mathrm{id}}{L}{\sigma}{F}{\mathrm{inc}}
%		\hspace{3em}
%		\diagtriruu{E}{\tau}{L}{\mathrm{inc}}{F}{\mathrm{inc}}
%	$$
%	\idxfig{embedding extension}
%	\label{fig:field embedding extensions}
%\end{figure}

\shrinkspacewithintheoremslike
\bit
\item
	assuming $F$, $E$, $\sigma$, and $\tau$ same as in \definitionname~\ref{definition:field embedding extension},
	if $\alpha\in E$ is root of $f\in F[X]$, then $\alpha^\tau$ is root of $f^\sigma$
	for
		 if $f(X) = \sum_{i=0}^n a_i X^i$, then $f(\alpha) = \sum_{i=0}^n a_i \alpha^i = 0$,
		 and
		$
		 	0
			= f(\alpha)^\tau
			= \sum_{i=0}^n (a_i^\tau ) (\alpha^\tau)^i
			= \sum_{i=0}^n a_i^\sigma (\alpha^\tau)^i
			= f^\sigma(\alpha^\tau)
		$
\eit


\myfoilhead{Embedding of field extensions}

\begin{mylemma}{field embedding of algebraic extension}%
		\index{field embedding!of algebraic extension}%
		\index{algebraic extension!field embeddings of}
	for field $k$ and its algebraic extension $E$,
	embedding of $E$ into itself over $k$
	is isomorphism
	\idximportant{
	for field $k$ and its algebraic extension $E$,
	embedding of $E$ into itself over $k$
	is isomorphism}
\end{mylemma}

\vfill
\begin{mylemma}{compositums of fields}%
		\index{compositums!field}%
		\index{field!compositums}
	for field $k$ and its field extensions $E$ and $F$ contained in common field,
	$$
		E[F] = F[E] = \bigcup_{n=1}^\infty \bigset{e_1f_1 + \cdots + e_nf_n}{e_i\in E, f_i\in F}
	$$
	and
	$EF$ is field of quotients of these elements
\end{mylemma}

\vfill
\begin{mylemma}{embeddings of compositum of fields}%
		\index{compositums!embedding}%
		\index{field!embedding!compositums}
	for
	field $k$,
	its field extensions $E_1$ and $E_2$ contained in commen field $E$,
	and
	embedding $\sigma:E\to L$ for field $L$,
	$$
		\sigma(E_1 E_2) = \sigma(E_1) \sigma(E_2)
	$$
\end{mylemma}
\vfill


\myfoilhead{Existence of roots of irreducible polynomial}%
	\index{irreducible polynomial!existence of roots}

\bit
\item
	assume $p(X) \in k[X]$ irreducible polynomial and consider canonical map, which is ring homeomorphism
	$$
		\sigma: k[X] \to k[X] / ((p(X))
	$$

\vitem
	consider $\Ker \restrict{\sigma}{k}$
	\bit
	\item
		every kernel of ring homeomorphism is ideal,
		hence if nonzero $a \in \Ker \restrict{\sigma}{k}$, $1\in \Ker \restrict{\sigma}{k}$
		because $a^{-1} \in \Ker \restrict{\sigma}{k}$,
		but $1\not\in (p(X))$
	\item
		thus, $\Ker \restrict{\sigma}{k} = \{0\}$,
		hence $p^\sigma\neq0$
	\eit

\vitem
	now for $\alpha = X^\sigma$
	$$
		p^\sigma(\alpha)
		= p^\sigma(X^\sigma) = (p(X))^\sigma = 0
	$$

\vitem
	thus, $\alpha$ is algebraic in $k^\sigma$,
	\ie,
	$\alpha \in k[X]^\sigma$ is root of $p^\sigma$ in $k^\sigma(\alpha)$
\eit

\begin{mylemma}{existence of roots of irreducible polynomial}%
		\index{irreducible polynomial!existence of roots}
	for field $k$ and irreducible $p(X)\in k[X]$ with $\deg p \geq 1$,
	exist field $L$ and homeomorphism $\sigma:k \to L$
	such that $p^\sigma$ with $\deg p^\sigma \geq 1$ has root
	in field extension of $k^\sigma$
\end{mylemma}
\vfill
%
%
%\myfoilhead{Algebraic closedness}
%
%\begin{mydefinition}{algebraic closedness}%
%		\index{algebraic closedness!field}%
%		\index{algebraically closed!field}%
%		\index{field!algebraic closedness}%
%		\index{field!algebraically closed}
%	field $L$ for which every polynomial $f$ in $L[X]$ with $\deg f \geq 1$
%	has root in $L$,
%	said to be \define{algebraically closed}
%\end{mydefinition}


\myfoilhead{Existence of algebraically closed algebraic field extensions}%
	\index{field!extension!algebraically closed algebraic}%
	\index{field!existence of algebraically closed algebraic extension}

\begin{myproposition}{existence of extension fields containing roots}%
		\index{field!existence of extension fields containing roots}
	for field $k$ and $f\in k[X]$ with $\deg f \geq 1$,
	exists extension of $k$ in which $f$ has root
\end{myproposition}

\vfill
\begin{mycorollary}{existence of extension fields containing roots}%
		\index{field!existence of extension fields containing roots}
	for field $k$ and $f_1$, \ldots, $f_n$ $\in$ $k[X]$ with $\deg f_i \geq 1$,
	exists extension of $k$ in which every $f_i$ has root
\end{mycorollary}

\vfill
\begin{mytheorem}{existence of algebraically closed field extensions}%
		\index{field!existence of algebraically closed extensions}
	for every field $k$,
	exists algebraically closed extension of $k$
\end{mytheorem}

\vfill
\begin{mycorollary}{existence of algebraically closed algebraic field extensions}%
		\index{field!existence of algebraically closed algebraic extension}%
		\index{field!extension!algebraically closed algebraic}
	for every field $k$,
	exists algebraically closed algebraic extension of $k$
	\proofref{existence of algebraically closed algebraic extensions}\
\end{mycorollary}


\myfoilhead{Isomorphism between algebraically closed algebraic extensions}

\begin{myproposition}{number of algebraic embedding extensions}%
		\index{number of algebraic embedding extensions!field}%
		\index{field!number of algebraic embedding extensions}
	for field, $k$,
	$\alpha$ being algebraic over $k$,
	algebraically closed field, $L$,
	and
	embedding, $\sigma:k\to L$,
	\# possible embedding extensions of $\sigma$ to $k(\alpha)$ in $L$
	is
	equal to \# distinct roots of $\Irr(\alpha,k,X)$,
	hence
	no greater than \# roots of $\Irr(\alpha,k,X)$\
\end{myproposition}

\begin{mytheorem}{algebraic embedding extensions}%
		\index{algebraic embedding extension!field}%
		\index{field!algebraic embedding extension}
	for field, $k$,
	its algebraic extensions, $E$,
	algebraically closed field, $L$,
	and
	embedding, $\sigma:k\to L$,
	exists embedding extension of $\sigma$ to $E$ in $L$;
	if $E$ is algebraically closed and $L$ is algebraic over $k^\sigma$,
	every such embedding extension is isomorphism of $E$ onto $L$
\end{mytheorem}

\begin{mycorollary}{isomorphism between algebraically closed algebraic extensions}%
		\index{isomorphism between algebraically closed algebraic extensions!field}%
		\index{field!isomorphism between algebraically closed algebraic extensions}
	for field, $k$,
	and
	its algebraically closed algebraic extensions, $E$ and $E'$,
	exists isomorphism bewteen $E$ and $E'$ which induces identity on $k$,
	\ie
	$$
		\tau: E \to E'
	$$
	\shrinkspacewithintheoremsliket
	\shrinkspacewithintheoremslike

	where \restrict{\tau}{k}\ is identity
\end{mycorollary}

\bit
\item
	thus, \eemph{algebraically closed algebraic extension is determined up to isomorphism}
	\idximportant{algebraically closed algebraic extension is determined up to isomorphism}
\eit


\myfoilhead{Algebraic closure}

\begin{mydefinition}{algebraic closure}%
		\index{algebraic closure!field}%
		\index{field!algebraic closure}
	for field, $k$,
	algebraically closed algebraic extension of $k$, which is determined up to isomorphism,
	called \define{algebraic closure of $k$},
	frequently denoted by \define{\algclosure{k}}
\end{mydefinition}

\bit
\item examples
	\bit
	\item
		complex conjugation is automorphism of \complexes\
		(which is the only continuous automorphism of \complexes)

	\item
		subfield of \complexes\ consisting of all numbers which are algebraic over \rationals\
		is algebraic closure of \rationals, \ie, \algclosure{\rationals}

	\item
		$\algclosure{\rationals} \neq \complexes$

	\item
		$\algclosure{\reals} = \complexes$

	\item
		\emph{\algclosure{\rationals}\ is countable}
	\eit
\eit

\vfill
\begin{mytheorem}{countability of algebraic closure of finite fields}%
		\index{countability of algebraic closure of finite field}%
		\index{field!countability of algebraic closure of finite field}
	algebraic closure of finite field is countable
\end{mytheorem}

\vfill
\begin{mytheorem}{cardinality of algebraic extensions of infinite fields}%
		\index{cardinality of algebraic extension of infinite field}%
		\index{field!cardinality of algebraic extension of infinite field}
	for infinite field, $k$,
	every algebraic extension of $k$
	has same cardinality as $k$
\end{mytheorem}


\myfoilhead{Splitting fields}%
	\index{field!splitting}%
	\index{splitting field}

\begin{mydefinition}{splitting fields}
	for field, $k$, and $f\in k[X]$ with $\deg f \geq 1$,
	field extension, $K$, of $k$,
	$f$ splits into linear factors in which,
	\ie,
	$$
		f(X) = c (X-\alpha_1) \cdots (X-\alpha_n)
	$$
	and which is finitely generated over $k$ by $\alpha_1$, \ldots, $\alpha_n$
	(hence $K=k(\alpha_1, \ldots, \alpha_n)$),
	called \define{splitting field of $f$}
\end{mydefinition}

\bit
\item
	for field, $k$,
	every $f\in k[X]$
	has splitting field
	in \algclosure{k}\
%	(\propositionname~\ref{proposition:existence of extension fields containing roots}\
%		\&
%	\corollaryname~\ref{corollary:existence of algebraically closed algebraic field extension})
\eit

\begin{mytheorem}{isomorphism between splitting fields}%
		\index{splitting field!isomorphism}%
		\index{field!splitting!isomorphism}
	for field, $k$, $f\in k[X]$ with $\deg f \geq1$,
	and two splitting fields of $f$, $K$ and $E$,
	exists isomorphism between $K$ and $E$;
	if $k\subset K\subset \algclosure{k}$,
	every embedding of $E$ into \algclosure{k}\
	over $k$
	is isomorphism of $E$ onto $K$\
\end{mytheorem}


\myfoilhead{Splitting fields for family of polynomials}

\begin{mydefinition}{splitting fields for family of polynomials}
	for field, $k$,
	index set, $\Lambda$,
	and indexed family of polynomials,
	\set{f_\lambda\in k[X]}{\lambda \in \Lambda, \deg f_\lambda \geq1},
	extension field of $k$,
	every $f_\lambda$ splits into linear factors in which
	and
	which is generated by all roots of all polynomials, $f_\lambda$,
	called \define{splitting field for family of polynomials}\
\end{mydefinition}

\bit
\item
	in most applications, deal with finite $\Lambda$\
\item
	becoming increasingly important to consider infinite algebraic extensions
\item
	various proofs would not be simpler if restricted ourselves to finite cases
\eit

\begin{mycorollary}{isomorphism between splitting fields for family of polynomials}
	for field, $k$,
	index set, $\Lambda$,
	and two splitting fields, $K$ and $E$, for family of polynomials,
	\set{f_\lambda\in k[X]}{\lambda \in \Lambda, \deg f_\lambda \geq1},
	every embedding of $E$ into \algclosure{K}\
	over $k$
	is isomorphism of $E$ onto $K$\
\end{mycorollary}


\myfoilhead{Normal extensions}

\begin{mytheorem}{normal extensions}
	for field, $k$,
	and its algebraic extension, $K$,
	with $k\subset K\subset \algclosure{k}$,
	following statements are equivalent\
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		every embedding of $K$ into \algclosure{k}\ over $k$ induces automorphism
	\iitem
		$K$ is splitting field of family of polynomials in $k[X]$
	\iitem
		every irreducible polynomial of $k[X]$ which has root in $K$
		splits into linear factors in $K$
	\eit
\end{mytheorem}

\begin{mydefinition}{normal extensions}
	for field, $k$,
	and its algebraic extension, $K$,
	with $k\subset K\subset \algclosure{k}$,
	satisfying properties in \theoremname~\ref{theorem:normal extensions},
	said to be \define{normal}
\end{mydefinition}

\bit
\item
	\emph{not} true that class of normal extensions is \emph{distinguished}
	\bit
	\item
		\eg, below tower of fields is tower of normal extensions
		$$
			\rationals
				\subset
			\rationals(\sqrt{2})
				\subset
			\rationals(\sqrt[4]{2})
		$$
	\vitem
		but, extension $\rationals \subset \rationals(\sqrt[4]{2})$ is not normal
		because complex roots of $X^4-2$ are not in $\rationals(\sqrt[4]{2})$
	\eit
\eit


\myfoilhead{Retention of normality of extensions}

\begin{mytheorem}{retention of normality of extensions}
	normal extensions remain normal under lifting;
	if $k\subset E\subset K$ and $K$ is normal over $k$,
	$K$ is normal over $E$;
	if $K_1$ and $K_2$ are normal over $k$ and are contained in common field,
	$K_1K_2$ is normal over $k$,
	and so is $K_1\cap K_2$\
\end{mytheorem}


\myfoilhead{Separable degree of field extensions}

\bit
\item
	for field, $F$, and its algebraic extension, $E$
	\bit
	\item
		let $L$ be algebraically closed field and assume embedding, $\sigma:F\to L$
		\bit
		\iitem
			exists embedding extension of $\sigma$ to $E$ in $L$
			by \theoremname~\ref{theorem:algebraic embedding extensions}
		\iitem
			such $\sigma$ maps $E$ on subfield of $L$ which is algebraic over $F^\sigma$
		\iitem
			hence, $E^\sigma$ is contained in algebraic closure of $F^\sigma$ which is contained in $L$
		\iitem
			will \emph{assume} that $L$ is the algebraic closure of $F^\sigma$
		\eit

	\item
		let $L'$ be another algebraically closed field and assume another embedding, $\tau:F\to L'$
		- assume as before that $L'$ is algebraic closure of $F^\tau$

	\item
		then \theoremname~\ref{theorem:algebraic embedding extensions}\
		implies, exists isomorphism, $\lambda:L\to L'$
		extending $\tau\circ \sigma^{-1}$ applied to $F^\sigma$

	\item
		let
			$S_\sigma$ \& $S_\tau$ be sets of embedding extensions of $\sigma$ to $E$ in $L$ and $L'$
			respectively
	\item
		then $\lambda$ induces map from $S_\sigma$ into $S_\tau$ with $\tilde{\sigma} \mapsto \lambda \circ \tilde{\sigma}$
		and $\lambda^{-1}$ induces inverse map from $S_\tau$ into $S_\sigma$,
		hence exists bijection between $S_\sigma$ and $S_\tau$, hence have same cardinality
	\eit
\eit

\begin{mydefinition}{separable degree of field extensions}
	above cardinality only depends on extension $E/F$,
	called \define{separable degree of $E$ over $F$},
	denoted by \define{$[E:F]_s$}
\end{mydefinition}


\myfoilhead{Multiplicativity of and upper bound on separable degree of field extensions}

\begin{mytheorem}{multiplicativity of separable degree of field extensions}
	for tower of algebraic field extensions, $k\subset F\subset E$,
	$$
		[E:k]_s
		=
		[E:F]_s
		[F:k]_s
	$$
\end{mytheorem}

\begin{mytheorem}{upper limit on separable degree of field extensions}
	for finite algebraic field extension, $k\subset E$
	$$
		[E:k]_s \leq [E:k]
	$$
\end{mytheorem}

\bit
\item
	\ie, separable degree is at most equal to degree (\ie, dimension) of field extension
\eit

\begin{mycorollary}{}
	for tower of algebraic field extensions, $k\subset F\subset E$,
	with $[E:k]<\infty$\
	$$
		[E:k]_s = [E:k]
	$$
	holds
	\iaoi\
	corresponding equality holds in every step of tower,
	\ie, for $E/F$ and $F/k$
\end{mycorollary}


\myfoilhead{Finite separable field extensions}

\begin{mydefinition}{finite separable field extensions}
	for finite algebraic field extension, $E/k$,
	with $[E:k]_s=[E:k]$,
	$E$,
	said to be \define{separable over $k$}
\end{mydefinition}

\begin{mydefinition}{separable algebraic elements}
	for field, $k$,
	$\alpha$, which is algebraic over $k$
	with $k(\alpha)$ being separable over $k$,
	said to be \define{separable over $k$}
\end{mydefinition}

\begin{myproposition}{separability and multiple roots}
	for field, $k$,
	$\alpha$, which is algebraic over $k$,
	is separable over $k$
	\iaoi\
	$\Irr(\alpha,k,X)$
	has no multiple roots
\end{myproposition}

\begin{mydefinition}{separable polynomials}
	for field, $k$,
	$f\in k[X]$ with no multiple roots,
	said to be \define{separable}
\end{mydefinition}

\begin{mylemma}{}
	for tower of algebraic field extensions,
	$k\subset F\subset K$,
	if $\alpha \in K$ is separable over $k$,
	then $\alpha$ is separable over $F$\
\end{mylemma}

\begin{mytheorem}{finite separable field extensions}
	for finite field extension, $E/k$,
	$E$ is separable over $k$
	\iaoi\
	every element of $E$ is separable over $k$
\end{mytheorem}


\myfoilhead{Arbitrary separable field extensions}

\begin{mydefinition}{arbitrary separable field extensions}
	for (not necessarily finite) field extension, $E/k$,
	$E$,
	of which
	every finitely generated subextension
	is separable over $k$,
	\ie,
	$$
		\left(
			\forall n\in\naturals\ \& \ \alpha_1,\ldots, \alpha_n \in E
		\right)
		\left(
			k(\alpha_1, \ldots, \alpha_n)
			\mbox{ is separable over }
			k
		\right)
	$$
	said to be \define{separable over $k$}
\end{mydefinition}

\begin{mytheorem}{separable field extensions}
	for algebraic extension, $E/k$,
	$E$, which is generated by family of elements, $\{\alpha_\lambda\}_{\lambda\in\Lambda}$,
	with every $\alpha_\lambda$ is separable over $k$,
	is separable over $k$\
\end{mytheorem}

\begin{mytheorem}{separable extensions are distinguished}
	separable extensions form distinguished class of extensions
\end{mytheorem}


\myfoilhead{Separable closure and conjugates}

\begin{mydefinition}{separable closure}
	for field, $k$,
	compositum of all separable extensions of $k$ in given algebraic closure \algclosure{k},
	called \define{separable closure of $k$},
	denoted by \define{$k^\mathrm{s}$ or \sepclosure{k}}
\end{mydefinition}

\begin{mydefinition}{conjugates of fields}
	for algebraic field extension, $E/k$,
	and embedding of $E$, $\sigma$, in \algclosure{k}\ over $k$,
	$E^\sigma$,
	called \define{conjugate of $E$ in \algclosure{k}}
\end{mydefinition}

\bit
\item
	smallest normal extension of $k$ containing $E$
	is compositum of all conjugates of $E$ in \algclosure{E}\
\eit

\begin{mydefinition}{conjugates of elements of fields}
	for field, $k$,
	$\alpha$ being algebraic over $k$,
	and
	distinct embeddings, $\sigma_1$, \ldots, $\sigma_r$ of $k(\alpha)$ into \algclosure{k} over $k$,
	$\alpha^{\sigma_1}$,
	\ldots,
	$\alpha^{\sigma_r}$,
	called \define{conjugates of $\alpha$ in \algclosure{k}}
\end{mydefinition}

\bit
\item
	$\alpha^{\sigma_1}$,
	\ldots,
	$\alpha^{\sigma_r}$
	are simply
	distinct roots of $\Irr(\alpha, k, X)$
\item
	smallest normal extension of $k$
	containing one of these conjugates
	is simply
	$k(\alpha^{\sigma_1}, \ldots, \alpha^{\sigma_r})$
\eit


\myfoilhead{Prime element theorem}

\begin{mytheorem}{prime element theorem}
	for finite algebraic field extension, $E/k$,
	exists $\alpha\in E$ such that $E=k(\alpha)$
	\iaoi\
	exists only finite \# fields, $F$, such that $k\subset F\subset E$;
	if $E$ is separable over $k$,
	exists such element, $\alpha$\
\end{mytheorem}

\begin{mydefinition}{primitive element of fields}
	for finite algebraic field extension, $E/k$,
	$\alpha\in E$ with $E=k(\alpha)$,
	called \define{primitive element of $E$ over $k$}
\end{mydefinition}


\myfoilhead{Finite fields}

\begin{mydefinition}{finite fields}
	for every prime number, $p$, and integer, $n\geq1$,
	exists finite field of order $p^n$,
	denoted by \define{\finitefield{p}{n}},
	uniquely determined as subfield of algebraic closure, \algclosure{\primefield{p}},
	which is splitting field of polynomial
	$$
		f_{p,n}(X) = X^{p^n} - X
	$$
	and whose elements are roots of $f_{p,n}$
\end{mydefinition}

\begin{mytheorem}{finite fields}
	for every finite field, $F$,
	exist prime number, $p$, and integer, $n\geq1$,
	such that $F=\finitefield{p}{n}$
\end{mytheorem}

\begin{mycorollary}{finite field extensions}
	for finite field, \finitefield{p}{n}, and integer, $m\geq1$,
	exists one and only one extension of degree, $m$,
	which is \finitefield{p}{mn}\
\end{mycorollary}

\begin{mytheorem}{multiplicative group of finite field}
	multiplicative group of finite field is cyclic\
\end{mytheorem}


\myfoilhead{Automorphisms of finite fields}

\begin{mydefinition}{Frobenius mapping}
	mapping
	$$
		\frobmap{p}{n}: \finitefield{p}{n} \to \finitefield{p}{n}
	$$
	defined by $x\mapsto x^p$,
	called \define{Frobenius mapping}
\end{mydefinition}

\bit
\item
	\frobmap{p}{n}\ is (ring) homeomorphism
	with $\Ker \frobmap{p}{n} = \{0\}$ since \finitefield{p}{n}\ is field,
	thus is injective (\propositionname~\ref{proposition:injectivity of field homeomorphism}),
	and surjective because \finitefield{p}{n}\ is finite,

\item
	thus, \frobmap{p}{n}\ is \emph{isomorphism
	leaving \primefield{p}\ fixed}
\eit

\begin{mytheorem}{group of automorphisms of finite fields}
	group of automorphisms of \finitefield{p}{n}\
	is cyclic of degree $n$,
	generated by \frobmap{p}{n}\
\end{mytheorem}

\begin{mytheorem}{group of automorphisms of finite fields over another finite field}
	for prime number, $p$, and integers, $m,n\geq1$,
	in any \algclosure{\primefield{p}},
	\finitefield{p}{n}\ is contained in \finitefield{p}{m}\
	\iaoi\
	$n$ divides $m$,
	\ie, exists $d\in\integers$ such that $m=dn$,
	in which case,
	\finitefield{p}{m}\ is \emph{normal and separable} over \finitefield{p}{n}\
	group of automorphisms of \finitefield{p}{m}\ over \finitefield{p}{n}\
	is cyclic of order, $d$,
	generated by $\frobmap{p}{m}^n$\
\end{mytheorem}


\titlefoil{Galois Theory}{Galois Theory}%
	\index{Galois theory}

\myfoilhead{What we will do to appreciate Galois theory}%
	\index{Galois theory}%
	\index{Galois theory!appreciation}

\bit
\item
	study
	\bit
	\item
		group of automorphisms of finite (and infinite) Galois extension (at length)\
	\item
		give examples, \eg, cyclotomic extensions, abelian extensions, (even) non-abelian ones
	\item
		leading into study of matrix representation of Galois group \& classifications
	\eit

\vitem
	have
	tools to prove
	\bit
	\item
		fundamental theorem of algebra
	\item
		insolvability of quintic polynomials
	\eit

\vitem
	mention unsolved problems
	\bit
	\item
		given finite group, exists Galois extension of $\rationals$ having this group as Galois group?
	\eit
\eit


\myfoilhead{Fixed fields}

\begin{mydefinition}{fixed field}%
		\index{field!fixed field}
	for field, $K$, and group of automorphisms, $G$, of $K$,
	$$
		\set{x\in K}{\forall \sigma\in G, x^\sigma = x}\subset K
	$$
	is subfield of $K$, and
	called \define{fixed field of $G$},
	denoted by \define{$K^G$}\
\end{mydefinition}

\bit
\item
	$K^G$ is subfield of $K$ because for every $x,y\in K^G$
	\bit
	\item
		$0^\sigma = 0 \Rightarrow 0\in K^G$
	\item
		$(x+y)^\sigma = x^\sigma + y^\sigma = x + y \Rightarrow x+y \in K^G$
	\item
		$(-x)^\sigma = - x^\sigma = - x \Rightarrow -x \in K^G$
	\item
		$1^\sigma = 1 \Rightarrow 1\in K^G$
	\item
		$(xy)^\sigma = x^\sigma y^\sigma = xy \Rightarrow xy\in K^G$
	\item
		$(x^{-1})^\sigma = (x^\sigma)^{-1} = x^{-1} \Rightarrow x^{-1} \in K^G$
	\eit
	hence, $K^G$ closed under addition \& multiplication,
	and is commutative division ring,
	thus field

\vitem
	$0,1\in K^G$, hence \emph{$K^G$ contains prime field}
\eit


\myfoilhead{Galois extensions and Galois groups}

\begin{mydefinition}{Galois extensions}%
		\index{Galois, \'{E}variste!Galois extension}%
		\index{Galois extension!algebraic extension}%
		\index{algebraic extension!Galois extension}
	algebraic extension, $K$, of field, $k$,
	which is normal and separable,
	said to be \define{Galois (extension of $k$)}\
	or \define{Galois over $k$}\
	considering $K$ as embedded in \algclosure{k};
	for convenience,
	sometimes say \define{$K/k$ is Galois}\
\end{mydefinition}

\begin{mydefinition}{Galois groups}%
		\index{Galois, \'{E}variste!Galois group}%
		\index{group!Galois group}
	for field, $k$ and its Galois extension, $K$,
	group of automorphisms of $K$ over $k$,
	called \define{Galois group of $K$ over $k$},
	denoted by \define{$G(K/k)$, $G_{K/k}$, $\Gal(K/k)$, or (simply) $G$}%
	\index{$G(K/k)$!Galois group!finite extension}%
	\index{$G_{K/k}$!Galois group!finite extension}%
	\index{$\Gal(K/k)$!Galois group!finite extension}%
	\index{$G$!Galois group!finite extension}
\end{mydefinition}

\begin{mydefinition}{Galois group of polynomials}
	for field, $k$, separable $f\in k[X]$ with $\deg f \geq 1$,
	and
	its splitting field, $K/k$,
	Galois group of $K$ over $k$ (\ie, $G(K/k)$),
	called \define{Galois group of $f$ over $k$}\
\end{mydefinition}

\begin{myproposition}{Galois group of polynomials and symmetric group}
	for field, $k$, separable $f\in k[X]$ with $\deg f \geq 1$,
	and
	its splitting field, $K/k$,
	$$
		f(X) = (X-\alpha_1) \cdots (X-\alpha_n)
	$$
	elements of Galois group of $f$ over $k$, $G$,
	permute roots of $f$,
	hence, exists injective homeomorphism
	of $G$ into $S_n$, \ie, symmetric group on $n$ elements\
\end{myproposition}


\myfoilhead{Fundamental theorem for Galois theory}

\begin{mytheorem}{fundamental theorem for Galois theory}%
	\index{fundamental theorem for Galois theory}%
	\index{fundamental theorem!for Galois theory}%
		\index{Galois theory}%
		\index{Galois, \'{E}variste!Galois theory}
	for finite Galois extension, $K/k$\
	%, and corresponding Galois group, $G$,
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		map $H \mapsto K^H$
		induces isomorphism between
%		\begin{center}
			set of subgroups of $G(K/k)$ \& set of intermediate fields
%		\end{center}
	\iitem
		subgroup, $H$, of $G(K/k)$,
		is normal
		\iaoi\
		$K^H/k$ is Galois
	\iitem
		for normal subgroup, $H$,
		$\sigma\mapsto \restrict{\sigma}{K^H}$
		induces isomorphism between $G(K/k)/H$\ and
		$G(K^H/k)$\
%		onto Galois group of $K^H$ over $k$\
	\eit
	\shrinkspacewithintheoremslike\
	(illustrated in \figref{diagrams for Galois main result})
\end{mytheorem}%
\shrinkspacewithintheoremslikehalf\
\bit
\item
	shall prove step by step
\eit

\begin{figure}
\begin{center}
	\mypsfrag{0}{$0$}
	\mypsfrag{H}{$H$}
	\mypsfrag{G}{$G(K/k)$}
	\mypsfrag{K}{$K$}
	\mypsfrag{E}{$K^H$}
	\mypsfrag{k}{$K^{G(K/k)}=k$}
	\includegraphics[width=.45\textwidth]{figures/py-fundamental-theorem-of-Galois-theory-psfragable}
	\idxfig{diagrams for Galois main result}
	\label{fig:diagrams for Galois main result}
\end{center}
\end{figure}
\vspace{-.5em}


\myfoilhead{Galois subgroups association with intermediate fields}

\begin{mytheorem}{Galois subgroups associated with intermediate fields - 1}
	for Galois extension, $K/k$,
	and intermediate field, $F$
	\shrinkspacewithintheoremslike
	\ibit
	\iitem
		$K/F$ is Galois \& $K^{G(K/F)} = F$, hence, $K^G = k$

	\iitem
		map
		$$
			F \mapsto G(K/F)
		$$
		induces \emph{injective} homeomorphism
		from set of intermediate fields
		to subgroups of $G$
	\eit
	\shrinkspacewithintheoremslike
	\proofref{theorem - Galois subgroups associated with intermediate fields}
\end{mytheorem}

\begin{mydefinition}{Galois subgroups associated with intermediate fields}
	for Galois extension, $K/k$, and intermediate field, $F$,\ % with $k\subset F\subset K$,
	subgroup, $G(K/F)\subset G(K/k)$,
	called \define{group associated with $F$},
	said to \define{belong to $F$}
\end{mydefinition}

\begin{mycorollary}{Galois subgroups associated with intermediate fields - 1}
	for Galois extension, $K/k$,
	and
	two intermediate fields, $F_1$ and $F_2$,
%
	$G(K/F_1) \cap G(K/F_2)$ belongs to $F_1F_2$,
	\ie,
	$$
		G(K/F_1) \cap G(K/F_2)
		= G(K/F_1F_2)
	$$
	\proofref{Galois subgroups associated with intermediate fields - 1}
\end{mycorollary}

\begin{mycorollary}{Galois subgroups associated with intermediate fields - 2}
	for Galois extension, $K/k$,
	and
	two intermediate fields, $F_1$ and $F_2$,
%
	smallest subgroup of $G$ containing $G(K/F_1)$ and $G(K/F_2)$
	belongs to
	$F_1\cap F_2$,\
	\ie
%	\ie,
	$$
		\bigcap_{G(K/F_1)\subset H, G(K/F_2)\subset H} \set{H}{H\subset G(K/k)}
		=
		G(K/(F_1\cap F_2))
	$$
%	\proofref{Galois subgroups associated with intermediate fields - 2}
\end{mycorollary}

\begin{mycorollary}{Galois subgroups associated with intermediate fields - 3}
	for Galois extension, $K/k$,
	and
	two intermediate fields, $F_1$ and $F_2$,
%
	$$
		F_1\subset F_2
		\mbox{ \iaoi\ }
		G(K/F_2)\subset G(K/F_1)
	$$
	\proofref{Galois subgroups associated with intermediate fields - 3}
\end{mycorollary}

%\begin{myproposition}{}
%	for normal extension, $K/k$,
%	and group of automorphisms of $K$ over $k$, $G$,
%	$K$ is separable over $K^G$
%\end{myproposition}
%
\begin{mycorollary}{}
	for finite separable field extension, $E/k$,
	the smallest normal extension of $k$ containing $E$, $K$,
	$K/k$ is finite Galois
	and exist only finite number of intermediate fields
\end{mycorollary}

\begin{mylemma}{}
	for algebraic separable extension, $E/k$,
	if every element of $E$ has degree no greater than $n$ over $k$
	for some $n\geq1$,
	$E$ is finite over $k$ and $[E:k]\leq n$
\end{mylemma}

\begin{mytheorem}{Artin's theorem}
	\label{theorem:Artins theorem}
	(Artin)
	for field, $K$,
	finite $\Aut(K)$ of order, $n$,
	and $k = K^{\Aut(K)}$,
	$K/k$ is Galois,
	$G(K/k)= \Aut(K)$,
	and $[K:k] = n$
\end{mytheorem}

\begin{mycorollary}{Galois subgroups associated with intermediate fields - 4}
	for finite Galois extension, $K/k$,
	every subgroup of $G(K/k)$ belongs to intermediate field
\end{mycorollary}

\begin{mytheorem}{Galois subgroups associated with intermediate fields - 2}
	for Galois extension, $K/k$,
	and intermediate field, $F$,
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		$F/k$ is normal extension
		\iaoi\
		$G(K/F)$ is normal subgroup of $G(K/k)$
	\iitem
		if $F/k$ is normal extension,
		map, $\sigma \mapsto \restrict{\sigma}{F}$,
		induces
		homeomorphism
		of $G(K/k)$ onto $G(F/k)$
		of which $G(K/F)$ is kernel,
		thus
		$$
			G(F/k) \isomorph G(K/k)/G(K/F)
		$$
	\eit
\end{mytheorem}


\myfoilhead{Proof for fundamental theorem for Galois theory}

\bit
\item
	finally, we prove \emph{fundamental theorem for Galois theory}\
	(\theoremname~\ref{theorem:fundamental theorem for Galois theory})

\vvitem
	assume $K/k$ is finite Galois extension
	and $H$ is subgroup of $G(K/k)$
	\bit
	\item
		\corollaryname~\ref{corollary:Galois subgroups associated with intermediate fields - 4}\
		implies $K^H$ is intermediate field,
		hence
		\theoremname~\ref{theorem:Galois subgroups associated with intermediate fields - 1}\
		implies
		$K/K^H$ is Galois,
		\theoremname~\ref{theorem:Artins theorem}\
		implies
		$G(K/K^H) = H$,
		thus,
		every $H$ is Galois

	\item
		map, $H\mapsto K^H$,
		induces homeomorphism, $\sigma$, of set of all subgroups of $G(K/k)$
		into set of intermediate fields

	\item
		$\sigma$ is \emph{injective}
		since
		for any two subgroups, $H$ and $H'$, of $G(K/k)$,
		if $K^H=K^{H'}$,
		then $H=G(K/K^H)=G(K/K^{H'})=H'$

	\item
		$\sigma$ is \emph{surjective}
		since
		for every intermediate field, $F$,
		\theoremname~\ref{theorem:Galois subgroups associated with intermediate fields - 1}\
		implies
		$K/F$ is Galois, $G(K/F)$ is subgroup of $G(K/k)$, and $K^{G(K/F)}=F$,
		thus,
		$\sigma(G(K/F)) = K^{G(K/F)}= F$

	\item
		\emph{therefore, $\sigma$ is isomorphism
		between
			set of all subgroups of $G(K/k)$
		and
			set of intermediate fields
			}

	\vitem
		since \theoremname~\ref{theorem:separable extensions are distinguished}\
		implies
		separable extensions are distinguished,
		$H^K/k$ is separable,
		thus
		\theoremname~\ref{theorem:Galois subgroups associated with intermediate fields - 2}\
		implies that $K^H/k$ is Galois \iaoi\ $G(K/K^H)$ is normal

	\vitem
		lastly,
		\theoremname~\ref{theorem:Galois subgroups associated with intermediate fields - 2}\
		implies that if $K^H/k$ is Galois,
		$G(H^K/k) \isomorph G(K/k) / H$
	\eit
\eit


\myfoilhead{Abelian and cyclic Galois extensions and groups}

\begin{mydefinition}{abelian Galois extensions}
	Galois extension with abelian Galois group,
	said to be \define{abelian}
\end{mydefinition}

\begin{mydefinition}{cyclic Galois extensions}
	Galois extension with cyclic Galois group,
	said to be \define{cyclic}
\end{mydefinition}

\begin{mycorollary}{}
	for Galois extension, $K/k$,
	and
	intermediate field, $F$,
\shrinkspacewithintheoremslike\
\ibit
\iitem
	if $K/k$ is abelian, $F/k$ is Galois and abelian
\iitem
	if $K/k$ is cyclic, $F/k$ is Galois and cyclic
\eit
\end{mycorollary}

\begin{mydefinition}{maximum abelian extension}
	for field, $k$,
	compositum of all abelian Galois extensions of $k$
	in given \algclosure{k},
	called \define{maximum abelian extension of $k$},
	denoted by \define{\maxabext{k}}
\end{mydefinition}


\myfoilhead{Theorems and corollaries about Galois extensions}

\begin{mytheorem}{}
	for Galois extension, $K/k$,
	and arbitrary extension, $F/k$,
	where $K$ and $F$ are subfields of common field,
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		$KF / F$ and $K/(K\cap F)$ are Galois extensions
	\iitem
		map $$\sigma \mapsto \sigma|K$$
		induces
		isomorphism
		between
		$G(KF / F)$ and $G(K/(K\cap F))$
	\eit
	\shrinkspacewithintheoremslike
	theorem illustrated in \figref{diagram for Galois lifting}
\end{mytheorem}

\begin{figure}
\begin{center}
	\galoisf{3em}
	\idxfig{diagram for Galois lifting}
	\label{fig:diagram for Galois lifting}
\end{center}
\end{figure}

\begin{mycorollary}{}
	for finite Galois extension, $K/k$,
	and arbitrary extension, $F/k$,
	where $K$ and $F$ are subfields of common field,
	$$
		[KF:F] \mbox{ divides } [F:k]
	$$
\end{mycorollary}

\begin{mytheorem}{}
	for Galois extensions, $K_1/k$ and $K_2/k$,
	where $K_1$ and $K_2$ are subfields of common field,
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		$K_1K_2/k$ is Galois extension
	\iitem
		map
		$$
			\sigma \mapsto (\restrict{\sigma}{K_1}, \restrict{\sigma}{K_2})
		$$
		of $G(K_1K_2/k)$ into $G(K_1/k) \times G(K_2/k)$
		is injective;
		if $K_1\cap K_2=k$,
		map is isomorphism
	\eit
	theorem illustrated in \figref{diagram for Galois two-side lifting}
\end{mytheorem}

\begin{figure}
\begin{center}
	\galoiss{4em}
	\idxfig{diagram for Galois two-side lifting}
	\label{fig:diagram for Galois two-side lifting}
\end{center}
\end{figure}

\begin{mycorollary}{}
	for $n$ Galois extensions, $K_i/k$,
	where $K_1$, \ldots, $K_n$ are subfields of common field
	and
	$K_{i+1}\cap(K_1\cdots K_i) = k$ for $i=1,\ldots,n-1$,
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		$K_1\cdots K_n/k$ is Galois extension
	\iitem
		map
		$$
			\sigma \mapsto (\restrict{\sigma}{K_1}, \ldots, \restrict{\sigma}{K_n})
		$$
		induces
		isomorphism
		of $G(K_1\cdots K_n/k)$ onto $G(K_1/k) \times \cdots \times G(K_n/k)$
	\eit
\end{mycorollary}

\begin{mycorollary}{}
	for Galois extension, $K/k$,
	where $G(K/k)$ can be written as $G_1\times \cdots \times G_n$,
	and
	$K_1$, \ldots, $K_n$,
	each of which is
	fixed field of
	$$
		G_1 \times \cdots \times \underbrace{\{e\}}_{i\mathrm{th\ position}} \times \cdots \times G_n
	$$
	\shrinkspacewithintheoremsliket\
	\ibit
	\iitem
		$K_1/k$, \ldots, $K_n/k$ are Galois extensions
	\iitem
		$G(K_i/k)=G_i$ for $i=1,\ldots,n$
	\iitem
		$K_{i+1}\cap(K_1\cdots K_i) = k$ for $i=1,\ldots,n-1$
	\iitem
		$K=K_1\cdots K_n$
	\eit
\end{mycorollary}

\begin{mytheorem}{}
	assume all fields are subfields of common field
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		for two abelian Galois extensions, $K/k$ and $L/k$,
		$KL/k$ is abelian Galois extension
	\iitem
		for abelian Galois extension, $K/k$,
		and any extension, $E/k$,
		$KE/E$ is abelian Galois extension
	\iitem
		for abelian Galois extension, $K/k$, and intermediate field, $E$,
		both $K/E$ and $E/k$ are abelian Galois extensions
	\eit
\end{mytheorem}


\myfoilhead{Solvable and radical extensions}

\begin{mydefinition}{sovable extensions}
	finite separable extension, $E/k$,
	such that
	Galois group
	of
	smallest Galois extension, $K/k$,
	containing $E$
	is solvable,
	said to be \define{solvable}
\end{mydefinition}

\begin{mytheorem}{solvable extensions are distinguished}
	solvable extensions form distinguished class of extensions
\end{mytheorem}

\begin{mydefinition}{solvable by radicals}
	finite extension, $F/k$,
	such that it is separable
	and exists finite extension, $E/k$, containing $F$
	admitting tower decomposition
	$$
		k = E_0 \subset E_1 \subset \cdots \subset E_m = E
	$$
	with $E_{i+1}/E_i$ is obtained by adjoining root of
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		unity, or
	\iitem
		$X^n=a$ with $a\in E_i$, and $n$ prime to characteristic, or
	\iitem
		$X_p-X-a$ with $a\in E_i$ if $p$ is positive characteristic
	\eit
	\shrinkspacewithintheoremslike\
	said to be \define{solvable by radicals}
\end{mydefinition}

\begin{mytheorem}{extensions solvable by radicals}
	separable extension, $E/k$,
	is solvable by radicals
	\iaoi\
	it is solvable
\end{mytheorem}


\myfoilhead{Applications of Galois theory}

\begin{mytheorem}{insolvability of quintic polynomials}
	general equation of degree, $n$,
	cannot be solved by radicals
	for $n\geq5$
	(implied by
	\definitionname~\ref{definition:Galois group of polynomials},
	\propositionname~\ref{proposition:Galois group of polynomials and symmetric group},
	\theoremname~\ref{theorem:extensions solvable by radicals},
	and
	\theoremname~\ref{theorem:solvability of finite symmetric groups})
\end{mytheorem}

\vfill
\begin{mytheorem}{fundamental theorem of algebra}%
	\index{fundamental theorem of algebra}%
	\index{fundamental theorem!of algebra}
	$f\in \complexes[X]$ of degree, $n$,
	has precisely $n$ roots in \complexes\
	(when counted with multiplicity),
	hence
	\complexes\ is algebraically closed
\end{mytheorem}
\vfill

}{}


\yesnoexec{\ranalysis}{
\TITLEFOIL{Real Analysis}{Real-Analysis}
\nocite{Royden:88}

\titlefoil{Set Theory}{set-theory}

\myfoilhead{Some principles}


\begin{myprinciple}{principle of mathematical induction}
	$$
		P(1) \& [P(n\Rightarrow P(n+1)] \Rightarrow (\forall n \in \naturals)P(n)
	$$
\end{myprinciple}

\vfill
\begin{myprinciple}{well ordering principle}
	\label{principle:well-ordering principle - smallest element}
	each nonempty subset of $\naturals$ has a smallest element
\end{myprinciple}

\vfill
\begin{myprinciple}{principle of recursive definition}
	for $f:X\to X$ and $a\in X$,
	exists unique infinite sequence $\langle x_n\rangle_{n=1}^\infty\subset X$
	such that
	$$
		x_1=a
	$$
	and
	$$
		\left(
			\forall n \in \naturals
		\right)
		\left(
			x_{n+1} = f(x_n)
		\right)
	$$
\end{myprinciple}

\bit
\vitem
	note that
%	\begin{quote}
		\principlename~\ref{principle:principle of mathematical induction}
			$\Leftrightarrow$
		\principlename~\ref{principle:well-ordering principle - smallest element}
			$\Rightarrow$
		\principlename~\ref{principle:principle of recursive definition}
%	\end{quote}
\eit


\myfoilhead{Some definitions for functions}

\begin{mydefinition}{functions}
	for $f:X\to Y$

%	\vspace{-.5cm}
	\bit
	\item
		terms, \define{map} and \define{function}, exterchangeably used%
			\index{maps}%
			\index{functions}

	\vitem
		$X$ and $Y$, called \define{domain of $f$} and \define{codomain of $f$} respectively%
			\index{domain!functions}%
			\index{functions!domain}%
			\index{codomain!functions}%
			\index{functions!codomain}

	\vitem
		$\set{f(x)}{x\in X}$, called \define{range of $f$}%
			\index{range!functions}%
			\index{functions!range}

	\vitem
		for $Z\subset Y$, $f^{-1}(Z) = \set{x\in X}{f(x)\in Z}\subset X$, called \define{preimage} or \define{inverse image of $Z$ under $f$}%
			\index{preimage!functions}%
			\index{functions!preimage}%
			\index{inverse image!functions}%
			\index{functions!inverse image}

	\vitem
		for $y\in Y$, $f^{-1}(\{y\})$, called \define{fiber of $f$ over $y$}%
			\index{fiber!functions}%
			\index{functions!fiber}%

	\vitem
		$f$, called \define{injective} or \define{injection} or \define{one-to-one}
			if $\left( \forall x\neq v \in X \right) \left( f(x) \neq f(v) \right)$%
				\index{injective!functions}%
				\index{functions!injective}%
				\index{injection!functions}%
				\index{functions!injection}%
				\index{one-to-one!functions}%
				\index{functions!one-to-one}


	\vitem
		$f$, called \define{surjective} or \define{surjection} or \define{onto}
			if $\left( \forall x \in X \right) \left( \exists y in Y \right) (y=f(x))$%
				\index{surjective!functions}%
				\index{functions!surjective}%
				\index{surjection!functions}%
				\index{functions!surjection}%
				\index{onto!functions}%
				\index{functions!onto}

	\vitem
		$f$, called \define{bijective} or \define{bijection} if $f$ is both injective and surjective,
			in which case, $X$ and $Y$, said to be \define{one-to-one correspondece} or \define{bijective correspondece}%
				\index{bijective!functions}%
				\index{functions!bijective}%
				\index{bijection!functions}%
				\index{functions!bijection}%
				\index{one-to-one correspondece!functions}%
				\index{functions!one-to-one correspondece}%
				\index{bijective correspondece!functions}%
				\index{functions!bijective correspondece}

	\vitem
		$g:Y\to X$, called \define{left inverse} if $g\circ f$ is identity function%
				\index{left inverse!functions}%
				\index{functions!left inverse}

	\vitem
		$h:Y\to X$, called \define{right inverse} if $f\circ h$ is identity function%
				\index{right inverse!functions}%
				\index{functions!right inverse}
	\eit
\end{mydefinition}


\myfoilhead{Some properties of functions}

\begin{mylemma}{functions}
	for $f:X\to Y$

%	\vspace{-.5cm}
	\bit
	\item
		$f$ is injective \iaoi\ $f$ has left inverse

	\vitem
		$f$ is surjective \iaoi\ $f$ has right inverse

	\vitem
		hence, $f$ is bijective \iaoi\ $f$ has both left and right inverse
		because if $g$ and $h$ are left and right inverses respectively,
		$g = g \circ (f\circ h) = (g\circ f)\circ h = h$

	\vitem if $|X|=|Y|<\infty$,
		$f$ is injective
			\iaoi\
		$f$ is surjective
			\iaoi\
		$f$ is bijective
	\eit
\end{mylemma}


\myfoilhead{Countability of sets}

\bit
	\item set $A$ is countable if range of some function whose domain is $\naturals$

	\vitem $\naturals$, $\integers$, $\rationals$: countable

	\vitem $\reals$: \emph{not} countable
\eit


\myfoilhead{Limit sets}

\bit
\item
	for sequence, \seq{A_n}, of subsets of $X$
	\bit
	\item
		\define{limit superior or limsup of \seq{A_n}}, defined by%
			\index{limit superior (limsup)!set}%
			\index{set!limit superior (limsup)}
		\[
			\limsup \seq{A_n} = \bigcap_{n=1}^\infty \bigcup_{m=n}^\infty A_m
		\]

	\item
		\define{limit inferior or liminf of \seq{A_n}}, defined by%
			\index{limit inferior (liminf)!set}%
			\index{set!limit inferior (liminf)}
		\[
			\liminf \seq{A_n} = \bigcup_{n=1}^\infty \bigcap_{m=n}^\infty A_m
		\]
	\eit

\vitem always
	\[
		\liminf \seq{A_n} \subset
		\limsup \seq{A_n}
	\]

\vitem when $\liminf \seq{A_n} = \limsup \seq{A_n}$, sequence, \seq{A_n},
	said to \define{converge to it}, denote%
		\index{convergence!of set}%
		\index{set!convergence of sequence}
	\[
		\lim \seq{A_n} = \liminf \seq{A_n} = \limsup \seq{A_n} = A
	\]
\eit


\myfoilhead{Algebras of sets}

\bit
	\item collection $\alg$ of subsets of $X$ called \define{algebra} or \define{Boolean algebra} if%
		\index{algebra}%
		\index{Boolean algebra}%
	\[
		(\forall A, B \in \alg) (A\cup B\in\alg)
		\mbox{ and }
		(\forall A \in \alg) (\compl{A}\in\alg)
	\]
	\bit
		\item $(\forall A_1, \ldots, A_n \in \alg)(\cup_{i=1}^n A_i \in \alg)$
		\item $(\forall A_1, \ldots, A_n \in \alg)(\cap_{i=1}^n A_i \in \alg)$
	\eit

	\vitem algebra $\alg$ called \define{$\sigma$-algebra} or \define{Borel field} if%
		\index{$\sigma$-algebra}%
		\index{Borel field}%
		\index{Borel, F\'{e}lix \'{E}douard Justin \'{E}mile!field}%
	\bit
		\item every union of a countable collection of sets in $\alg$ is in $\alg$, \ie,
		\[
			(\forall \seq{A_i})(\cup_{i=1}^\infty A_i \in \alg)
		\]
	\eit

	\vitem given sequence of sets in algebra $\alg$, $\seq{A_i}$, exists disjoint sequence, $\seq{B_i}$ such that
		\[
			B_i \subset A_i \mbox{ and }
			\bigcup_{i=1}^\infty B_i = \bigcup_{i=1}^\infty A_i
		\]
\eit


\labelfoilhead{Algebras generated by subsets}

\bit
	\item \define{algebra generated by} collection of subsets of $X$, $\coll$, can be found by%
		\index{algebra generated by}%
		\index{algebra!generated by}%
		\index{generated by!algebra}
		\[
			\alg =
			\bigcap \set{\algk{B}}{\algk{B} \in \collF}
		\]
		where \collF\ is family of all algebras containing $\coll$
	\bit
		\item smallest algebra $\alg$ containing $\coll$, \ie,%
			\index{smallest algebra containing subsets}%
			\index{algebra!smallest containing subsets}
		\[
			(\forall \algk{B} \in \collF)(\alg \subset \algk{B})
		\]
	\eit

	\vitem \define{$\sigma$-algebra generated by} collection of subsets of $X$, $\coll$, can be found by%
		\index{$\sigma$-algebra!generated by subsets}%
		\index{generated by!$\sigma$-algebra!by subsets}%
		\[
			\alg=
			\bigcap \set{\algk{B}}{\algk{B} \in \collG}
		\]
		where $\collG$ is family of all $\sigma$-algebras containing $\coll$
	\bit
		\item smallest $\sigma$-algebra $\alg$ containing $\coll$, \ie,%
			\index{smallest $\sigma$-algebra containing subsets}%
			\index{$\sigma$-algebra!smallest containing subsets}
		\[
			(\forall \algk{B} \in \collG)(\alg \subset \algk{B})
		\]
	\eit
\eit


\myfoilhead{Relation}

\bit
	\item $x$ said to \define{stand in relation} $\rel$ to $y$\index{relation!stand in relation},
		denoted by $\relxy{x}{y}$

	\vitem $\rel$ said to \define{be relation on} $X$ if $\relxy{x}{y}$ $\Rightarrow$ $x\in X$ and $y\in X$
		\index{relation!be relation on}

	\vitem $\rel$ is
	\bit
		\vitem transitive if $\relxy{x}{y}$ and $\relxy{y}{z}$ $\Rightarrow$ $\relxy{x}{z}$
		\vitem symmetric if $\relxy{x}{y} = \relxy{y}{x}$
		\vitem reflexive if $\relxy{x}{x}$
		\vitem antisymmetric if $\relxy{x}{y}$ and $\relxy{y}{x}$ $\Rightarrow$ $x=y$
	\eit

	\vitem $\rel$ is
	\bit
		\vitem \define{equivalence relation} if transitive, symmetric, and reflexive, \eg, modulo%
			\index{equivalence relation}%
		\vitem \define{partial ordering} if transitive and antisymmetric, \eg, ``$\subset$''%
			\index{partial ordering}%
			\index{ordering!partial}
		\vitem \define{linear (or simple) ordering} if transitive, antisymmetric, and $\relxy{x}{y}$ or $\relxy{y}{x}$ for all $x,y\in X$%
			\index{linear ordering}%
			\index{simple ordering}%
			\index{ordering!linear}%
			\index{ordering!simple}
		\bit
			\iitem \eg, ``$\geq$'' linearly orders $\reals$ while ``$\subset$'' does not $\powerset(X)$%
		\eit
	\eit
%
%	\vitem \name{Hausdorff maximal principle -}\index{Hausdorff, Felix!maximal principle}
%		\pagelabel{Hausdorff-maximal-principle}
%		for any \emph{partial ordering}, $\prec$, on set $X$,
%		exists \emph{maximal linearly ordered} subset $S$ of $X$,
%		\ie,
%		$S$ is linearly ordered,
%		and
%		if $S\subset T\subset X$ linearly ordered,
%		then $S=T$
\eit


\myfoilhead{Ordering}

\bit
	\item given partial order, $\prec$, $a$ is
	\bit
		\vitem a first/smallest/least element if $x \neq a \Rightarrow a\prec x$
		\vitem a last/largest/greatest element if $x \neq a \Rightarrow x\prec a$
		\vitem a minimal element if $x \neq a \Rightarrow x \not\prec a$
		\vitem a maximal element if $x \neq a \Rightarrow a \not\prec x$
	\eit

	\vvitem partial ordering $\prec$ is
	\bit
		\vitem strict partial ordering if $x\not\prec x$
		\vitem reflexive partial ordering if $x\prec x$
	\eit

	\vvitem strict linear ordering $<$ is
	\bit
		\vitem \define{well ordering} for $X$ if every nonempty set contains a first element
	\eit
\eit


\myfoilhead{Axiom of choice and equivalent principles}

\begin{myaxiom}{axiom of choice}
	given a collection of nonempty sets, $\coll$,
	there exists $f:\coll\ \to \cup_{A\in\coll} A $ such that
	$$
		\left(
			\forall A\in\coll\
		\right)
		\left(
			f(A) \in A
		\right)
	$$
\end{myaxiom}

\bit
\item [-]
	also called \define{multiplicative axiom}%
		\index{multiplicative axiom}%
		\index{axiom!multiplicative axiom}\
		- preferred to be called to axiom of choice by Bertrand Russell
		for reason writte \foilref{Infinite direct product}\
\item [-]
	no problem when \coll\ is finite
\item [-]
	need axiom of choice when \coll\ is not finite
\eit
\vfill

\begin{myprinciple}{Hausdorff maximal principle}%
		\index{Hausdorff, Felix!maximal principle}
	for particial ordering $\prec$ on $X$,
	exists a maximal linearly ordered subset $S\subset X$,
	\ie,
	$S$ is linearity ordered by $\prec$
	and if $S\subset T\subset X$ and $T$ is linearly ordered by $\prec$,
	$S=T$
\end{myprinciple}

\vfill

\begin{myprinciple}{well-ordering principle}
	every set $X$ can be well ordered,
		\ie,
	there is a relation $<$ that well orders $X$
\end{myprinciple}

\bit
\item
	note that
	\axiomname~\ref{axiom:axiom of choice}
	$\Leftrightarrow$
	\principlename~\ref{principle:Hausdorff maximal principle}
	$\Leftrightarrow$
	\principlename~\ref{principle:well-ordering principle}
\eit


\labelfoilhead{Infinite direct product}

\begin{mydefinition}{direct product}
	for collection of sets, \seq{X_\lambda}, with index set, $\Lambda$,
	$$
		\bigtimes_{\lambda\in\Lambda} X_\lambda
	$$
	called \define{direct product}
	\bit
	\item [-]
		for $z=\seq{x_\lambda}\in\bigtimes X_\lambda$,
		$x_\lambda$ called \define{$\lambda$-th coordinate of $z$}
	\eit
\end{mydefinition}

\bit
\item
	if one of $X_\lambda$ is empty, $\bigtimes X_\lambda$ is empty

\item
	\emph{axiom of choice} is equivalent to converse, \ie,
		if none of $X_\lambda$ is empty, $\bigtimes X_\lambda$ is not empty

	if one of $X_\lambda$ is empty, $\bigtimes X_\lambda$ is empty

\item
	this is why Bertrand Russell prefers \emph{multiplicative axiom} to \emph{axiom of choice} for name of axiom (\axiomname~\ref{axiom:axiom of choice})%
		\index{multiplicative axiom}%
		\index{axiom!multiplicative axiom}\
\eit


\titlefoil{Real Number System}{real-number-system}

\myfoilhead{Field axioms}

\bit
	\item field axioms - for every $x,y,z\in\field$
	\bit
		\vitem $(x+y)+z= x+(y+z)$ - additive associativity
		\vitem $(\exists 0\in\field)(\forall x\in\field)(x+0=x)$ - additive identity
		\vitem $(\forall x\in\field)(\exists w\in\field)(x+w=0)$ - additive inverse
		\vitem $x+y= y+x$ - additive commutativity
		\vvitem $(xy)z= x(yz)$ - multiplicative associativity
		\vitem $(\exists 1\neq0\in\field)(\forall x\in\field)(x\cdot 1=x)$ - multiplicative identity
		\vitem $(\forall x\neq0\in\field)(\exists w\in\field)(xw=1)$ - multiplicative inverse
		\vvitem $x(y+z) = xy + xz$ - distributivity
		\vitem $xy= yx$ - multiplicative commutativity
	\eit

	\vvitem system (set with $+$ and $\cdot$) satisfying axiom of field called \define{field}
	\bit
		\vitem \eg, field of module $p$ where $p$ is prime, \primefield{p}\
%		\vitem allows elementary algebra including linear algebra, \eg, solving linear system
	\eit
\eit


\myfoilhead{Axioms of order}

\bit
	\item axioms of order - subset, $\field_{++}\subset \field$, of positive (real) numbers satisfies
	\bit
		\vitem $x,y\in \field_{++} \Rightarrow x+y\in \field_{++}$
		\vitem $x,y\in \field_{++} \Rightarrow xy\in \field_{++}$
		\vitem $x\in \field_{++} \Rightarrow -x\not\in \field_{++}$
		\vitem $x\in \field \Rightarrow x=0\lor x\in \field_{++} \lor -x \in \field_{++}$
	\eit
	\vvitem system satisfying field axioms \& axioms of order called \define{ordered field}
	\bit
		\item \eg, set of real numbers ($\reals$), set of rational numbers ($\rationals$)
	\eit
\eit


\myfoilhead{Axiom of completeness}

\bit
	\item completeness axiom
	\bit
		\item every nonempty set $S$ of real numbers which has an upper bound has a least upper bound,
		\ie,
		\[
			\set{l}{(\forall x\in S)(l\leq x)}
		\]
		has least element.
		\item use $\inf S$ and $\sup S$ for least and greatest element (when exist)
	\eit

	\vitem ordered field that is complete is \define{complete ordered field}%
		\index{complete!ordered field}
	\bit
		\item \eg, $\reals$ (with $+$ and $\cdot$)
	\eit

	\vvitem [$\Rightarrow$] axiom of Archimedes
	\bit
		\item given any $x\in\reals$, there is an integer $n$ such that $x<n$
	\eit

	\vitem [$\Rightarrow$] corollary
	\bit
		\item given any $x<y \in \reals$, exists $r\in\rationals$ such tat
		$
			x < r < y
		$
	\eit
\eit


\myfoilhead{Sequences of $\reals$}

\bit
	\item sequence of $\reals$ denoted by $\seq{x_i}_{i=1}^\infty$ or $\seq{x_i}$
	\bit
		\item mapping from $\naturals$ to $\reals$
	\eit

	\vitem limit of $\seq{x_n}$ denoted by $\lim_{n\to\infty} x_n$ or $\lim x_n$ - defined by $a\in\reals$
		\[
			(\forall \epsilon>0)(\exists N\in\naturals) (n \geq N \Rightarrow |x_n-a|<\epsilon)
		\]
	\bit
		\item $\lim x_n$ unique if exists
	\eit

	\vitem $\seq{x_n}$ called Cauchy sequence if
		\[
			(\forall \epsilon>0)(\exists N\in\naturals) (n,m \geq N \Rightarrow |x_n-x_m|<\epsilon)
		\]

	\vitem Cauchy criterion - characterizing complete metric space (including $\reals$)%
		\index{complete!metric spaces}
	\bit
		\item sequence converges \iaoi\ Cauchy sequence
	\eit
	\eit


	\myfoilhead{Other limits}

	\bit
		\item cluster point of $\seq{x_n}$ - defined by $c\in\reals$
		\[
			(\forall \epsilon>0, N\in\naturals)(\exists n>N)(|x_n-c|<\epsilon)
		\]

		\vitem limit superior or limsup of $\seq{x_n}$
		\[
			\limsup x_n = \inf_n \sup_{k>n} x_k
		\]

		\vitem limit inferior or liminf of $\seq{x_n}$
		\[
			\liminf x_n = \sup_n \inf_{k>n} x_k
		\]

		\vvitem $\liminf x_n \leq \limsup x_n$

		\vitem $\seq{x_n}$ converges \iaoi\ $\liminf x_n = \limsup x_n$ (=$\lim x_n$)
	\eit


\myfoilhead{Open and closed sets}

\bit
	\item $O$ called \define{open} if
	\[
		(\forall x\in O)(\exists \delta>0)(y\in\reals)(|y-x|<\delta\Rightarrow y\in O)
	\]
	\bit
		\item intersection of finite collection of open sets is open
		\item union of any collection of open sets is open
	\eit

	\vitem $\closure{E}$ called \define{closure} of $E$ if
	\[
		(\forall x \in \closure{E} \ \&\ \delta>0)(\exists y\in E)(|x-y|<\delta)
	\]

	\vitem $F$ called \define{closed} if
	\[
		F = \closure{F}
	\]
	\bit
		\item union of finite collection of closed sets is closed
		\item intersection of any collection of closed sets is closed
	\eit
\eit


\myfoilhead{Open and closed sets - facts}
\pagelabel{page:open-closed-fact}

\bit
	\item \fact{every open set is union of countable collection of disjoint open intervals}%
	\idximportant{open set in $\reals$ is union of countable collection of disjoint open intervals}
	\pagelabel{page:open-set-in-reals-is-union-of-countable-collection-of-disjoint-open-intervals}

	\vitem (Lindel\"{o}f) any collection \coll\ of open sets has a countable subcollection $\seq{O_i}$ such that
	\[
		\bigcup_{O\in\coll} O = \bigcup_{i} O_i
	\]
	\bit
		\item equivalently,
			any collection \collk{F}\ of closed sets has a countable subcollection $\seq{F_i}$ such that
		\[
			\bigcap_{O\in\collk{F}} F = \bigcap_{i} F_i
		\]
	\eit
\eit


\myfoilhead{Covering and Heine-Borel theorem}
\pagelabel{page:heine-borel-theorem}

\bit
	\item collection $\coll$ of sets called \define{covering} of $A$ if
		\[
			A \subset \bigcup_{O\in\coll} O
		\]
	\bit
		\item $\coll$ said to \define{cover} $A$
		\item $\coll$ called \define{open covering} if every $O\in\coll$ is open
		\item $\coll$ called \define{finite covering} if $\coll$ is finite
	\eit

	\vitem \name{Heine-Borel theorem\index{Heine-Borel theorem}\index{Heine, Heinrich Eduard!Heine-Borel theorem}\index{Borel, F\'{e}lix \'{E}douard Justin \'{E}mile!Heine-Borel theorem} -}
%	\vitem \name{Heine-Borel theorem -}
		for any closed and bounded set, every open covering has finite subcovering

	\vitem corollary
	\bit
		\item any collection $\coll$ of closed sets including at least one bounded set every finite subcollection of which has nonempty intersection
		has nonempty intersection.

	\eit
\eit


\myfoilhead{Continuous functions}

\bit
	\item $f$ (with domain $D$) called \define{continuous at} $x$ if
	\[
		(\forall\epsilon >0)(\exists \delta>0)(\forall y\in D)(|y-x|<\delta \Rightarrow |f(y)-f(x)|<\epsilon)
	\]

	\vitem $f$ called \define{continuous on} $A\subset D$ if $f$ is continuous at every point in $A$

	\vitem $f$ called \define{uniformly continuous on} $A\subset D$ if
	\[
		(\forall\epsilon >0)(\exists \delta>0)(\forall x,y\in D)(|x-y|<\delta \Rightarrow |f(x)-f(y)|<\epsilon)
	\]
\eit


\myfoilhead{Continuous functions - facts}

\bit
	\item $f$ is continuous \iaoi\ for every open set $O$ (in co-domain), $f^{-1}(O)$ is open

	\vitem $f$ continuous on closed and bounded set is uniformly continuous

	\vvitem \name{extreme value theorem -}
		$f$ continuous on closed and bounded set, $F$, is \emph{bounded on $F$ and assumes its maximum and minimum on $F$}
		\[
			(\exists x_1, x_2 \in F)(\forall x\in F)(f(x_1) \leq f(x) \leq f(x_2))
		\]

	\vitem \name{intermediate value theorem -}
		for $f$ continuous on $[a,b]$ with $f(a) \leq f(b)$,
		\[
			(\forall d)(f(a) \leq d \leq f(b))(\exists c\in[a,b])(f(c) = d)
		\]
\eit


\myfoilhead{Borel sets and Borel $\sigma$-algebra}
\pagelabel{page:borel-algebra}

\bit
	\item \define{Borel set}%
		\index{Borel sets}\index{Borel, F\'{e}lix \'{E}douard Justin \'{E}mile!sets}
	\bit
		\item any set that can be formed from open sets (or, equivalently, from closed sets)
		through the operations of countable union, countable intersection, and relative complement
	\eit

	\vitem \define{Borel algebra} or \define{Borel $\sigma$-algebra}%
		\index{Borel $\sigma$-algebra}\index{Borel, F\'{e}lix \'{E}douard Justin \'{E}mile!$\sigma$-algebra}
	\bit
		\item
		\emph{smallest $\sigma$-algebra containing all open sets}

		\vitem also
		\bit
			\item [-] smallest $\sigma$-algebra containing all closed sets
			\item [-] smallest $\sigma$-algebra containing all open intervals
				(due to statement on page~\pageref{page:open-set-in-reals-is-union-of-countable-collection-of-disjoint-open-intervals})
		\eit
	\eit
\eit
\vfill


\myfoilhead{Various Borel sets}

\bit
	\item countable union of closed sets (in $\reals$),
		called \define{an $F_\sigma$} ($F$ for closed \& $\sigma$ for sum)%
		\index{Borel sets!$F_\sigma$}
	\bit
		\item thus, every countable set,
			every closed set,
			every open interval,
			every open sets,
			is an $F_\sigma$ (note $(a,b)=\bigcup_{n=1}^\infty [a+1/n,b-1/n]$)
		\item countable union of sets in $F_\sigma$ again is an $F_\sigma$
	\eit

	\vitem countable intersection of open sets
		called \define{a $G_\delta$} ($G$ for open \& $\delta$ for durchschnitt - average in German)\
		\index{Borel sets!$G_\delta$}
	\bit
		\item complement of $F_\sigma$ is a $G_\delta$ and vice versa
	\eit

	\vvitem $F_\sigma$ and $G_\delta$ are simple types of Borel sets

	\vvitem countable intersection of $F_\sigma$'s is $F_{\sigma\delta}$,
		countable union of $F_{\sigma\delta}$'s is $F_{\sigma\delta\sigma}$,
		countable intersection of $F_{\sigma\delta\sigma}$'s is $F_{\sigma\delta\sigma\delta}$, \etc,
		\& likewise for $G_{\delta \sigma \ldots}$%
		\index{Borel sets!$F_{\sigma\delta}$}%
		\index{Borel sets!$G_{\delta\sigma}$}

	\vitem below are all classes of Borel sets, but not every Borel set belongs to one of these classes
	\[
		F_{\sigma},
		F_{\sigma\delta},
		F_{\sigma\delta\sigma},
		F_{\sigma\delta\sigma\delta},
		\ldots,
		G_{\delta},
		G_{\delta\sigma},
		G_{\delta\sigma\delta},
		G_{\delta\sigma\delta\sigma},
		\ldots,
	\]
\eit


\yesnoexec{\meastheory}{

\titlefoil{Lebesgue Measure}{lebesgue-measure}%
\index{Lebesgue measure}

\myfoilhead{Riemann integral}

\bit
	\item Riemann integral%
		\index{Riemann integral}%
		\index{Riemann, Bernhard!Riemann integral}
	\bit
		\vitem partition induced by sequence $\seq{x_i}_{i=1}^n$ with $a=x_1<\cdots<x_n=b$
		\vitem lower and upper sums
		\bit
			\vitem $L(f,\seq{x_i}) = \sum_{i=1}^{n-1} \inf_{x\in[x_i,x_{i+1}]} f(x) (x_{i+1}-x_{i})$
			\item $U(f,\seq{x_i}) = \sum_{i=1}^{n-1} \sup_{x\in[x_i,x_{i+1}]} f(x) (x_{i+1}-x_{i})$
		\eit
		\vitem always holds: $L(f,\seq{x_i}) \leq U(f,\seq{y_i})$, hence
		\[
			\sup_{\seq{x_i}} L(f,\seq{x_i}) \leq \inf_{\seq{x_i}} U(f,\seq{x_i})
		\]
		\vitem Riemann integrable if
		\[
			\sup_{\seq{x_i}} L(f,\seq{x_i}) = \inf_{\seq{x_i}} U(f,\seq{x_i})
		\]
	\eit
	\vitem every continuous function is Riemann integrable
\eit


\myfoilhead{Motivation - want measure better than Riemann integrable}%
\index{Lebesgue measure!motivation}

\bit
	\item consider indicator (or characteristic) function $\chi_\rationals:[0,1] \to [0,1]$
	\[
		\chi_\rationals(x) = \left\{\begin{array}{ll}
			1 &\mbox{if } x \in \rationals
			\\
			0 &\mbox{if } x \not\in \rationals
		\end{array}\right.
	\]

	\vitem \emph{not} Riemann integrable: $\sup_{\seq{x_i}} L(f,\seq{x_i}) = 0 \neq 1 = \inf_{\seq{x_i}} U(f,\seq{x_i})$

	\vitem however, irrational numbers infinitely more than rational numbers, hence
	\bit
		\vitem \emph{want to} have some integral $\int$ such that, \eg,
		\[
			\int_{[0,1]} \chi_\rationals(x) dx = 0
			\mbox{ and }
			\int_{[0,1]} (1-\chi_\rationals(x)) dx = 1
		\]
	\eit
\eit


\myfoilhead{Properties of desirable measure}%
\index{measure!properties of desirable}

\bit
	\item want some measure $\mu:\subsetset{M}\to\preals=\set{x\in\reals}{x\geq0}$
	\bit
		\vitem defined for every subset of $\reals$, \ie, $\subsetset{M} = \powerset(\reals)$
		\vitem equals to length for open interval
			\[
				\mu[b,a] = b-a
			\]
		\vitem countable additivity: for disjoint $\seq{E_i}_{i=1}^\infty$\[ \mu(\cup E_i) = \sum \mu(E_i)\]
		\vitem translation invariant \[\mu(E+x) = \mu(E) \mbox{ for } x\in\reals\]
	\eit

	\vitem \emph{no} such measure exists
	\vitem \emph{not} known whether measure with first three properties exists
	\vitem want to find translation invariant \emph{countably additive measure}
	\bit
		\item hence, \emph{give up on first property}
	\eit
\eit


\myfoilhead{Race won by Henri Lebesgue in 1902!}

\bit
	\item mathematicians in 19th century struggle to solve this problem

	\vitem race won by French mathematician, \eemph{Henri L\'eon Lebesgue in 1902!}%
		\index{Lebesgue, Henri L\'{e}on}

	\vitem Lebesgue integral covers much wider range of functions
	\bit
		\item indeed, $\chi_\rationals$ is Lebesgue integrable
		\[
				\int_{[0,1]} \chi_\rationals(x) dx = 0
				\mbox{ and }
				\int_{[0,1]} (1-\chi_\rationals(x)) dx = 1
		\]
	\eit
\eit


\myfoilhead{Outer measure}%
	\index{Lebesgue measure!outer measure}%
	\index{outer measure}

\bit
	\item for $E\subset\reals$, define outer measure $\mu^\ast:\powerset(\reals)\to\preals$
	\[
		\mu^\ast E = \inf_{\seq{I_i}} \left\{\left.\sum l(I_i) \right| E\subset \cup I_i\right\}
	\]
	where $I_i=(a_i,b_i)$ and $l(I_i) = b_i-a_i$

	\vitem outer measure of open interval is length
	\[
		\mu^\ast(a_i,b_i) = b_i-a_i
	\]

	\vitem countable subadditivity
	\[
		\mu^\ast\left(\cup E_i\right) \leq \sum \mu^\ast E_i
	\]

	\vitem corollaries
	\bit
		\item $\mu^\ast E = 0$ if $E$ is countable
		\item $[0,1]$ not countable
	\eit
\eit


\myfoilhead{Measurable sets}%
	\index{Lebesgue measure!measurable sets}%
	\index{measurable sets}%
	\pagelabel{page:measurable-sets}

\bit
	\item $E\subset\reals$ called measurable if for every $A\subset\reals$
	\[
		\mu^\ast A = \mu^\ast (E\cup A) + \mu^\ast (\compl{E}\cup {A})
	\]

	\vitem $\mu^\ast E =0$, then $E$ measurable

	\vitem every open interval $(a,b)$ with $a\geq -\infty$ and $b\leq \infty$ is measurable

	\vitem disjoint countable union of measurable sets is measurable, \ie, $\cup E_i$ is measurable

	\vitem collection of measurable sets is $\sigma$-algebra%
		\index{Lebesgue measure!measurable sets!$\sigma$-algebra}%
		\idximportant{collection of measurable sets is $\sigma$-algebra}
\eit


\myfoilhead{Borel algebra is measurable}

\bit
	\item note
	\bit
		\item every open set is disjoint countable union of open intervals (page~\pageref{page:open-closed-fact})
		\item disjoint countable union of measurable sets is measurable (page~\pageref{page:measurable-sets})
		\item open intervals are measurable (page~\pageref{page:measurable-sets})
	\eit

	\vitem hence, every open set is measurable

	\vvitem also
	\bit
		\item collection of measurable sets is $\sigma$-algebra (page~\pageref{page:measurable-sets})
		\item every open set is Borel set and Borel sets are $\sigma$-algebra (page~\pageref{page:borel-algebra})
	\eit

	\vitem hence, \fact{Borel sets are measurable}%
		\index{Lebesgue measure!measurable sets!Borel sets}

	\vvitem specifically, \fact{Borel algebra (smallest $\sigma$-algebra containing all open sets) is measurable}
		\index{Lebesgue measure!measurable sets!Borel algebra}
\eit


\myfoilhead{Lebesgue measure}

\bit
	\item restriction of $\mu^\ast$ in collection $\subsetset{M}$ of measurable sets called \define{Lebesgue measure}%
		\index{Lebesgue measure}%
		\index{Lebesgue, Henri L\'{e}on!measure}
	\[
		\mu:\subsetset{M}\to\preals
	\]

	\vitem countable subadditivity - for $\seq{E_n}$%
		\index{Lebesgue measure!countable subadditivity}
	\[
		\mu (\cup E_n) \leq \sum \mu E_n
	\]

	\vitem \define{countable additivity} - for disjoint $\seq{E_n}$
		\index{Lebesgue measure!countable additivity}
	\[
		\mu (\cup E_n) = \sum \mu E_n
	\]

	\vitem for dcreasing sequence of measurable sets, \seq{E_n},
		\ie, $(\forall n\in\naturals)(E_{n+1} \subset E_n)$
	\[
		\mu\left(
			\bigcap E_n
		\right)
		=
		\lim
		\mu E_n
	\]
\eit


\myfoilhead{(Lebesgue) measurable sets are nice ones!}%
\idximportant{(Lebesgue) measurable sets are nice ones}%
\index{Lebesgue measure!measurable sets!nice ones}

\bit
	\item following statements are equivalent
	\begin{eqnarray*}
		&-&
		E \mbox{ is measurable}
		\\
		&-&
			(\forall \epsilon >0)
			(\exists \mbox{ open } O\supset E)
			(\mu^\ast(O\sim E)<\epsilon)
		\\
		&-&
			(\forall \epsilon >0)
			(\exists \mbox{ closed } F\subset E)
			(\mu^\ast(E\sim F)<\epsilon)
		\\
		&-&
			(\exists G_\delta)
			(G_\delta \supset E)
			(\mu^\ast(G_\delta\sim E)<\epsilon)
		\\
		&-&
			(\exists F_\sigma)
			(F_\sigma \subset E)
			(\mu^\ast(E\sim F_\sigma)<\epsilon)
	\end{eqnarray*}

	\vitem if $\mu^\ast E$ is finite, above statements are equivalent to
	\[
		(\forall \epsilon>0)
		\left(\exists U = \bigcup_{i=1}^n (a_i,b_i) \right)
		(\mu^\ast (U\Delta E) < \epsilon)
	\]
\eit


\myfoilhead{Lebesgue measure resolves problem in movitation}%
\index{Lebesgue measure!movitation!resolution}

\bit
	\item let
	\[
		E_1 = \set{x\in[0,1]}{x\in\rationals},\ %
		E_2 = \set{x\in[0,1]}{x\not\in\rationals}
	\]

	\vitem $\mu^\ast E_1=0$ because $E_1$ is countable, hence measurable and
	\[
		\mu E_1 = \mu^\ast E_1 = 0
	\]

	\vitem algebra implies $E_2 = [0, 1] \cap \compl{E_1}$ is measurable

	\vitem countable additivity implies $\mu E_1 + \mu E_2 = \mu[0,1] = 1$, hence
	\[
		\mu E_1 = 1
	\]
\eit


\titlefoil{Lebesgue Measurable Functions}{measurable-functions}

\myfoilhead{Lebesgue measurable functions}%
	\index{Lebesgue measurable functions}%
	\index{Lebesgue, Henri L\'{e}on!measurable functions}
	\index{measurable functions!Lebesgue}
\pagelabel{page:Lebesgue-measurable-functions}

\bit
	\item for $f:X\to\reals\cup\{-\infty, \infty\}$,
		\ie, extended real-valued function, the followings are equivalent
	\bit
		\item for every $a\in\reals$, $\set{x\in{X}}{f(x) < a}$ is measurable
		\item for every $a\in\reals$, $\set{x\in{X}}{f(x) \leq a}$ is measurable
		\item for every $a\in\reals$, $\set{x\in{X}}{f(x) > a}$ is measurable
		\item for every $a\in\reals$, $\set{x\in{X}}{f(x) \geq a}$ is measurable
	\eit

	\vitem if so,
	\bit
		\item for every $a\in\reals\cup\{-\infty, \infty\}$, $\set{x\in{X}}{f(x) = a}$ is measurable
	\eit

	\vitem extended real-valued function, $f$, called \define{(Lebesgue) measurable function} if%
		\index{Lebesgue measurable functions}%
	\bit
		\item domain is measurable
		\item any one of above four statements holds
	\eit
\refertocounterpart{abstract}{page:Measurable functions}
\eit


\myfoilhead{Properties of Lebesgue measurable functions}%
	\index{Lebesgue measurable functions!properties}
\pagelabel{page:measurable:function:facts}

\bit
	\item for real-valued measurable functions, $f$ and $g$, and $c\in\reals$
	\bit
		\item $f+c$, $cf$, $f+g$, $fg$ are measurable
	\eit

	\vitem for every extended real-valued measurable function sequence, $\seq{f_n}$
	\bit
		\item $\sup f_n$, $\limsup f_n$ are measurable
		\item hence, $\inf f_n$, $\liminf f_n$ are measurable
		\item thus, if $\lim f_n$ exists, it is measurable
	\eit
	\refertocounterpart{abstract}{page:Properties of measurable functions}
\eit


\myfoilhead{Almost everywhere - a.e.}
\pagelabel{page:almost:everywhere}

\bit
	\item statement, $P(x)$, called \define{almost everywhere} or \define{a.e.} if%
		\index{almost everywhere}%
		\index{a.e.!almost everywhere}
	\[
		\mu \set{x}{\sim P(x)} = 0
	\]
	\bit
		\vitem \eg, $f$ said to be equal to $g$ a.e. if $\mu\set{x}{f(x)\neq g(x)}=0$
		\vitem \eg, $\seq{f_n}$ said to converge to $f$ a.e. if
		\[
			(\exists E \mbox{ with } \mu E=0)(\forall x \not\in E)(\lim f_n (x) = f(x))
		\]
	\eit

	\vvitem facts
	\bit
		\vitem if $f$ is measurable and $f=g$ i.e., then $g$ is measurable
		\vitem if measurable extended real-valued $f$ defined on $[a,b]$ with $f(x) \in\reals$ a.e.,
			then for every $\epsilon>0$, exist step function $g$ and continuous function $h$ such that
			\[
				\mu\set{x}{|f-g| \geq \epsilon} < \epsilon,\ %
				\mu\set{x}{|f-h| \geq \epsilon} < \epsilon
			\]
	\eit
\eit


\myfoilhead{Characteristic \& simple functions}
\pagelabel{page:Characteristic and simple functions}

\bit
	\item for any $A\subset\reals$, $\chi_A$ called \define{characteristic function} if%
		\index{Lebesgue measurable functions!characteristic functions}
	\[
		\chi_A(x) = \left\{\begin{array}{ll}
			1 & x\in A\\
			0 & x\not\in A\\
		\end{array}\right.
	\]
	\bit
		\item $\chi_A$ is measurable \iaoi\ $A$ is measurable
	\eit

	\vitem measurable $\varphi$ called \define{simple} if for some distinct $\seq{a_i}_{i=1}^n$%
		\index{Lebesgue measurable functions!simple}
	\[
		\varphi(x) = \sum_{i=1}^n a_i \chi_{A_i}(x)
	\]
	where $A_i = \set{x}{x= a_i}$

	\refertocounterpart{abstract}{page:Simple functions}
\eit


\myfoilhead{Littlewood's three principles}%
\index{Littlewood's three principles}%
\index{Littlewood, John Edensor!Littlewood's three principles}
\phantomsection\label{page:littlewood:three:principles}

\bit
	\item [] let $M(E)$ with measurable set, $E$, denote set of measurable functions defined on $E$% - I just invented it!

	\vitem \fact{every (measurable) set is nearly finite union of intervals}, \eg,
	\bit
		\item $E$ is measurable \iaoi%
		\[
			(\forall \epsilon>0)
			(\exists \{I_i: \mbox{open\ interval}\}_{i=1}^n)
			(\mu^\ast(E \Delta (\cup I_n)) < \epsilon)
		\]
	\eit

	\vitem \fact{every (measurable) function is nearly continuous}, \eg,
	\bit
		\item (Lusin's theorem)
		\[
			(\forall f \in M[a,b])(\forall \epsilon >0)(\exists g \in C[a,b]) (\mu\set{x}{f(x)\neq g(x)}< \epsilon)
		\]
	\eit

	\vitem \fact{every convergent (measurable) function sequence is nearly uniformly convergent}, \eg,
			\begin{eqnarray*}
			\lefteqn{
					(\forall \mbox{ measurable }\seq{f_n} \mbox{ converging to } f \mbox { a.e. on } E \mbox{ with } \mu E<\infty)
				}
				\\
				&&
					(\forall \epsilon>0 \mbox{ and } \delta>0)
					(\exists A\subset E \mbox{ with } \mu(A)<\delta \mbox{ and } N\in\naturals)
				\\
				&&
					(\forall n > N, x\in E\sim A)(|f_n(x)-f(x)|<\epsilon)
			\end{eqnarray*}
\eit


\myfoilhead{Egoroff's theorem}%
\index{Egoroff, Dmitri Fyodorovich!Egoroff's theorem}%
\index{Egoroff's theorem}%
\index{Lebesgue measurable functions!Egoroff's theorem}%

\bit
	\item \name{Egoroff theorem -} provides stronger version of third statement
		on page~\pageref{page:littlewood:three:principles}
			\begin{eqnarray*}
			\lefteqn{
				(\forall \mbox{ measurable }\seq{f_n} \mbox{ converging to } f \mbox { a.e. on } E \mbox{ with } \mu E<\infty)
				}
				\\
				&&
					(\exists A\subset E \mbox{ with } \mu(A)<\epsilon)
					(f_n \mbox{ uniformly converges to } f \mbox{ on } E\sim A )
			\end{eqnarray*}
\eit


\titlefoil{Lebesgue Integral}{lebesgue-integral}%
	\index{Lebesgue integral}

\myfoilhead{Integral of simple functions}
\pagelabel{page:Integral of simple functions}

\bit
	\item \define{canonical representation} of simple function%
		\index{Lebesgue functions!canonical representation}
	\[
		\varphi(x) = \sum_{i=1}^n a_i \chi_{A_i}(x)
	\]
	where $a_i$ are \emph{distinct} $A_i=\{x|\varphi(x)=a_i\}$ - note $A_i$ are \emph{disjoint}

	\vitem when $\mu\set{x}{\varphi(x)\neq0}< \infty$ and $\varphi = \sum_{i=1}^n a_i \chi_{A_i}$ is canonical representation,
		define \define{integral of $\varphi$} by%
			\index{Lebesgue integral!simple functions}
	\[
		\int \varphi = \int \varphi (x) dx= \sum_{i=1}^n a_i \mu A_i
	\]

	\vitem when $E$ is measurable, {define}
	\[
		\int_E \varphi = \int \varphi \chi_E
	\]

	\refertocounterpart{abstract}{page:Integration}
\eit


\myfoilhead{Properties of integral of simple functions}

\bit
	\item for simple functions $\varphi$ and $\psi$
		that vanish out of finite measure set,
		\ie,
		$\mu\set{x}{\varphi(x)\neq0}<\infty$, $\mu\set{x}{\psi(x)\neq0}<\infty$,
		%with $\mu\set{x}{\varphi(x)\neq0}, \mu\set{x}{\psi(x)\neq0}<\infty$
		and for every $a,b\in\reals$\
		\pagelabel{page:linearity of Lebesgue integral of simple functions}
	\[
		\int (a\varphi + b\psi) = a \int\varphi + b \int\psi
	\]

	\refertocounterpart{abstract}{page:Integration}

	\vitem thus, even for simple function, $\varphi = \sum_{i=1}^n a_i \chi_{A_i}$
		that vanishes out of finite measure set,
		%with $\mu\set{x}{\varphi(x)\neq0}< \infty$,
		not necessarily in canonical representation,
	\[
		\int \varphi = \sum_{i=1}^n a_i \mu A_i
	\]

	\vitem if $\varphi \geq \psi$ a.e.
	\[
		\int \varphi \geq \int \psi
	\]
\eit


\myfoilhead{Lebesgue integral of bounded functions}
\pagelabel{page:Lebesgue integral of bounded functions}

\bit
	\item for bounded function, $f$, and finite measurable set, $E$,
	\[
		\sup_{\varphi:\ \mathrm{simple},\ \varphi \leq f} \int_E \varphi
		\leq
		\inf_{\psi:\ \mathrm{simple},\ f \leq \psi} \int_E \psi
	\]

	\bit
		\item if $f$ is defined on $E$, $f$ is measurable function \iaoi\
		\[
			\sup_{\varphi:\ \mathrm{simple},\ \varphi \leq f} \int_E \varphi
			=
			\inf_{\psi:\ \mathrm{simple},\ f \leq \psi} \int_E \psi
		\]
	\eit

	\vitem
		for bounded measurable function, $f$, defined on measurable set, $E$, with $\mu E < \infty$,
		define \define{(Lebesgue) integral of $f$ over $E$}%
			\index{Lebesgue integral!bounded functions}
		\pagelabel{page:Lebesgue integral of simple functions}
		\[
			\int_E f(x) dx =
			\sup_{\varphi:\ \mathrm{simple},\ \varphi \leq f} \int_E \varphi
			=
			\inf_{\psi:\ \mathrm{simple},\ f \leq \psi} \int_E \psi
		\]

	\refertocounterpart{abstract}{page:integral of simple functions}
\eit


\myfoilhead{Properties of Lebesgue integral of bounded functions}%
	\index{Lebesgue integral!bounded functions!properties}

\bit
	\item for bounded measurable functions, $f$ and $g$, defined on $E$ with finite measure
		\pagelabel{page:linearity of nonnegative integral - Lebesgue}
	\bit
		\item for every $a,b\in\reals$

		\begin{eqn}
%		\[
			\int_E (af+bg) = a \int_E f + b\int_E g
%		\]
		\end{eqn}

		\item if $f\leq g$ a.e.

		\begin{eqn}
%		\[
			\int_E f \leq \int_E g
%		\]
		\end{eqn}

		\item for disjoint measurable sets, $A,B\subset E$,

		\begin{eqn}
%		\[
			\int_{A\cup B} f = \int_A f + \int_B f
%		\]
		\end{eqn}
	\eit
	\refertocounterpart{abstract}{page:linearity of nonnegative integral - abstract}

	\item hence,
		\[
			\left|\int_E f \right| \leq \int_E |f|
			\mbox{  \&  }
			f=g \mbox{ a.e. } \Rightarrow \int_E f = \int_E g
		\]
\eit


\myfoilhead{Lebesgue integral of bounded functions over finite interval}

\bit
	\item if bounded function, $f$, defined on $[a,b]$ is Riemann integrable,
		then $f$ is measurable and
		\[
			\int_{[a,b]} f
			=
			R \int_a^b f(x) dx
		\]
		where $R\int$ denotes Riemann integral

	\vitem bounded function, $f$, defined on $[a,b]$ is Riemann integrable
		\iaoi\ set of points where $f$ is discontinuous has measure zero

	\vitem for sequence of measurable functions, $\seq{f_n}$, defined on measurable $E$ with finite measure, and $M>0$,
		if $|f_n|<M$ for every $n$ and $f(x) = \lim f_n(x)$ for every $x\in E$
		\[
			\int_E f = \lim \int_E f_n
		\]
\eit


\myfoilhead{Lebesgue integral of nonnegative functions}
\pagelabel{page:Lebesgue integral of nonnegative functions}

\bit
	\item for nonnegative measurable function, $f$, defined on measurable set, $E$, define%
			\index{Lebesgue integral!nonnegative functions}
		\pagelabel{page:Lebesgue integral of nonnegative measurable function}
	\[
		\int_E f = \sup_{h:\ \mathrm{bounded\ measurable\ function},\ \mu\set{x}{h(x)\neq0}<\infty,\ h\leq f} \int_E h
	\]

	\refertocounterpart{abstract}{page:integral of nonnegative extended real-valued measurable function}

	\vitem for nonnegative measurable functions, $f$ and $g$
	\bit
		\item for every $a,b\geq0$
		\[
			\int_E (af + bg) = a\int_E f + b\int_E g
		\]
		\item if $f\geq g$ a.e.
		\[
			\int_E f \leq \int_E g
		\]
	\eit

	\vitem thus,
	\bit
		\item for every $c>0$
		\[
			\int_E cf = a\int_E f
		\]
	\eit
\eit


\myfoilhead{Fatou's lemma and monotone convergence theorem for Lebesgue integral}
\pagelabel{page:integral:nonneg:facts}

\bit
	\item \name{Fatou's lemma -}
	for nonnegative measurable function sequence, $\seq{f_n}$,
	with $\lim f_n = f$ a.e. on measurable set, $E$%
		\index{Fatou, Pierre Joseph Louis!Fatou's lemma}%
		\index{Lebesgue integral!Fatou's lemma}%
		\index{Fatou's lemma!Lebesgue integral}
	\[
		\int_E f \leq \liminf \int_E f_n
	\]
	\bit
		\item note $\lim f_n$ is measurable (page~\pageref{page:measurable:function:facts}),
		hence $f$ is measurable (page~\pageref{page:almost:everywhere})
	\eit

	\vitem \name{monotone convergence theorem -}
	for nonnegative increasing measurable function sequence, $\seq{f_n}$,
	with $\lim f_n = f$ a.e. on measurable set, $E$%
		\index{Lebesgue integral!monotone convergence theorem}%
		\index{monotone convergence theorem!Lebesgue integral}
		\pagelabel{page:Lebesgue - monotone convergence theorem}
	\[
		\int_E f = \lim \int_E f_n
	\]

	\refertocounterpart{abstract}{page:abstract - monotone convergence theorem}

	\vitem for nonnegative measure function, $f$, and sequence of disjoint measurable sets, $\seq{E_i}$,
	\[
		\int_{\cup E_i} f = \sum \int_{E_i} f
	\]
\eit


\myfoilhead{Lebesgue integrability of nonnegative functions}
\pagelabel{integrable:nonnegative:function}

\bit
	\item nonnegative measurable function, $f$, said to be \define{integrable} over measurable set, $E$, if%
		\index{Lebesgue integral!nonnegative functions!integrable}%
		\index{Lebesgue integral!integrable!nonnegative functions}%
		\index{integrable!Lebesgue!nonnegative functions}
		\pagelabel{page:Lebesgue integrability of nonnegative functions}
	\[
		\int_E f < \infty
	\]

	\refertocounterpart{abstract}{page:integrability of nonnegative functions}

	\vitem for nonnegative measurable functions, $f$ and $g$, if $f$ is integrable on measurable set, $E$,
		and $g\leq f$ a.e. on $E$, then $g$ is integrable and
	\[
		\int_E (f-g) = \int_E f - \int_E g
	\]

	\vitem for nonnegative integrable function, $f$, defined on measurable set, $E$, and every $\epsilon$,
		exists $\delta >0$ such that for every measurable set $A\subset E$ with $\mu A< \epsilon$
		(then $f$ is integrable on $A$, of course),
		\[
			\int_A f < \epsilon
		\]
\eit


\myfoilhead{Lebesgue integral}

\bit
	\item for (any) function, $f$, define $f^+$ and $f^-$ such that for every $x$
%
%	\begin{eqna}
	\begin{eqnarray*}
		f^+(x) &=& \max\{f(x), 0\}
		\\
		f^-(x) &=& \max\{-f(x), 0\}
	\end{eqnarray*}
%	\end{eqna}

	\vitem note
		$f = f^+ - f^-,\ |f| = f^+ + f^-,\ f^- = (-f)^+$

	\vitem measurable function, $f$, said to be \define{(Lebesgue) integrable} over measurable set, $E$,
		if (nonnegative measurable) functions, $f^+$ and $f^-$, are integrable%
			\index{Lebesgue integral}%
			\index{Lebesgue, Henri L\'{e}on!integral}%
			\index{Lebesgue integral!integrable}%
			\index{integrable!Lebesgue}
		\pagelabel{page:Lebesgue integral}
%
%	\begin{eqn}
	\[
		\int_E f = \int_E f^+ - \int_E f^-
	\]
%	\end{eqn}

	\refertocounterpart{Lebesgue}{page:integral}
\eit


\myfoilhead{Properties of Lebesgue integral}%
	\index{Lebesgue integral!properties}

\bit
	\item for $f$ and $g$ integrable on measure set, $E$, and $a,b\in\reals$
		\pagelabel{page:properties of Lebesgue integral}
	\bit
		\vitem $af+bg$ is integral and
		\[
			\int_E (af+bg) = a \int_E f + b\int_E g
		\]

		\vitem if $f\geq g$ a.e. on $E$,
		\[
			\int_E f \geq \int_E g
		\]

		\vitem for disjoint measurable sets, $A,B\subset E$
		\[
			\int_{A\cup B} f = \int_A f + \int_B g
		\]
	\eit

	\refertocounterpart{abstract}{page:properties of integral}

\eit


\myfoilhead{Lebesgue convergence theorem (for Lebesgue integral)}

\bit
	\item \name{Lebesgue convergence theorem -}
		for measurable $g$ integrable on measurable set, $E$,
		and measurable sequence $\seq{f_n}$ converging to $f$ with $|f_n|<g$ a.e. on $E$,
		($f$ is measurable (page~\pageref{page:measurable:function:facts}),
		every $f_n$ is integrable (page~\pageref{integrable:nonnegative:function}))
		and%
			\index{Lebesgue convergence theorem!Lebesgue integral}%
			\index{Lebesgue integral!Lebesgue convergence theorem}%
			\index{Lebesgue, Henri L\'{e}on!convergence theorem}
		\pagelabel{page:lebesgue-integral-facts-2}
		\[
			\int_E f = \lim \int_E f_n
		\]

		\pagelabel{page:Lebesgue convergence theorem for Lebesgue integral}

	\refertocounterpart{abstract}{page:Lebesgue convergence theorem}
\eit


\myfoilhead{Generalization of Lebesgue convergence theorem (for Lebesgue integral)}

\bit
	\item \name{generalization of Lebesgue convergence theorem -}
		for sequence of functions, $\seq{g_n}$, integrable on measurable set, $E$,
		converging to integrable $g$ a.e. on $E$,
		and sequence of measurable functions, $\seq{f_n}$,
		converging to $f$ a.e. on $E$
		with $|f_n|<g_n$ a.e. on $E$,
		if
		\[
			\int_E g = \lim \int_E g_n
		\]
		then
		($f$ is measurable (page~\pageref{page:measurable:function:facts}),
		every $f_n$ is integrable (page~\pageref{integrable:nonnegative:function}))
		and
		\[
			\int_E f = \lim \int_E f_n
		\]
\eit


\myfoilhead{Comments on convergence theorems}%
	\index{Lebesgue convergence theorem!Lebesgue integral!comments}

\bit
	\item Fatou's lemma (page~\pageref{page:integral:nonneg:facts}),
		monotone convergence theorem (page~\pageref{page:integral:nonneg:facts}),
		Lebesgue convergence theorem (page~\pageref{page:lebesgue-integral-facts-2}),
		\emph{all}
		state that under suitable conditions, we say something about
		\[
			\int \lim f_n
		\]
		in terms of
		\[
			\lim \int f_n
		\]

	\vitem Fatou's lemma requires weaker condition than Lebesgue convergence theorem, \ie,
		only requires ``bounded below'' whereas Lebesgue converges theorem also requires ``bounded above''
		\[
			\int \lim f_n \leq \liminf \int f_n
		\]

	\vitem monotone convergence theorem is somewhat between the two;
	\bit
		\item advantage - applicable even when $f$ not integrable
		\item Fatou's lemma and monotone converges theorem very clsoe in sense that
			can be derived from each other using only facts of positivity and linearity of integral
	\eit
\eit


\myfoilhead{Convergence in measure}%
	\index{convergence!in measure}

\bit
	\item $\seq{f_n}$ of measurable functions said to \define{converge $f$ in measure} if
	\[
		(\forall \epsilon>0)
		(\exists N\in\naturals)
		(\forall n > N)
		(\mu\set{x}{|f_n-f|>\epsilon} < \epsilon)
	\]

	\vitem thus, third statement on page~\pageref{page:littlewood:three:principles} implies
	\[
			(\forall \seq{f_n} \mbox{ converging to } f \mbox { a.e. on } E \mbox{ with } \mu E<\infty)
			(f_n \mbox{ converge in measure to }f)
			\]
	\vitem however, the converse is \emph{not} {true}, \ie,
		exists $\seq{f_n}$ converging in measure to $f$ that does not converge to $f$ a.e.
	\bit
		\item \eg, XXX\idxtodo{5 - counter-example for convergence in measure}
	\eit

	\vvitem Fatou's lemma (page~\pageref{page:integral:nonneg:facts}),
		monotone convergence theorem (page~\pageref{page:integral:nonneg:facts}),
		Lebesgue convergence theorem (page~\pageref{page:lebesgue-integral-facts-2})
		\eemph{remain valid!}
		even when ``convergence a.e.'' replaced by ``convergence in measure''
\eit


\myfoilhead{Conditions for convergence in measure}


\begin{myproposition}{necessary condition for converging in measure}
	$$
		\left(
			\forall \seq{f_n} \mbox{ converging in measure to } f
		\right)
		\left(
			\exists \mbox{ subsequence }\seq{f_{n_k}} \mbox{ converging a.e. to } f
		\right)
	$$
\end{myproposition}


\begin{mycorollary}{necessary and sufficient condition for converging in measure}
		for sequence $\seq{f_n}$ measurable on $E$ with $\mu E<\infty$
		\begin{eqnarray*}
			\lefteqn{\seq{f_n} \mbox{ converging in measure to } f}
			\\
			&\Leftrightarrow&
			\left(
				\forall \mbox{ subsequence }\seq{f_{n_k}}
			\right)
			\left(
				\exists \mbox{ its subsequence }\seq{f_{n_{k_l}}} \mbox{ converging a.e. to } f
			\right)
		\end{eqnarray*}
\end{mycorollary}
}{}


\yesnoexec{\topspaces}{
\titlefoil{Space Overview}{Space Overview}

\myfoilhead{Diagrams for relations among various spaces}%
	\pagelabel{page:diagrams-for-relations-among-spaces}

\bit
\item
	note from \figref{diagrams for relations among various spaces}\
	\bit
	\item
		metric should be defined to utter completeness
	\item
		metric spaces can be induced from normed spaces
%	\item weak topology makes sense only in normed vector spaces
	\eit
\eit

\vfill
\begin{figure}
\begin{center}
	\mypsfrag{metric spaces}
		{metric spaces}

	\mypsfrag{normed spaces}
		{normed spaces}

	\mypsfrag{complete spaces}
		{complete spaces}

	\mypsfrag{weak topology}
		{weak topology}

	\mypsfrag{topological spaces}
		{topological spaces}

	\mypsfrag{vector spaces}
		{vector spaces}

	\includegraphics[width=.6\textwidth]{figures/diagrams-all-spaces}%
		\idxfig{diagrams for relations among various spaces}%
		\label{fig:diagrams for relations among various spaces}
\end{center}
\end{figure}
\vfill


\titlefoil{Classical Banach Spaces}{classical-banach-spaces}

\myfoilhead{Normed linear space}

\bit
	\item $X$ called \define{linear space} if
	\[
		(\forall x, y \in X, a, b \in \reals)(ax + by \in X)
	\]

	\vitem linear space, $X$, called \define{normed space} with associated norm $\|\cdot\|: X \to \preals$ if%
		\index{normed spaces}%
		\index{normed spaces!linear}
	\bit
		\item
		\[
			(\forall x\in X)(\|x\|=0 \Rightarrow x \equiv 0)
		\]
		\item
		\[
			(\forall x \in X, a \in \reals)(\|ax\| = |a|\|x\|)
		\]
		\item subadditivity
		\[
			(\forall x,y\in X)(\|x+y\| \leq \|x\| + \|y\|)
		\]
	\eit
\eit


\myfoilhead{$L^p$ spaces}


\bit
\item
	$L^p = L^p[0,1]$ denotes space of (Lebesgue) measurable functions such that%
		\index{$L^p$ spaces!linear normed spaces}%
	\[
		\int_{[0,1]} |f|^p < \infty
	\]

\vitem
	define $\|\cdot\|:L^p\to\preals$
	\[
		\|f\| = \|f\|_p = \left(\int_{[0,1]} |f|^p\right)^{1/p}
	\]

\vitem
	$L^p$ are \emph{linear normed spaces} with norm $\|\cdot\|_p$ when $p\geq 1$ because
	\bit
	\item
		$|f(x)|^p + |g(x)|^p \leq 2^p(|f(x)|^p + |g(x)|^p)$ implies $(\forall f, g\in L^p)(f+g \in L^p)$
	\item
		$|\alpha f(x)|^p = |a|^p|f(x)|^p$ implies $(\forall f\in L^p, a \in \reals)(af \in L^p)$
	\item
		$\|f\|=0\Rightarrow f=0\mbox{ a.e.}$
	\item
		$\|a f\| = |a|\|f\|$
	\item
		$\|f+g\|\geq \|f\|+\|g\|$ (Minkowski inequality)
	\eit
\eit


\myfoilhead{$L^\infty$ space}

\bit
\item
	$L^\infty = L^\infty[0,1]$ denotes space of measurable functions bounded a.e.%
		\index{$L^\infty$ space!linear normed spaces}

\vitem
	$L^\infty$ is linear normed space with norm
	\[
		\|f\| = \|f\|_\infty = \mathrm{ess\ sup} |f|
		= \inf_{g: g=f \ \mathrm{a.e}} \sup_{x\in[0,1]} |g(x)|
	\]
	\bit
	\item
		thus
		\[
			\|f\|_\infty = \inf\set{M}{\mu\set{x}{f(x)>M}=0}
		\]
	\eit
\eit


\myfoilhead{Inequalities in $L^\infty$}

\bit
	\item \name{Minkowski inequality -} for $p\in[1,\infty]$%
		\index{Minkowski inequality!linear normed spaces}%
		\index{Minkowski, Hermann!Minkowski inequality!linear normed spaces}
	\[
		(\forall f,g\in L^p)(\|f+g\|_p \leq \|f\|_p + \|g\|_p)
	\]
	\bit
		\item if $p\in(1,\infty)$, equality holds \iaoi\ %
			$(\exists a,b\geq 0 \mbox{ with } ab\neq0)(af = bg \mbox{ a.e.})$
	\eit

	\vitem Minkowski inequality for $0<p<1$:%
		\index{Minkowski inequality!linear normed spaces!for $0<p<1$}%
		\index{Minkowski, Hermann!Minkowski inequality!linear normed spaces!for $0<p<1$}
	\[
		(\forall f,g\in L^p)(f,g\geq0 \mbox{ a.e.} \Rightarrow \|f+g\|_p \geq \|f\|_p + \|g\|_p)
	\]

	\vitem \name{H\"{o}lder's inequality -} for $p,q\in[1,\infty]$ with $1/p+1/q=1$%
		\index{H\"{o}lder's inequality!linear normed spaces}%
		\index{H\"{o}lder, Ludwig Otto!H\"{o}lder's inequality!linear normed spaces}
		\pagelabel{Holder inequality!linear normed spaces}
	\[
		(\forall f\in L^p, g\in L^q)
		\left(fg \in L^1 \mbox{ and } \int_{[0,1]} |fg| \leq \int_{[0,1]} |f|^p \int_{[0,1]} |g|^q\right)
	\]
	\bit
		\item equality holds \iaoi\ %
			$(\exists a,b\geq 0 \mbox{ with } ab\neq0)(a|f|^p = b|g|^q \mbox{ a.e.})$
	\eit

	\refertocounterpart{complete measure spaces}{page:Holder inequality-complete measure spaces}
\eit


\myfoilhead{Convergence and completeness in normed linear spaces}

\bit
	\vvitem $\seq{f_n}$ in normed linear space
	\bit
	\item said to \define{converge} to $f$, \ie, $\lim f_n =f$ or $f_n \to f$, if
	\[
		(\forall \epsilon>0)(\exists N\in\naturals)(\forall n> N)(\|f_n-f\|<\epsilon)
	\]

	\item called \define{Cauchy sequence} if
	\[
		(\forall \epsilon>0)(\exists N\in\naturals)(\forall n,m> N)(\|f_n-f_m\|<\epsilon)
	\]
	\item called \define{summable} if $\sum^n_{i=1} f_i$ converges
	\item called \define{absolutely summable} if $\sum^n_{i=1} |f_i|$ converges
	\eit

	\vvitem normed linear space called \define{complete} if every Cauchy sequence converges%
		\index{complete!normed spaces}

	\vitem normed linear space is \emph{complete} \iaoi\ every absolutely summable series is summable
\eit


\myfoilhead{Banach space}
\pagelabel{page:Banach-space}

\bit
	\item \emph{complete normed linear space} called \define{Banach space}%
		\index{Banach spaces}%
		\index{Banach, Stefan!Banach spaces}%
		\index{complete!Banach spaces}%
		\index{Banach, Stefan!Banach spaces!complete}

	\vitem (Riesz-Fischer) $L^p$ spaces are compact, hence Banach spaces
	\vitem convergence in $L^p$ called \define{convergence in mean of order $p$}
	\vitem convergence in $L^\infty$ implies nearly uniformly converges
\eit


\myfoilhead{Approximation in $L^p$}

\bit
	\item $\Delta=\seq{d_i}_{i=0}^n$ with $0=d_1<d_2<\cdots<d_n=1$ called \define{subdivision} of $[0,1]$
		(with $\Delta_i = [d_{i-1},d_{i}]$)

	\vitem $\varphi_{f,\Delta}$ for $f\in L^p$ called \define{step function} if
	\[
		\varphi_{f,\Delta}(x) = \frac{1}{d_i-d_{i+1}}\int_{d_{i-1}}^{d_i} f(t)dt \mbox{ for } x\in[d_{i-1},d_i)
	\]

	\vitem for $f\in L^p$ ($1<p\leq \infty$), exist $\varphi_{f,\Delta}$ and continuous function, $\psi$ such that
	\[
		\|\varphi_{f,\Delta_i}-f\|<\epsilon
		\mbox{ and }
		\|\psi-f\|<\epsilon
	\]
	\bit
		\item $L^p$ version of Littlewood's second principle (page~\pageref{page:littlewood:three:principles})%
			\pagelabel{page:normed space version of Littlewood's second principle}%
			\index{Littlewood's three principles!second principle!linear normed spaces}

		\refertocounterpart{complete measure spaces}{page:complete measure space version of Littlewood's second principle}
	\eit

	\vitem for $f\in L^p$, $\varphi_{f,\Delta}\to f$ as $\max \Delta_i\to0$, \ie,
	\[
		(\forall \epsilon>0)(\exists \delta>0)(\max \Delta_i < \delta \Rightarrow \|\varphi_{f,\Delta}-f\|_p < \epsilon)
	\]
\eit


\myfoilhead{Bounded linear functionals on $L^p$}

\bit
	\item $F:X\in\reals$ for normed linear space $X$ called \define{linear functional} if

		\begin{eqn}
			(\forall f, g \in F, a,b \in\reals)(F(af+bg)=aF(f)+bF(g))
		\end{eqn}

	\vitem linear functional, $F$, said to be \define{bounded} if

		\begin{eqn}
			(\exists M)(\forall f\in X)(|F(f)|\leq M\|f\|)
		\end{eqn}

	\vitem smallest such constant called \define{norm of $F$}, \ie,

		\begin{eqn}
			\|F\| = \sup_{f\in X, f\neq0} {|F(f)|}/{\|f\|}
		\end{eqn}
\eit


\myfoilhead{Riesz representation theorem}

\bit
	\item for every $g\in L^q$ ($1\leq p\leq \infty$), following defines a bounded linear functional in $L^p$

	\begin{eqn}
		F(f) = \int fg
	\end{eqn}

	where $\|F\|=\|g\|_q$

	\vitem \name{Riesz representation theorem -}
		for every bounded linear functional in $L^p$, $F$, ($1\leq p<\infty$),
		there exists $g\in L^q$ such that%
			\index{Riesz representation theorem!linear normed spaces}%
			\index{Riesz representation theorem}%
			\index{Riesz, Frigyes!Riesz representation theorem!linear normed spaces}%
			\index{Riesz, Frigyes!Riesz representation theorem}
			\idximportant{Riesz representation theorem}%
			\pagelabel{page:Riesz-representation-theorem}
	\begin{eqn}
		F(f) = \int fg
	\end{eqn}
	where $\|F\|=\|g\|_q$

	\refertocounterpart{complete measure spaces}{page:Riesz representation theorem!complete measure spaces}


	\vvitem for each case, $L^q$ is dual of $L^p$
		(refer to page \pageref{page:Dual-of-normed-spaces} for definition of dual)
\eit


\titlefoil{Metric Spaces}{metric-spaces}

\myfoilhead{Metric spaces}

\bit
	\item $\metrics{X}{\rho}$ with nonempty set, $X$, and \define{metric} $\rho: X\times X\to\preals$ called \define{metric space}
		if for every $x,y,z \in X$
	\bit
		\item $\rho(x,y)=0 \Leftrightarrow x=y$
		\item $\rho(x,y)=\rho(y,x)$
		\item $\rho(x,y) \leq \rho(x,z) + \rho(z,y)$ (triangle inequality)
	\eit

	\vitem examples of metric spaces
	\bit
		\item $\metrics{\reals}{|\cdot|}$, $\metrics{\reals^n}{\|\cdot\|_p}$ with $1\leq p\leq \infty$
	\eit

	\vvitem for $f\subset X$, $S_{x,r} = \set{y}{\rho(y,x)<r}$ called \define{ball}
	\vitem for $E\subset X$, $\sup\set{\rho(x,y)}{x,y \in E}$ called diameter of $E$ defined by

	\vvitem $\rho$ called \define{pseudometric} if 1st requirement removed
	\vitem $\rho$ called \define{extended metric} if $\rho: X\times X \to\preals\cup\{\infty\}$
\eit


\myfoilhead{Cartesian product}

\bit
	\item for two metric spaces $\metrics{X}{\rho}$ and $\metrics{Y}{\sigma}$,
		metric space $\metrics{X\times Y}{\tau}$ with $\tau:X\times Y\to\preals$ such that
		\[
			\tau((x_1,y_1),(x_2,y_2)) = (\rho(x_1,x_2)^2 + \sigma(y_1,y_2)^2)^{1/2}
		\]
		called \define{Cartesian product metric space}

	\vitem $\tau$ satisfies all properties required by metric
	\bit
		\item \eg, $\reals^{n} \times \reals^{m} = \reals^{n+m}$
	\eit
\eit


\myfoilhead{Open sets - metric spaces}

\bit
	\item $O \subset X$ said to be open \define{open} if
	\[
		(\forall x\in O)(\exists \delta>0)(\forall y\in X)(\rho(y,x)<\delta \Rightarrow y\in O)
	\]
	\bit
		\item $X$ and $\emptyset$ are open
		\item intersection of \emph{finite} collection of open sets is open
		\item union of \emph{any} collection of open sets is open
	\eit
\eit


\myfoilhead{Closed sets - metric spaces}

\bit
	\vitem $x\in X$ called \define{point of closure of $E\subset X$} if
	\[
		(\forall \epsilon>0)(\exists y\in E)(\rho(y,x) < \epsilon)
	\]
	\bit
		\item $\closure{E}$ denotes set of points of closure of $E$; called \define{closure} of $E$
		\item $E\subset \closure{E}$
	\eit
	\vitem $F \subset X$ said to be \define{closed} if
	\[
		F = \closure{F}
	\]
	\bit
		\item $X$ and $\emptyset$ are closed
		\item union of \emph{finite} collection of closed sets is closed
		\item intersection of \emph{any} collection of closed sets is closed
	\eit

	\vvitem complement of closed set is open
	\vitem complement of open set is closed
\eit


\myfoilhead{Dense sets and separability - metric spaces}
\pagelabel{page:dense-sets-and-separability-metric-spaces}

\bit
	\item $D\subset X$ said to be dense if

	\begin{eqn}
		\closure{D} = X
	\end{eqn}

	\vitem $X$ is said to be separable if exists finite dense subset, \ie,
	\index{metric spaces!separable}
	\index{separable!metric spaces}

	\begin{eqn}
		(\exists D\subset X)(|D| < \infty \ \& \ \closure{D}=X)
	\end{eqn}

	\vitem $X$ is separable \iaoi\ exists countable collection of open sets $\seq{O_i}$ such that
		for all open $O\subset X$

	\begin{eqn}
		O = \bigcup_{O_i\subset O} O_i
	\end{eqn}
\eit


\myfoilhead{Continuous functions - metric spaces}

\bit
	\item $f:X\to Y$ for metric spaces $\metrics{X}{\rho}$ and $\metrics{Y}{\sigma}$ called \define{mapping} or \define{function}
		from $X$ into $Y$

	\vitem $f$ said to be \define{onto} if \[f(X)=Y\]

	\vitem $f$ said to be \define{continuous} at $x\in X$ if
	\[
		(\forall \epsilon>0)(\exists \delta>0)(\forall y\in X)(\rho(y,x)<\delta \Rightarrow \sigma(f(y),f(x))<\epsilon)
	\]

	\vitem $f$ said to be \define{continuous} if $f$ is continuous at every $x\in X$

	\vitem $f$ is continuous \iaoi\ for every open $O\subset Y$, $f^{-1}(O)$ is open

	\vitem if $f:X\to Y$ and $g:Y\to Z$ are continuous, $g\circ f:X\to Z$ is continuous
\eit


\myfoilhead{Homeomorphism}

\bit
	\item one-to-one mapping of $X$ onto $Y$ (or equivalently, one-to-one correspondece between $X$ and $Y$), $f$,
		said to be \define{homeomorphism} if
	\bit
		\item both $f$ and $f^{-1}$ are continuous
	\eit

	\vitem $X$ and $Y$ said to be \define{homeomorphic} if exists homeomorphism

	\vitem \define{topology} is study of properties unaltered by homeomorphisms and
		such properties called \define{topological}

	\vitem one-to-one correspondece $X$ and $Y$ is homeomorphism \iaoi\
		it maps open sets in $X$ to open sets in $Y$ and vice versa

	\vitem every property defined by means of \emph{open sets} (or equivalently, \emph{closed sets})
		or/and being \emph{continuous functions}
		is \emph{topological one}
	\bit
		\item \eg, $f$ is continuous on $X$ is homeomorphism, then $f\circ h^{-1}$ is continuous function on $Y$
	\eit
\eit


\myfoilhead{Isometry}

\bit
	\item homeomorphism preserving distance called \define{isometry}, \ie,
	\[
		(\forall x,y \in X)(\sigma(h(x),h(y)) = \rho(x,y))
	\]

	\vitem $X$ and $Y$ said to be \define{isometric} if exists isometry

	\vitem (from abstract point of view)
		two isometric spaces are exactly \emph{same};
		it's nothing but relabeling of points

	\vitem two metrics, $\rho$ and $\sigma$ on $X$, said to be \define{equivalent}
		if identity mapping of $\metrics{X}{\rho}$ onto $\metrics{X}{\sigma}$
		is homeomorphism
	\bit
		\item hence, two metrics are equivalent \iaoi\ set in one metric is open whenever open in the other metric
	\eit
\eit


\myfoilhead{Convergence - metric spaces}

\bit
	\item $\seq{x_n}$ defined for metric space, $X$

	\bit
		\item said to \define{converge} to $x$, \ie, $\lim x_n =x$ or $x_n \to x$, if
		\[
			(\forall \epsilon>0)(\exists N\in\naturals)(\forall n> N)(\rho(x_n,x)<\epsilon)
		\]
		\bit
			\item [--] equivalently, every ball about $x$ contains all but finitely many points of $\seq{x_n}$
		\eit

		\vitem said to have cluster point, $x$, if
		\[
			(\forall \epsilon>0, N\in\naturals)(\exists n> N)(\rho(x_n,x)<\epsilon)
		\]
		\bit
			\item [--] equivalently, every ball about $x$ contains infinitely many points of $\seq{x_n}$
			\item [--] equivalently, every ball about $x$ contains at least one point of $\seq{x_n}$
		\eit
	\eit

	\vitem every convergent point is cluster point
	\bit
		\item converse not true
	\eit
\eit


\myfoilhead{Completeness - metric spaces}
\pagelabel{page:Completeness---metric-spaces}

\bit
	\item $\seq{x_n}$ of metric space, $X$, called \define{Cauchy sequence} if
	\[
		(\forall \epsilon>0)(\exists N\in\naturals)(\forall n,m> N)(\rho(x_n,x_m)<\epsilon)
	\]

	\vitem convergence sequence is Cauchy sequence

	\vitem $X$ said to be \define{complete} if every Cauchy sequence converges%
		\index{complete!metric spaces}
	\bit
		\item \eg, $\metrics{\reals}{\rho}$ with $\rho(x,y)=|x-y|$
	\eit

	\vitem for incomplete $\metrics{X}{\rho}$, exists complete $X^\ast$
		where $X$ is isometrically embedded in $X^\ast$ as dense set

	\vitem if $X$ contained in complete $Y$,
		$X^\ast$ is isometric with $\closure{X}$ in $Y$
\eit


\myfoilhead{Uniform continuity - metric spaces}
\pagelabel{page:uniform-continuity-metric-spaces}

\bit
	\item $f:X\to Y$ for metric spaces $\metrics{X}{\rho}$ and $\metrics{Y}{\sigma}$
		said to be \define{uniformly continuous} if
		\[
			(\forall \epsilon>0)(\exists \delta)(\forall x,y \in X)(\rho(x,y) < \delta \Rightarrow \sigma(f(x),f(y))<\epsilon)
		\]
	\bit
		\item example of continuous, but not uniformly continuous function
		\bit
			\item [--] $h:[0,1)\to\preals$ with $h(x)=x/(1-x)$
			\item [--] $h$ maps Cauchy sequence $\seq{1-1/n}_{n=1}^\infty$ in $[0,1)$
				to $\seq{n-1}_{n=1}^\infty$ in $\preals$, which is \emph{not} Cauchy sequence
		\eit
	\eit

	\vitem homeomorphism $f$ between $\metrics{X}{\rho}$ and $\metrics{Y}{\sigma}$ with both $f$ and $f^{-1}$
		uniformly continuous called \define{uniform homeomorphism}
\eit


\myfoilhead{Uniform homeomorphism}

\bit
	\item uniform homeomorphism $f$ between $\metrics{X}{\rho}$ and $\metrics{Y}{\sigma}$
		maps every Cauchy sequence $\seq{x_n}$ in $X$ mapped to $\seq{f(x_n)}$ in $Y$ which is Cauchy
	\bit
		\item being Cauchy sequence, hence, being complete preserved by {uniform homeomorphism}
		\item being uniformly continuous also preserved by {uniform homeomorphism}
	\eit

	\vitem each of three properties (being Cauchy sequence, being complete, being uniformly continuous)
		called \define{uniform property}

	\vitem uniform properties are \emph{not} topological properties, \eg, $h$ on page~\pageref{page:uniform-continuity-metric-spaces}
	\bit
		\item is \define{homeomorphism} between incomplete space $[0,1)$ and complete space $\preals$
		\item maps Cauchy sequence $\seq{1-1/n}_{n=1}^\infty$ in $[0,1)$
				to $\seq{n-1}_{n=1}^\infty$ in $\preals$, which is {not} Cauchy sequence
		\item its inverse maps uniformly continuous function $\sin$ back to non-uniformly continuity function on $[0,1)$
	\eit
\eit


\myfoilhead{Uniform equivalence}

\bit
	\item two metrics, $\rho$ and $\sigma$ on $X$, said to be \define{uniformly equivalent}
		if identity mapping of $\metrics{X}{\rho}$ onto $\metrics{X}{\sigma}$
		is uniform homeomorphism, \ie,
		\[
			(\forall \epsilon, \delta>0, x,y \in X)
			(\rho(x,y)<\delta \Rightarrow \sigma(x,y)<\epsilon
			\ \&\ %
			\sigma(x,y)<\delta \Rightarrow \rho(x,y)<\epsilon)
		\]

	\vitem example of uniform equivalence on $X\times Y$
	\bit
		\item any two of below metrics are uniformly equivalent on $X\times Y$
		\begin{eqnarray*}
			&&\tau((x_1,y_1),(x_2,y_2)) = (\rho(x_1,x_2)^2 + \sigma(y_1,y_2)^2)^{1/2}
			\\
			&&\rho_1((x_1,y_1),(x_2,y_2)) = \rho(x_1,x_2) + \sigma(y_1,y_2)
			\\
			&&\rho_\infty((x_1,y_1),(x_2,y_2)) = \max\{\rho(x_1,x_2), \sigma(y_1,y_2)\}
		\end{eqnarray*}
%		\bit
%			\item [--] $\tau((x_1,y_1),(x_2,y_2)) = (\rho(x_1,x_2)^2 + \sigma(y_1,y_2)^2)^{1/2}$
%			\item [--] $\rho_1((x_1,y_1),(x_2,y_2)) = \rho(x_1,x_2) + \sigma(y_1,y_2)^2$
%			\item [--] $\rho_\infty((x_1,y_1),(x_2,y_2)) = \max\{\rho(x_1,x_2), \sigma(y_1,y_2)^2\}$
%		\eit
	\eit

	\vitem for $\metrics{X}{\rho}$ and complete $\metrics{Y}{\sigma}$ and $f:X\to Y$ uniformly continuous on $E\subset X$ into $Y$,
		exists unique continuous extension $g$ of $f$ on $\closure{E}$, which is uniformly continuous
\eit


\myfoilhead{Subspaces}
\pagelabel{page:subspaces}

\bit
	\item for metric space, $\metrics{X}{\rho}$,
		metric space $\metrics{S}{\rho_S}$ with $S\subset X$ and $\rho_S$ being restriction of $\rho$ to S,
		called \define{subspace} of $\metrics{X}{\rho}$
	\bit
		\item \eg\ (with standard Euclidean distance)
		\bit
			\iitem $\rationals$ is subspace of $\reals$
			\iitem $\bigsetl{(x,y)\in\reals^2}{y=0}$ is subspace of $\reals^2$, which is isometric to $\reals$
		\eit
	\eit

	\vitem for metric space, $X$, and its subspace, $S$,
	\bit
		\item $\closure{E}\subset S$ is closure of $E$ relative to $S$.
		\item $A\subset S$ is closure relative to $S$ \iaoi\ $(\exists \closure{F}\subset A)(A = \closure{F}\cap S)$
		\item $A\subset O$ is open relative to $S$ \iaoi\ $(\exists \mbox{ open }{O}\subset A)(A = {O}\cap S)$
	\eit

	\vitem also
	\bit
		\item every subspace of separable metric space is separable
			\index{metric spaces!separable}
			\index{separable!metric spaces}
		\item every complete subset of metric space is closed
		\item every closed subset of complete metric space is complete
	\eit
\eit


\myfoilhead{Compact metric spaces}
\pagelabel{page:Compact-metric-spaces}
\index{metric spaces!compact}

\bit
	\item motivation - want metric spaces where
	\bit
		\item conclusion of Heine-Borel theorem (page~\pageref{page:heine-borel-theorem}) are valid
		\item many properties of $[0,1]$ are true, \eg,
			Bolzano-Weierstrass property
			(page~\pageref{page:bolzano-weierstrass-property-and-sequential-compactness})
	\eit

	\vitem \eg,
	\bit
		\item bounded closed set in $\reals$ has \emph{finite open covering property}
	\eit

	\vitem metric space $X$ called \define{compact metric space} if
		every open covering of $X$, $\collk{U}$,
		contains finite open covering of $X$,
		\eg,
		\[
			(\forall \mbox{ open covering of $X$}, \collk{U})(\exists \{O_1,\ldots,O_n\} \subset \collk{U})
			(X\in\cup O_i)
		\]

	\vitem $A\subset X$ called \define{compact} if
		compact as subspace of $X$
	\bit
		\item \ie, every open covering of $A$ contains finite open covering of $A$
	\eit
\eit


\myfoilhead{Compact metric spaces - alternative definition}
\bit
	\item collection, $\collk{F}$, of sets in $X$ said to have
		\define{finite intersection property}
		if every finite subcollection of $\collk{F}$ has nonempty intersection

	\vitem if rephrase definition of compact metric spaces in terms of \emph{closed} instead of \emph{open}
	\bit
		\item $X$ is called \emph{compact metric space}
			if every collection of closed sets with empty intersection
			contains finite subcollection with empty intersection
	\eit

	\vitem thus, $X$ is compact \iaoi\
		every collection of closed sets with \emph{finite intersection property}
		has nonempty intersection
\eit


\myfoilhead{Bolzano-Weierstrass property and sequential compactness}
\pagelabel{page:bolzano-weierstrass-property-and-sequential-compactness}

\bit
	\item metric space said to
	\bit
		\item have \define{Bolzano-Weierstrass property}
			if every sequence has cluster point
		\item $X$ said to be \define{sequentially compact}
			if every sequence has convergent subsequence
	\eit

	\vitem \fact{$X$ has Bolzano-Weierstrass property
		\iaoi\
		sequentially compact}
		\proofref{Bolzano-Weierstrass-implies-seq-compact}
\eit


\myfoilhead{Compact metric spaces - properties}

\bit
	\vitem following three statements about metric space are equivalent
		\fact{(not true for general topological sets)}
	\bit
		\item being compact
		\item having Bolzano-Weierstrass property
		\item being sequentially compact
	\eit

	\vitem compact metric spaces have corresponding to some of those of complete metric spaces
	(compare with statements on page~\pageref{page:subspaces})
	\bit
		\item every compact subset of metric space is closed \emph{and bounded}
		\item every closed subset of compact metric space is compact
	\eit

	\vitem (will show above in following slides)
\eit


\myfoilhead{Necessary condition for compactness}
\pagelabel{page:Necessary-condition-for-compactness}

\bit
	\item compact metric space is sequentially compact \proofref{compact-in-metric-implies-seq-compact}

	\vitem equivalently, compact metric space has Bolzano-Weierstrass property
		(page~\pageref{page:bolzano-weierstrass-property-and-sequential-compactness})
\eit


\myfoilhead{Necessary conditions for sequentially compactness}
\pagelabel{page:sequentially-compact-metric-spaces-facts}

\bit
	\item every continuity real-valued function on sequentially compact space
		is \emph{bounded and assumes its maximum and minimum}

	\vitem sequentially compact space is \emph{totally bounded}

	\vitem every open covering of sequentially compact space
		has \emph{Lebesgue number}
\eit


\myfoilhead{Sufficient conditions for compactness}
\pagelabel{page:Sufficient-conditions-for-compactness}

\bit
	\item metric space that is totally bounded and has Lebesgue number for every covering
		is compact
\eit


\myfoilhead{Borel-Lebesgue theorem}
\pagelabel{page:Borel-Lebesgue-theorem}

\bit
	\item conditions on
		pages \pageref{page:Necessary-condition-for-compactness},
		\pageref{page:sequentially-compact-metric-spaces-facts},
		and
		\pageref{page:Sufficient-conditions-for-compactness}
		imply the following equivalent statements
	\bit
		\item $X$ is \emph{compact}
		\item $X$ has \emph{Bolzano-Weierstrass property}
		\item $X$ is \emph{sequentially compact}
	\eit

	\vitem above called \name{Borel-Lebesgue theorem}%
		\index{Borel-Lebesgue theorem}%
		\index{Borel, F\'{e}lix \'{E}douard Justin \'{E}mile!Borel-Lebesgue theorem}%
		\index{Lebesgue, Henri L\'{e}on!Borel-Lebesgue theorem}


	\vitem hence, can drop \emph{sequentially} in every statement on page~\pageref{page:sequentially-compact-metric-spaces-facts},
		\ie,
	\bit
		\item every continuity real-valued function on \graystrikethrough{sequentially} compact space is \emph{bounded and assumes its maximum and minimum}
		\item \graystrikethrough{sequentially} compact space is \emph{totally bounded}
		\item every open covering of \graystrikethrough{sequentially} compact space
			has \emph{Lebesgue number}
	\eit
\eit


\myfoilhead{Compact metric spaces - other facts}
\pagelabel{page:Compact-metric-spaces---other-facts}

\bit
	\item closed subset of compact space is compact

	\vitem compact subset of metric space is \emph{closed and bounded}

	\bit
		\item hence, Heine-Borel theorem (page~\pageref{page:heine-borel-theorem}) implies

		\item [] \fact{set of $\reals$ is compact \iaoi\ closed and bounded}
	\eit

	\vitem metric space is compact \iaoi\ it is complete and totally bounded
		\pagelabel{page:compac-complete-and-totally-bounded}

	\vitem thus, \eemph{compactness can be viewed as absolute type of closedness}
	\bit
		\item [-] refer to page~\pageref{page:Compact-spaces---facts} for exactly same comments for general topological spaces
	\eit

	\vitem continuous image of compact set is compact

	\vitem continuous mapping of compact metric space into metric space
		is uniformly continuous
\eit


\myfoilhead{Diagrams for relations among metric spaces}%
	\index{metric spaces!diagrams for relations among}%
	\pagelabel{page:Necessary-and-sufficient-conditions-for-compactness}

\bit
	\item \figref{diagrams for relations among metric spaces}\
		shows relations among metric spaces stated on pages
		\pageref{page:sequentially-compact-metric-spaces-facts},
		\pageref{page:Sufficient-conditions-for-compactness},
		\pageref{page:Borel-Lebesgue-theorem},
		and
		\pageref{page:compac-complete-and-totally-bounded}
\eit

\vfill
\begin{figure}
\begin{center}
	\mypsfrag{totally bounded}{totally bounded}
	\mypsfrag{Lebesgue number}{Lebesgue number}
	\mypsfrag{compact}{compact $\Leftrightarrow$ Bolzano-Weierstrass $\Leftrightarrow$ sequentially compact}
	\mypsfrag{complete}{complete}
	\mypsfrag{empty}{empty}
%	\includegraphics[width=.6\textwidth]{figures/metric-compact-diagrams-circles}
%	\includegraphics[width=.7\textwidth]{figures/metric-compact-diagrams-wrong}
	\includegraphics[width=.55\textwidth]{figures/metric-compact-diagrams}%
		\idxfig{diagrams for relations among metric spaces}%
		\label{fig:diagrams for relations among metric spaces}
\end{center}
\end{figure}


\myfoilhead{Baire category}

\bit
	\item do (more) deeply into certain aspects of complete metric spaces,%
			\index{complete!metric spaces}
		namely, \define{Baire theory of category}%
			\index{metric spaces!Baire theory of category}%
			\index{category theory!Baire theory of category}%
			\index{Baire, Ren'{e}-Louis!Baire theory of category}

	\vitem subset $E$ in metric space where $\sim (\closure{E})$ is dense,
		said to be \define{nowhere dense}%
		\index{category theory!nowhere dense}
	\bit
		\item equivalently, \closure{E}\ contains no nonempty open set
	\eit

	\vitem union of countable collection of \emph{nowhere open sets},
		said to be \define{of first category or meager}%
		\index{category theory!first category or meager}
		\index{category theory!meager}

	\vitem set not of first category, said to be \define{of second category or nonmeager}
		\index{category theory!second category or nonmeager}
		\index{category theory!nonmeager}

	\vitem complement of set of first category, called \define{residual or co-meager}
		\index{category theory!residual}
		\index{category theory!co-meager}
\eit


\myfoilhead{Baire category theorem}

\bit
	\item \name{Baire theorem -}%
		\index{metric spaces!Baire theorem}%
		\index{category theory!Baire theorem}%
		\index{Baire, Ren'{e}-Louis!Baire theorem}\pagelabel{page:Baire-theorem}
		for complete metric space, $X$,
		and countable collection of dense open subsets, $\seq{O_k}\subset X$,
		the intersection of the collection
		\[
			\bigcap O_k
		\]
		is dense

		\bit
			\item [-] refer to page~\pageref{page:locally-compact-space-version-of-Baire-theorem}
				for locally compact space version of Baire theorem
		\eit

	\vitem \name{Baire category theorem -}%
		\index{metric spaces!Baire category theorem}%
		\index{category theory!Baire category theorem}%
		\index{Baire, Ren'{e}-Louis!Baire category theorem}
		no nonempty open subset of complete metric space
		is of first category,
		\ie,
		union of countable collection of nowhere dense subsets

	\vitem Baire category theorem is \emph{unusual} in that
		\emph{uniform property, \ie, completeness of metric spaces,
		implies purely topological nature}\footnote{
			``no nonempty open subset of complete metric space is of first category''
			is purely topological nature
			because if two spaces are (topologically) homeomorphic,
			and no nonempty open subsets of one space is of first category,
			then neither is any nonempty open subset of the other space
		}
\eit


\myfoilhead{Second category everywhere}%
\index{category theory!second category everywhere}

\bit
	\item metric or topological spaces with property that
			``no nonempty open subset of complete metric space is of first category'',
			said to be \define{of second category everywhere}
			(with respect to themselves)%
			\index{category theory!second category everywhere}

	\vitem Baire category theorem says \emph{complete metric space} is of second category everywhere%
		\index{complete!metric spaces}

	\vitem locally compact Hausdorff spaces are of second category everywhere, too
			(refer to page~\pageref{page:Locally-compact-Hausdorff-spaces} for definition of locally compact Hausdorff spaces)%
			\index{category theory!locally compact Hausdorff spaces}
	\bit
		\item for these spaces, though, many of results of category theory
			follow directly from \emph{local compactness}
	\eit
\eit


\myfoilhead{Sets of first category}%
\index{category theory!first category}

\bit
	\item collection of sets with following properties, called \define{a $\sigma$-ideal of sets}%
		\index{topological spaces!$\sigma$-ideal of sets}
		\index{ideals!topological spaces!$\sigma$-ideal of sets}
	\bit
		\item countable union of sets in the collection is, again, in the collection
		\item subset of any in the collection is, again, in the collection
	\eit

	\vitem both of below collections are $\sigma$-ideal of sets
	\bit
		\item sets of first category in topological space
		\item measure zero sets in complete measure space
	\eit

	\vitem sets of first category regards as ``small'' sets%
		\index{category theory!first category}
	\bit
		\item such sets in complete metric spaces no interior points
	\eit

	\vitem interestingly!
		set of first category in $[0,1]$
		can have Lebesgue measure $1$,
		hence complement of which is residual set of measure zero
\eit


\myfoilhead{Some facts of category theory}

\bit
	\item for open set, $O$, and closed set, $F$,
		$\closure{O}\sim O$ and $F\sim \interior{F}$ are nowhere dense

	\vitem closed set of first category in complete metric space is nowhere dense

	\vitem subset of complete metric space is residual \iaoi\ contains dense $G_\delta$,
		hence
		subset of complete metric space is of first category \iaoi\ contained in $F_\sigma$
		whose complement is dense

	\vitem for countable collection of closed sets, \seq{F_n},
		$\bigcup \interior{F_n}$ is residual open set;
		if $\bigcup F_n$ is complete metric space,
		$O$ is dense

	\vvitem some applications of category theory to analysis
		seem almost too good to be belived;
		here's one:

	\vitem \name{uniform boundedness principle -}
		for family, \collF, of real-valued continuous functions on complete metric space, $X$,
		with property that $(\forall x\in X)(\exists M_x\in\reals)(\forall f\in\collF)(|f(x)|\leq M_x)$
	\[
		(\exists \mbox{ open }O, M\in\reals)(\forall x\in O, f\in\collF)(|f(x)|\leq M)
	\]
\eit


\yesnoexec{\showincomplete}{%
\myfoilhead{Absolute $G_\delta$'s}%
\index{metric spaces!absolute $G_\delta$'s}%
\index{absolute $G_\delta$'s!metric spaces}

\bit
	\item XXX Royden p164\idxtodo{3 - absolute $G_\delta$'s}
\eit
}{}


\yesnoexec{\showincomplete}{%
\myfoilhead{Ascoli-Arzel\'{a} theorem}%
\index{metric spaces!Ascoli-Arzel\'{a} theorem}%
\index{Ascoli-Arzel\'{a} theorem!metric spaces}

\bit
	\item XXX Royden p167\idxtodo{5 - Ascoli-Arzel\'{a} theorem}
\eit
}{}


\titlefoil{Topological Spaces}{topological-spaces}%
	\index{topological spaces}

\myfoilhead{Motivation for topological spaces}%
	\index{topological spaces!motivation}%
	\index{topological spaces}

\bit
	\item want to have something like
	\bit
		\item notion of open set is fundamental
		\item other notions defined in terms of open sets
		\item more general than metric spaces
	\eit

	\vitem why not stick to metric spaces?
	\bit
		\item certain notions have natural meaning
			\emph{not} consistent with topological concepts
			derived from metric spaces
		\bit
			\item [--] \eg. weak topologies in Banach spaces
		\eit
	\eit
\eit


\myfoilhead{Topological spaces}%
	\index{topological spaces}

\bit
	\item \topos{X}{J}\ with nonempty set $X$ of points and family \tJ\ of subsets,
		which we call open, having the following properties
		called
		\define{topological spaces}%
			\index{topological spaces}
	\bit
		\item $\emptyset, X\in\tJ$
		\item $O_1, O_2 \in\tJ \Rightarrow O_1 \cap O_2 \in\tJ$
		\item $O_\alpha \Rightarrow \cup_\alpha O_\alpha \in \tJ$
	\eit

	\vitem family, \tJ, is called \define{topology}%
			\index{topological spaces!topology}%
			\index{topology}

	\vitem for $X$, \emph{always exist two topologies} defined on $X$
	\bit
		\item \define{trivial topology} having only $\emptyset$ and $X$%
			\index{topology!trivial topology}%
			\index{topological spaces!trivial topology}%
			\index{trivial topology}
		\item \define{discrete topology} for which every subset of $X$ is an open set%%%%%
			\index{topology!discrete topology}%
			\index{topological spaces!discrete topology}%
			\index{discrete topology}
	\eit
\eit


\myfoilhead{Topological spaces associated with metric spaces}

\bit
	\item can associate topological space, \topos{X}{J}, to any metric space \metrics{X}{\rho}
		where \tJ\ is family of open sets in \metrics{X}{\rho}

	\bit
		\bitem because properties in definition of topological space
			satisfied by open sets in metric space
	\eit

	\vitem \topos{X}{J}\ assiaciated with metric space, \metrics{X}{\rho}\ said to be \define{metrizable}

	\bit
		\item $\rho$ called \define{metric for} \tXJ\
	\eit

	\vitem distinction between metric space and associated topological space is \emph{essential}
	\bit
		\bitem because different metric spaces associate same topological space
		\item in this case, these metric spaces are equivalent
	\eit

	\vitem metric and topological spaces are couples
\eit


\myfoilhead{Some definitions for topological spaces}

\bit
	\item subset $F\subset X$ with $\compl{F}$ is open called \define{closed}
	\vitem intersection of all closed sets containing $E\subset X$ called \define{closure} of $E$ denoted by \closure{E}\
	\bit
		\item [--] \closure{E}\ is smallest closed set containing $E$
	\eit
	\vitem $x\in X$ called \define{point of closure} of $E\subset X$
		if every open set containing $x$ meets $E$,
		\ie, has nonempty intersection with $E$
	\vitem union of all open sets contained in $E\subset X$ is called \define{interior} of $E$ denoted by \interior{E}\
	\vitem $x\in X$ called \emph{interior point} of $E$ if exists open set, $E$, with $x\in O\subset E$
\eit


\myfoilhead{Some properties of topological spaces}

\bit
	\item $\emptyset$, $X$ are closed
	\vitem union of closed sets is closed
	\vitem intersection of any collection of closed sets is closed

	\vvitem $E\subset \closure{E}$, $\closure{\closure{E}} = \closure{E}$, $\closure{A\cup B} = \closure{A} \cup \closure{B}$
	\vitem $F$ closed \iaoi\ $\closure{F}=F$
	\vitem \closure{E}\ is set of \emph{points of closure} of $E$

	\vvitem $\interior{E}\subset E$, $\interior{(\interior{E})} = \interior{E}$, $\interior{(A\cup B)} = \interior{A} \cup \interior{B}$
	\vitem \interior{E}\ is set of \emph{interior points} of $E$

	\vvitem $\interior{(\compl{E})} = \sim \closure{E}$
\eit


\myfoilhead{Subspace and convergence of topological spaces}

\bit
	\item for subset of \topos{X}{J}, $A$,
		define \define{topology \tS\ for} $A$
		with $\tS = \set{A\cap O}{O \in \tJ}$
	\bit
		\item \tS\ called \define{topology inherited from \tJ}
		\item \topos{A}{S}\ called \define{subspace} of \topos{X}{J}\
	\eit

	\vitem \seq{x_n}\ said to \define{converge} to $x\in X$ if

	\begin{eqn}
		(\forall O \in \tJ \mbox{ containing } x)(\exists N\in\naturals)(\forall n>N)(x_n \in O)
	\end{eqn}

	\bit
		\item denoted by
%$
		\begin{eqn}
%		\[
			\lim x_n = x
%		\]
		\end{eqn}
%$
	\eit

	\vitem \seq{x_n}\ said to have $x\in X$ as \define{cluster point} if

	\begin{eqn}
%	\[
		(\forall O \in\tJ\mbox{ containing } x, N\in\naturals)(\exists n>N)(x_n \in O)
%	\]
	\end{eqn}

	\vitem \seq{x_n}\ has converging subsequence to $x\in X$, then $x$ is cluster point of \seq{x_n}\
	\bit
		\item converse is \emph{not} true for arbitrary topological space
	\eit
\eit


\myfoilhead{Continuity in topological spaces}

\bit
	\item mapping $f:X\to Y$ with \topos{X}{J}, \topos{Y}{S}\ said to be \define{continuous} if

	\begin{eqn}
		(\forall O\in \tS)(f^{-1}(O) \in \tJ)
	\end{eqn}

	\vitem $f:X \to Y$ said to be \define{continuous at} $x\in X$ if

	\begin{eqn}
		(\forall O\in\tS\mbox{ containing } f(x))(\exists U\in\tJ\mbox{ containing } x)(f(U)\subset O)
	\end{eqn}

	\vitem $f$ is continuous \iaoi\ $f$ is continuous at every $x\in X$

	\vitem for continuous $f$ on \topos{X}{J}, restriction $g$ on $A\subset X$ is continuous
		\proofref{restriction-of-continuous-topology-continuous}

	\vitem for $A$ with $A=A_1 \cup A_2$ where both $A_1$ and $A_2$ are either open or closed,
		$f:A\to Y$ with each of both restrictions, \restrict{f}{A_1}\ and \restrict{f}{A_2}, continuous,
		is continuous
\eit


\myfoilhead{Homeomorphism for topological spaces}

\bit
	\item one-to-one continuous function of $X$ onto $Y$, $f$, with continuous inverse function, $f^{-1}$,
		called \define{homeomorphism} between $\topos{X}{J}$ and $\topos{Y}{S}$

	\vitem $\topos{X}{J}$ and $\topos{Y}{S}$ said to be \define{homeomorphic} if exists homeomorphism between them

	\vitem homeomorphic spaces are indistinguishable where homeomorphism amounting to relabeling of points
		(from abstract pointp of view)

	\vitem thus, below roles are same
	\bit
		\item role that \emph{homeomorphism plays for topological spaces}
		\item role that \emph{isometry plays for metric spaces}
		\item role that \emph{isomorphism plays for algebraic systems}\index{isomorphism!algebraic systems}
	\eit
\eit


\myfoilhead{Stronger and weaker topologies}
\pagelabel{page:topological-spaces-stronger-and-weaker-topologies}

\bit
	\item for two topologies, \tJ\ and \tS\ for same $X$ with $\tS\supset\tJ$
	\bit
		\item \tS\ said to be \define{stronger or finer} than \tJ\
		\item \tJ\ said to be \define{weaker or coarser} than \tS\
	\eit

	\vitem \tS\ is stronger than \tJ\ \iaoi\ identity mapping of \topos{X}{S}\ to \topos{Y}{J}\ is continuous

	\vitem for two topologies, \tJ\ and \tS\ for same $X$, $\tJ\cap\tS$ also topology

	\vitem for any collection of topologies, $\{\tJ_\alpha\}$ for same $X$,
		$\cap_\alpha \tJ_\alpha$ is topology

	\vitem for nonempty set, $X$, and any collection of subsets of $X$, \coll\
	\bit
		\item \fact{exists weakest topology containing \coll,} \ie, weakest topology where all subsets in \coll\ are open
		\item it is intersection of all topologies containing \coll\
	\eit
\eit


\myfoilhead{Bases for topological spaces}

\bit
	\item collection \collB\ of open sets of \tXJ\
		called \define{a base for topology,} \tJ, of $X$\index{topological spaces!base}\index{base!topological spaces}\
		if

	\begin{eqn}
		(\forall O\in \tJ, x\in O)(\exists B\in\collB)(x\in B\subset O)
	\end{eqn}

	\vitem collection $\collB_x$ of open sets of \tXJ\ containing $x$ called \define{a base at} $x$
		if

	\begin{eqn}
		(\forall O\in\tJ \mbox{ containing }x)(\exists B\in\collB_x)(x\in B\subset O)
	\end{eqn}

	\bit
		\item elements of $\collB_x$ often called \define{neighborhoods of} $x$\index{topological spaces!neighborhood}
		\item when no base given, \define{neighborhood of} $x$ is an open set containing $x$
	\eit

	\vitem thus, \collB\ of open sets is a base \iaoi\ contains a base for every $x\in X$

	\vitem for topological space that is also metric space
	\bit
		\item all balls from a base
		\item balls centered at $x$ form a base at $x$
	\eit
\eit


\myfoilhead{Characterization of topological spaces in terms of bases}

\bit
	\item \emph{definition of open sets in terms of base} - when \collB\ is base of \tXJ\

	\begin{eqn}
		(O\in\tJ) \Leftrightarrow (\forall x\in O)(\exists B\in\collB)(x\in B\subset O)
	\end{eqn}

	\vitem often, convenient to specify topology for $X$ by
	\bit
		\item specifying a base of open sets, \collB, and
		\item using above criterion to define open sets
	\eit

	\vitem collection of subsets of $X$, \collB, is base for some topology \iaoi\

	\begin{eqna}
		&(\forall x\in X)(\exists B\in\collB)(x\in B)&
		\\
		&\mbox{and}&
		\\
		&(\forall x\in X, B_1, B_2 \in \collB \mbox{ with } x\in B_1\cap B_2)
		(\exists B_3\in \collB)(x\in B_3 \subset B_1\cap B_2)&
	\end{eqna}

	\bit
		\item \emph{condition of collection to be basis for some topology}
	\eit
\eit


\myfoilhead{Subbases for topological spaces}

\bit
	\item for \tXJ, collection of open sets, \coll\, called \define{a subbase} for topology \tJ\
		if

	\begin{eqn}
		(\forall O\in \tJ, x\in O)(\exists \seq{C_i}_{i=1}^n\subset\coll)(x\in \cap C_i \subset O)
	\end{eqn}

	\bit
		\item sometimes convenient to define topology in terms of subbase
	\eit

	\vitem for subbase for \tJ, \coll, collection of finite intersections of sets from \coll\
		forms base for \tJ\

	\vitem any collection of subsets of $X$ is subbase for weakest topology
		where sets of the collection are open
\eit


\myfoilhead{Axioms of countability}%
\index{countability!axiom of countability}
\pagelabel{page:topological-spaces-axioms-of-countability}

\bit
	\item topological space said to satisfy \define{first axiom of countability}%
			\index{axiom of countability!first}
		if
		exists countable base at every point

	\bit
		\item every metric space satisfies first axiom of countability
			because for every $x\in X$, set of balls centered at $x$ with rational radii
			forms base for $x$
	\eit

	\vitem topological space said to satisfy \define{second axiom of countability}%
			\index{axiom of countability!second}
		if
		exists countable base for the space

	\bit
		\item every metric space satisfies second axiom of countability
			\iaoi\ separable (refer to page~\pageref{page:dense-sets-and-separability-metric-spaces} for definition of separability)
			\index{separable!metric spaces}
			\index{metric spaces!separable}
	\eit
\eit


\myfoilhead{Topological spaces - facts}

\bit
	\item given base, \collB, for \tXJ\

	\bit
		\vitem $x \in \closure{E}$ \iaoi\ $(\exists B\in\collB)(x\in B \ \&\ B\cap E \neq \emptyset)$
	\eit

	\vvitem given base at $x$ for \tXJ, $\collB_x$, and base at $y$ for $\topos{Y}{S}$, $\topol{C}_y$

	\bit
		\vitem $f:X\to Y$ continuous at $x$ \iaoi\
		$
			(\forall C\in\topol{C}_y)(\exists B\in\collB_x)(B\subset f^{-1}(C))
		$
	\eit

	\vvitem if \tXJ\ satisfies \emph{first axiom of countability}%
			\index{axiom of countability!first}

	\bit
		\vitem $x \in \closure{E}$ \iaoi\ $(\exists \seq{x_n} \mbox{ from } E)(\lim x_n = x)$
		\vitem $x$ cluster point of \seq{x_n}\ \iaoi\ exists its subsequence converging to $x$
	\eit


	\vvitem \tXJ\ said to be \define{Lindel\"of space}
		or have \define{Lindel\"of property}
		if
		every open covering of $X$ has countable subcover

	\vitem second axiom of countability implies \define{Lindel\"of property}%
			\index{axiom of countability!second}
\eit


\myfoilhead{Separation axioms}

\bit
	\item why separation axioms

	\bit
		\item properties of topological spaces are (in general) quite different from those of metric spaces
		\item often convenient assume additional conditions true in metric spaces
	\eit

	\vvitem separation axioms%
		\index{topological spaces!separation axioms}

	\bit
		\vitem \define{$T_1$ - Tychonoff spaces}\index{topological spaces!Tychonoff spaces}\index{Tychonoff, Andrey Nikolayevich!Tychonoff spaces}
		\bit
			\item [-] $(\forall x \neq y \in X)(\exists \mbox{ open }O\subset X)(y \in O, x \not\in O)$
		\eit

		\vitem \define{$T_2$ - Hausdorff spaces}\index{topological spaces!Hausdorff spaces}\index{Hausdorff, Felix!spaces}
		\bit
			\item [-] $(\forall x \neq y \in X)(\exists \mbox{ open }O_1, O_2\subset X \mbox{ with } O_1\cap O_2=\emptyset)(x \in O_1, y \in O_2) $
		\eit

		\vitem \define{$T_3$ - regular spaces}\index{topological spaces!regular spaces}
		\bit
			\item [-] $T_1$ \&\
			$ (\forall \mbox{ closed } F \subset X, x \not\in F)
			(\exists \mbox{ open }O_1, O_2\subset X \mbox{ with } O_1\cap O_2=\emptyset)
			(x \in O_1, F \subset O_2)$
		\eit

		\vitem \define{$T_4$ - normal spaces}\index{topological spaces!normal spaces}
		\bit
			\item [-] $T_1$ \&\
			$ (\forall \mbox{ closed } F_1, F_2 \subset X)
			(\exists \mbox{ open }O_1, O_2\subset X \mbox{ with } O_1\cap O_2=\emptyset)
			(F_1 \subset O_1, F_2 \subset O_2)$
		\eit
	\eit
\eit


\myfoilhead{Separation axioms - facts}

\bit
	\item necessary and sufficient condition for $T_1$

	\bit
		\vitem topological space satisfies $T_1$ \iaoi\ every singletone, $\{x\}$, is closed
	\eit

	\vvitem important consequences of normality, $T_4$

	\bit
		\vitem \name{Urysohn's lemma -} for normal topological space, $X$\
%
%		\begin{leqn}
		$$
			(\forall \mbox{ disjoint closed } A, B \subset X) (\exists f\in C(X,[0,1])) (f(A) = \{0\}, f(B) = \{1\})
		$$
%		\end{leqn}

		\vitem \name{Tietze's extension theorem -} for normal topological space, $X$\
%
%		\begin{leqn}
		$$
			(\forall \mbox{ closed } A \subset X, f\in C(A,\reals))
			(\exists g \in C(X,\reals))
			(\forall x \in A)
			(g(x) = f(x))
		$$
%		\end{leqn}

		\vitem \name{Urysohn metrization theorem -}
			\emph{normal} topological space satisfying \emph{second axiom of countability}\index{axiom of countability!second}
			is \emph{metrizable}
%			(refer to page~\pageref{page:topological-spaces-axioms-of-countability})
	\eit
\eit


\myfoilhead{Weak topology generated by functions}

\bit
	\item given any set of points, $X$ \& any collection of functions of $X$ into $\reals$, \collk{F},
		exists weakest totally on $X$ such that
		all functions in \collk{F}\ is continuous

	\bit
		\vitem it is weakest topology containing
			- refer to page~\pageref{page:topological-spaces-stronger-and-weaker-topologies}\

			\begin{eqn}
				\coll\ = \bigcup_{f\in\collk{F}} \bigcup_{O\subset \reals} f^{-1}(O)
			\end{eqn}

		\vitem called \define{weak topology generated by} \collk{F}\
	\eit
\eit
\vfill
\vfill
\vfill
\vfill


\myfoilhead{Complete regularity}

\bit
	\item for \tXJ\ and continuous function collection \collk{F},
		\emph{weak topology} generated by \collk{F}\ is weaker than \tJ\

	\bit
		\item however, if

		\begin{eqn}
			(\forall \mbox{ closed } F\subset X, x \not\in F)(\exists f\in\collk{F})(f(A)=\{0\}, f(x)=1)
		\end{eqn}

		then, \emph{weak topology generated by} \collk{F} coincides with \tJ\

		\vitem if condition satisfied by $\collk{F} = C(X,\reals)$,
			$X$ said to be \define{completely regular}
			provided $X$ satisfied $T_1$ (Tychonoff space)
	\eit

	\vvitem every normal topological ($T_4$) space is completely regular (Urysohn's lemma)
	\vitem every completely regular space is regular space ($T_3$)
	\vitem complete regularity sometimes called \define{$T_{3\frac{1}{2}}$}
\eit


\myfoilhead{Diagrams for separation axioms for topological spaces}%
	\index{topological spaces!diagrams for separation axioms for}%
	\pagelabel{page:Diagram-for-separation-axioms}

\bit
	\item \figref{diagrams for separation axioms for topological spaces}\
		shows $T_4 \Rightarrow T_{3\frac{1}{2}} \Rightarrow T_3 \Rightarrow T_2 \Rightarrow T_1$
	\vitem every metric spaces is normal space
\eit

\vfill
\begin{figure}
\begin{center}
	\mypsfrag{T1}{$T_1$ - Tychonoff spaces}
	\mypsfrag{T2}{$T_2$ - Hausdorff spaces}
	\mypsfrag{T3}{$T_3$ - regular spaces}
	\mypsfrag{T3.5}{$T_{3\frac{1}{2}}$ - completely regular spaces}
	\mypsfrag{T4}{$T_4$ - normal spaces}
	\mypsfrag{M}{$M$ - metric spaces}

%	\mypsfrag{T1}{Tychonoff spaces - $T_1$}
%	\mypsfrag{T2}{Hausdorff spaces - $T_2$}
%	\mypsfrag{T3}{regular spaces - $T_3$}
%	\mypsfrag{T3.5}{completely regular spaces - $T_{3\frac{1}{2}}$}
%	\mypsfrag{T4}{normal spaces - $T_4$}
%	\mypsfrag{M}{metric spaces - $M$}

	\includegraphics[width=.5\textwidth]{figures/separation-axioms-diagrams}%
		\idxfig{diagrams for separation axioms for topological spaces}%
		\label{fig:diagrams for separation axioms for topological spaces}
\end{center}
\end{figure}
\vfill


\myfoilhead{Topological spaces of interest}

\bit
	\item very general topological spaces quite bizarre

	\bit
		\item do \eemph{not} seem to be much needed in analysis
	\eit

	\vitem only topological spaces (Royden) found useful for analysis are

	\bit
		\item metrizable topological spaces
		\item locally compact Hausdorff spaces%
			\index{topological spaces!locally compact Hausdorff spaces}%
			\index{Hausdorff, Felix!locally compact spaces}%
			\index{locally compact spaces!Hausdorff, Felix}
		\item topological vector spaces
	\eit

	\vitem all above are \emph{completely regular}

	\vfill
	\vvitem algebraic geometry, however, uses Zariski topology on affine or projective space,
		topology giving us compact $T_1$ space which is not Hausdorff%
\eit


\myfoilhead{Connectedness}

\bit
	\item topological space, $X$,said to be \define{connected} if \emph{not} exist two nonempty disjoint open sets, $O_1$ and $O_2$,
		such that $O_1\cup O_2 = X$

	\bit
		\vitem such pair, $(O_1, O_2)$, if exist, called \define{separation of} $X$

		\vitem pair of disjoint nonempty closed sets, $(F_1,F_2)$, with $F_1\cup F_2=X$
			is also \define{separation of} $X$ - because they are also open
	\eit

	\vitem $X$ is connected \iaoi\ only subsets that are both closed and open are $\emptyset$ and $X$

	\vitem subset $E\subset X$ said to be \define{connected}
		if connected in topology inherited from \tXJ\

	\bit
		\vitem thus, $E$ is connected if not exist two nonempty open sets, $O_1$ and $O_2$,
			such that $E\subset O_1\cup O_2$ and $E\cap O_1\cap O_2 = \emptyset$
	\eit
\eit


\myfoilhead{Properties of connected space, component, and local connectedness}

\bit
	\item if exists continuous mapping of connected space to topological space, $Y$, $Y$ is connected

	\vitem \name{(generalized version of) intermediate value theorem -} for $f:X\to\reals$ where $X$ is connected

	\begin{eqn}
		(\forall x, y \in X, c\in \reals \mbox{ with } f(x) < c < f(y))(\exists z \in X)(z=f(z))
	\end{eqn}

	\vitem subset of $\reals$ is connected \iaoi\ is either interval or singletone

	\vvitem for $x\in X$, union of all connected sets containing $x$ is called \define{component}

	\bit
		\vitem component is \emph{connected and closed}
		\vitem two components containing same point coincide
		\vitem thus, \fact{$X$ is disjoint union of components}
	\eit

	\vvitem $X$ said to be \define{locally connected} if exists base for $X$ consisting of connected sets

	\bit
		\vitem components of locally connected space are \emph{open}
		\vitem space \emph{can be connected, but not locally connected}
	\eit
\eit


\myfoilhead{Product topological spaces}%
	\index{product topological spaces}

\bit
	\item for \tXJ\ and \topos{Y}{S}, topology on $X\times Y$ taking as \emph{a base} the following

	\begin{eqn}
		\set{O_1 \times O_2}{O_1 \in \tJ, O_2 \in \topol{S}}
	\end{eqn}

	\item [] called \define{product topology} for $X\times Y$%
		\index{topological spaces!product!product topology}

	\bit
		\item for metric spaces, $X$ and $Y$, \emph{product topology is product metric}
	\eit

	\vitem for indexed family with index set, \collk{A},
		\topos{X_\alpha}{\tJ_\alpha}, product topology on $\bigtimes_{\alpha\in\collk{A}} X_{\alpha}$
		defined as taking as \emph{a base} the following

	\begin{teqn}
		\bigsetl{\bigtimes X_\alpha}{O_\alpha\in \tJ_\alpha, O_\alpha = X_\alpha \mbox{ except finite number of }\alpha}
	\end{teqn}

	\vitem $\pi_\alpha: \bigtimes X_{\alpha} \to X_\alpha$ with $\pi_\alpha(y) = x_\alpha$,
		\ie, $\alpha$-th coordinate, called \define{projection}%
			\index{topological spaces!product!projection}

	\bit
		\item every $\pi_\alpha$ continuous
		\item $\bigtimes X_\alpha$ \emph{weakest topology} with continuous $\pi_\alpha$'s
	\eit

	\vitem if $(\forall \alpha\in\collk{A})(X_\alpha=X)$, $\bigtimes X_{\alpha}$ denoted by $X^\collk{A}$
\eit


\myfoilhead{Product topology with countable index set}
\pagelabel{page:product-topology-with-countable-index-set}

\bit
	\item for countable \collk{A}%
		\index{topological spaces!product!countable}

	\bit
		\item $\bigtimes X_\alpha$ \emph{denoted by $X^\omega$ or $X^\naturals$}
			$\because$ only \# elements of \collk{A}\ important

		\bit
			\item [--] \eg, $\mbox{\bf 2}^\omega$ is \emph{Cantor set} if denoting discrete topology with two elements by \mbox{\bf 2}\
		\eit
	\eit

	\vitem if $X$ is metrizable, $X^\omega$ is metrizable
		\index{topological spaces!metrizable}

	\vitem \fact{$\naturals^\omega = \naturals^\naturals$ is topology space homeomorphic to $\reals\sim\rationals$}
		when denoting discrete topology with countable set also by $\naturals$%
		\idximportant{$\naturals^\omega = \naturals^\naturals$ is topology space homeomorphic to $\reals\sim\rationals$}
\eit


\myfoilhead{Product topologies induced by set and continuous functions}

\bit
	\item for $I=[0,1]$, $I^\collk{A}$ called \define{cube}

	\vitem $I^\omega$ is metrizable, and called \define{Hilbert cube}

	\vitem for any set $X$ and any collection of $f:X\to[0,1]$, \collk{F}\
		with $(\forall x\neq y\in X)(\exists f\in\collk{F})(f(x)\neq f(y))$

	\bit
		\vitem can define \emph{one-to-one mapping of \collk{F}\ into $I^X$}
			with $f(x)$ as $x$-th coordinate of $f$

		\bit
			\vitem [--] $\pi_x: \collk{F} \to I$ (mapping of \collk{F}\ into $I$) with $\pi_x(f) = f(x)$

			\vitem [--] topology that \collk{F}\ inherits as subspace of $I^X$ called
				\define{topology of pointwise convergence}
				(because $\pi_x$ is project, hence continuous)
		\eit

		\vitem can define \emph{one-to-one mapping of $X$ into $I^\collk{F}$}
			with $f(x)$ as $f$-th coordinate of $x$

		\bit
			\vitem [--] topology of $X$ as subspace of $I^\collk{F}$ is \emph{weak topology generated by \collk{F}}

			\vitem [--] if every $f\in\collk{F}$ is continuous,

			\bit
				\vitem [--] topology of $X$ into $I^\collk{F}$ is continuous
				\vitem [--] if for every closed $F\subset X$ and for each $x\not\in F$,
					exists $f\in\collk{F}$ such that $f(x)=1$ and $f(F)=\{0\}$,
					then \emph{$X$ is homeomorphic to image of $I^\collk{F}$}
			\eit

		\eit
	\eit
\eit


\yesnoexec{\showincomplete}{%
\myfoilhead{Direct union and direct summand}%
\index{topological spaces!direct union and direct summand}

\bit
	\item XXX\idxtodo{4 - direct union and direct summand}

	\bit
		\item Royden p 185
	\eit
\eit
}{}


\yesnoexec{\showincomplete}{%
\myfoilhead{Topological and uniform properties}%
\index{topological spaces!topological and uniform properties}

\bit
	\item XXX\idxtodo{3 - topological and uniform properties}

	\bit
		\item Royden p 187
	\eit
\eit
}{}


\yesnoexec{\showincomplete}{%
\myfoilhead{Nets}%
\index{topological spaces!nets}

\bit
	\item XXX\idxtodo{5 - nets}

	\bit
		\item Royden p 188
	\eit
\eit
}{}


\titlefoil{Compact and Locally Compact Spaces}{Compact-and-Locally-Compact-Spaces}

\myfoilhead{Compact spaces}

\bit
	\item compactness for metric spaces (page~\pageref{page:Compact-metric-spaces})
		can be generalized to topological spaces

	\bit
		\item things are very much {similar} to those of metrics spaces
	\eit

	\vitem for subset $K\subset X$, collection of open sets, \openconv, the union of which $K$ is contained in
		called \define{open covering} of $K$

	\vitem topological space, $X$, said to be \define{compact} if every open convering of contains finite subcovering

	\vitem $K\subset X$ said to be \define{compact} if compact as subspace of $X$

	\bit
		\item or equivalently, $K$ is compact if every covering of $K$ \emph{by open sets of $X$}
			has finite subcovering

		\item thus, Heine-Borel (page~\pageref{page:heine-borel-theorem}) says every closed and bounded subset of $\reals$
			is compact
	\eit

	\vitem for $\collk{F}\subset\powerset(X)$ any finite subcollection of which has nonempty intersection
		called \define{finite intersection property}

	\vitem thus, topological space compact \iaoi\ every collection with \emph{finite intersection property}
		has nonempty intersection
\eit


\myfoilhead{Compact spaces - facts}
\pagelabel{page:Compact-spaces---facts}

\bit
	\item \eemph{compactness can be viewed as absolute type of closedness} because
	\bit
		\item closed subset of compact space is compact
		\item compact subset of Hausdorff space is closed%
			\index{compact spaces!Hausdorff spaces}
	\eit
	\item [-] refer to page~\pageref{page:Compact-metric-spaces---other-facts} for exactly the same comments for metric spaces

	\vitem thus, every compact set of $\reals$ is closed and bounded

	\vvitem continuous image of compact set is compact

	\vitem one-to-one continuous mapping of compact space into Hausdorff space is homeomorphism%
		\index{compact spaces!Hausdorff spaces}
\eit


\myfoilhead{Refinement of open covering}

\bit
	\item for open covering of $X$, \openconv, open covering of $X$ every element of which is subset of element of \openconv,
		called \define{refinement} of \openconv\ or said to \define{refine} \openconv\

	\vitem $X$ is cmopact \iaoi\ every open covering has finite refinement

	\vitem any two open covers, \openconv\ and \collk{V}, have common refinement, \ie,

	\begin{eqn}
		\set{U\cap V}{U\in\openconv, V\in\collk{V}}
	\end{eqn}
\eit


\myfoilhead{Countable compactness and Lindel\"of}
\pagelabel{page:Countable-compactness-and-Lindelof}

\bit
	\item topological space for which every open covering has countable subcovering
		said to be \define{Lindel\"of}

	\vitem topological space for which every countable open covering has finite subcovering
		said to be \define{countably compact} space

	\vitem thus, topological space is compact \iaoi\ both Lindel\"of and countably compact

	\vitem every second countable space is Lindel\"of

	\vitem thus, countable compactness coincides with compactness if second countable
		(\ie, satisfying second axiom of countability)%
		\index{axiom of countability!second}

	\vvitem continuous image of compact countably compact space is countably compact
\eit


\myfoilhead{Bolzano-Weierstrass property and sequential compactness}
\pagelabel{page:Bolzano-Weierstrass-property-and-sequential-compactness}

\bit
	\item topological space, $X$, said to have \define{Bolzano-Weierstrass property}
		if every sequence, \seq{x_n}, in $X$ has at least one cluster point,
		\ie,

	\begin{leqn}
		(\forall \seq{x_n})
		(\exists x\in X)
		(\forall \epsilon>0, N\in\naturals)
		(\exists n>N, O\subset X)
		(x\in O, O \mbox{ is open}, x_n \in O)
	\end{leqn}

	\vitem topological space has \emph{Bolzano-Weierstrass properties} \iaoi\ countably compact

	\vitem topological space said to be \define{sequentially compact}
		if every sequence has converging subsequence

	\vitem sequentially compact space is countably compact

	\vitem thus, Lindel\"of coincides with compactness if sequentially compact

	\vvitem countably compact and first countable (\ie, satisfying first axiom of countability) space
		is sequentially compact%
			\index{axiom of countability!first}
\eit


\myfoilhead{Diagrams for relations among topological spaces}%
	\index{topological spaces!diagrams for relations among}%
	\pagelabel{page:Diagrams-for-relations-among-topological-spaces}

\bit
	\item \figref{diagrams for relations among topological spaces}\
		shows relations among topological spaces stated on pages
		\pageref{page:Countable-compactness-and-Lindelof}
		and
		\pageref{page:Bolzano-Weierstrass-property-and-sequential-compactness}
\eit

\vfill
\begin{figure}
\begin{center}
	\mypsfrag{2nd countable}{second countable}
	\mypsfrag{Lindelof}{Lindel\"of}
	\mypsfrag{sequentially compact}{sequentially compact}
	\mypsfrag{BW == CC}{Bolzano-Weierstrass $\Leftrightarrow$ countably compact}
	\mypsfrag{compact}{compact}
		\includegraphics[width=.5\textwidth]{figures/diagrams-topological-spaces}
		\idxfig{diagrams for relations among topological spaces}%
		\label{fig:diagrams for relations among topological spaces}
\end{center}
\end{figure}


\myfoilhead{Real-valued functions on topological spaces}

\bit
	\item continuous real-valued function on countably compact space
		is bounded and assumes maximum and minimum

	\vitem $f:X\to\reals$ with topological space, $X$,
		called \define{upper semicontinuous}
		if $\set{x\in X}{f(x)<\alpha}$ is open for every $\alpha \in \reals$

	\vitem stronger statement -
		upper semicontinuous real-valued function on countably compact space
		is bounded (from above) and assumes maximum


	\vitem \name{Dini -}
		for sequence of upper semicontinuous real-valued functions on countably compact space, \seq{f_n},
		with property that \seq{f_n(x)} decreases monotonically to zero for every $x\in X$,
		\seq{f_n} converges to zero uniformly
\eit


\myfoilhead{Products of compact spaces}%
\index{topological spaces!products of compact spaces}

\bit
	\item \fact{Tychonoff theorem - (probably) most important theorem in general topology}%
		\idximportant{Tychonoff theorem - (probably) most important theorem in general topology}

	\vitem most applications in analysis need only special case of product of (closed) intervals,
		but this special case does not seem to be easire to prove than general case, \ie, Tychonoff theorem

	\vitem lemmas needed to prove Tychonoff theorem
	\bit
		\item for collection of subsets of $X$ with finite intersection property, \collk{A},
			exists collection $\collk{B}\supset\collk{A}$ with finite intersection property
			that is maximal with respect to this property,
			\ie,
			no collection with finite intersection property properly contains \collk{B}\

		\item for collection, \collk{B}, of subsets of $X$
			that is maximal with respect to finite intersection property,
			each intersection of finite number of sets in \collk{B}\ is again in \collk{B}\
			and
			each set that meets each set in \collk{B}\ is itself in \collk{B}\
	\eit

	\vitem \name{Tychonoff theorem -}%
		\index{topological spaces!Tychonoff theorem}%
		\index{Tychonoff, Andrey Nikolayevich!Tychonoff theorem}
		product space $\bigtimes X_\alpha$ is compact
		for indexed family of compact topological spaces, \seq{X_\alpha}\
\eit


\myfoilhead{Locally compact spaces}%
\index{topological spaces!locally compact spaces}%
\index{locally compact spaces}

\bit
	\item topological space, $X$, with
	\[
		(\forall x\in X)(\exists \mbox{ open }O\subset X)(x\in O, \closure{O} \mbox{ is compact})
	\]
	called \define{locally compact}%
	\index{locally compact spaces!local compactness}%
	\index{locally compact spaces}

	\vitem topological space is locally compact\
		\iaoi\
		set of all open sets with compact closures
		forms base for the topological space%
		\idxrevisit{topological space is locally compact\
		\iaoi\
		set of all open sets with compact closures
		forms base for the topological space}

	\vitem every compact space is locally compact
	\bit
		\item but converse it \emph{not} true
		\bit
			\item [-] \eg, Euclidean spaces $\reals^n$ are locally compact,
				but not compact
		\eit
	\eit
\eit


\myfoilhead{Locally compact Hausdorff spaces}%
\index{locally compact spaces!Hausdorff, Felix}%
\index{Hausdorff, Felix!locally compact spaces}
\pagelabel{page:Locally-compact-Hausdorff-spaces}

\bit
	\item \emph{locally compact Hausdorff spaces}%
		\index{locally compact spaces!Hausdorff, Felix}%
		\index{Hausdorff, Felix!locally compact spaces}
		constitute one of most important classes of topological spaces

	\vitem
		so useful is combination of Hausdorff separation axioms in connection with compactness
		that French usage (following Bourbaki) reserves term `compact space'
		for those compact and Hausdorff,
		using term `pseudocompact' for those not Hausdorff!

	\vitem following slides devote to establishing some of their basic properties

%	\vitem behavior of locally compact spaces subject to additional assumptions
%		dealt in other parts (\eg, page~XXX, page~XXX, page~\pageref{title-page:Measure-and-Topology})%
			\idxtodo{CANCELED - 2024 0324 - references to slides dealing with additional locally compact Hausdorff space properties}
\eit


\myfoilhead{Support and subordinateness}

\bit
	\item for function, $f$, on topological spaces,
		closure of \set{x}{f(x)\neq0},
		called \define{support} of $f$,\index{topological spaces!support}
		\ie,
		\[
			\support f = \closure{\set{x}{f(x)\neq0}}
		\]

	\vitem given covering \indexedcol{O_\lambda}\ of $X$,
		collection \indexedcol{\varphi_\alpha}\ with $\varphi_\alpha:X\to\reals$
		satisfying
		\[
			(\forall \varphi_\alpha)(\exists O_\lambda)(\support \varphi_\alpha \subset O_\lambda)
		\]
		said to be \define{subordinate to} \indexedcol{O_\lambda}\index{topological spaces!subordinateness}
\eit



\myfoilhead{Some properties of locally compact Hausdorff spaces}%
\index{locally compact spaces!Hausdorff, Felix}%
\index{Hausdorff, Felix!locally compact spaces}

\bit
	\item for compact subset, $K$, of locally compact Hausdorff space, $X$\
	\bit
		\item exists open subset with compact closure, $O\subset X$, containing $K$

		\item exists continuous nonnegative function, $f$, on $X$,
		with
		\[
			(\forall x\in K)(f(x)=1) \mbox{ and } (\forall x\not\in O)(f(x)=0)
		\]
		if $K$ is $G_\delta$, may take $f<1$ in \compl{K}\
	\eit

	\vitem for open covering, \indexedcol{O_\lambda}, for compact subset, $K$, of locally compact Hausdorff space,
		exists $\seq{\varphi_i}_{i=1}^n \subset C(X,\preals)$ subordinate to \indexedcol{O_\lambda}
		such that
		\[
			(\forall x \in K)(\varphi_1(x)+\cdots+\varphi_n(x) =1)
		\]
\eit


\myfoilhead{Local compactness and second Baire category}%
\index{locally compact spaces!local compactness and second Baire category}%

\bit
	\item
		for locally compact space, $X$,
		and countable collection of dense open subsets, $\seq{O_k}\subset X$,
		the intersection of the collection
		\[
			\bigcap O_k
		\]
		is dense
		\pagelabel{page:locally-compact-space-version-of-Baire-theorem}

	\bit
		\item analogue of Baire theorem for complete metric spaces
			(refer to page~\pageref{page:Baire-theorem} for Baire theorem)
	\eit

	\vitem thus, \emph{every locally compact space is locally of second Baire category with respect to itself}
\eit


\myfoilhead{Local compactness, Hausdorffness, and denseness}%
\index{locally compact spaces!local compactness, Hausdorffness, and denseness}

\bit
	\item for countable union, $\bigcup F_n$, of closed sets containing open subset, $O$, in locally compact space,
		union of interiors, $\bigcup \interior{F_n}$, is
		open set dense in $O$

	\vitem dense subset of Hausdorff space, $X$, which is locally compact in its subspace topology,
		is open subset of $X$

	\vitem subset, $Y$, of locally compact Hausdorff space is locally compact in its subspace topology
		\iaoi\
		$Y$ is relatively open subset of \closure{Y}\
\eit


\myfoilhead{Alexandroff one-point compactification}

\bit
	\item for locally compact Hausdorff space, $X$,
		can form $X^\ast$ by adding single point $\omega\not\in X$ to $X$
		and take set in $X^\ast$ to be open
		if it is either open in $X$ or complement of compact subset in $X$,
		then
	\bit
		\item {$X^\ast$ is compact Hausdorff spaces}
		\item identity mapping of $X$ into $X^\ast$ is homeomorphism of $X$ and $X^\ast\sim\{\omega\}$
		\item $X^\ast$ called \define{Alexandroff one-point compactification of $X$}%
			\index{locally compact spaces!Hausdorff, Felix!Alexandroff one-point compactification}%
			\index{Alexandroff, Paul!Alexandroff one-point compactification}%
			\index{Alexandroff one-point compactification}
		\item $\omega$ often referred to as \define{infinity in $X^\ast$}%
			\index{locally compact spaces!Alexandroff one-point compactification}
	\eit

	\vitem continuous mapping, $f$, from topological space to topological space
		inversely mapping compact set to compact set,
		said to be \define{proper}\index{topological spaces!proper mapping}

	\vitem proper maps from locally compact Hausdorff space into locally compact Hausdorff space
		are precisely those continuous maps of $X$ into $Y$
		tha can be extended to continuous maps $f^\ast$ of $X^\ast$ into $Y^\ast$
		by taking point at infinity in $X^\ast$ to point at infinity in $Y^\ast$%
			\index{locally compact spaces!Hausdorff, Felix!proper map}
\eit


\yesnoexec{\showincomplete}{%
\myfoilhead{$\sigma$-compact spaces}

\bit
	\item XXX - Royden p203\idxtodo{4 - $\sigma$-compact spaces}
\eit
}{}


\myfoilhead{Manifolds}\index{manifolds}

\bit
	\item
		connected Hausdorff space with each point having neighborhood homeomorphic to ball in $\reals^n$
		called $n$-dimensional \define{manifold}%
		\index{manifolds!Hausdorff spaces}

	\vitem sometimes say manifold is connected Hausdorff space that is \emph{locally Euclidean}

	\vitem thus, manifold has all local properties of Euclidean space;
		particularly \emph{locally compact and locally connected}

	\vitem neighborhood homeomorphic to ball called
		\define{coordinate neighborhood} or \define{coordinate ball}

	\vitem pair \pair{U}{\varphi}\ with coordinate ball, $U$, with homeomorphism from $U$ onto ball in $\reals^n$, $\varphi$,
		called \define{coodinate chart};
		$\varphi$ called \define{coordinate map}

	\vitem coordinate (in $\reals^n$) of point, $x\in U$, under $\varphi$
		said to be \define{coordinate of $x$} in the chart
\eit


\myfoilhead{Equivalent properties for manifolds}

\bit
	\item for manifold, $M$, the following are equivalent

	\bit
		\vitem $M$ is paracompact
		\vitem $M$ is $\sigma$-compact
		\vitem $M$ is Lindel\"of
		\vitem every open cover of $M$ has star-finite open refinement
		\vitem exist sequence of open subsets of $M$, \seq{O_n},
			with $\closure{O_n}$ compact,
			$\closure{O_n}\subset O_{n+1}$,
			and
			$M=\bigcup O_n$
		\vitem exists proper continuous map, $\varphi:M\to [0,\infty)$
		\vitem $M$ is second countable
	\eit
\eit
\vfill


\yesnoexec{\showincomplete}{
\myfoilhead{XXX other things about manifolds}

\bit
	\item Royden p 207--208\idxtodo{4 - other things about manifolds}
\eit
}{}


\yesnoexec{\showincomplete}{
\myfoilhead{Stone-\u{C}ech compactification}%
	\index{Stone-\u{C}ech compactification}%
	\index{Stone, Marshall H.!Stone-\u{C}ech compactification}%
	\index{\u{C}ech, Eduard!Stone-\u{C}ech compactification}%

\bit
	\item XXX Royden p209\idxtodo{3 - Stone-\u{C}ech compactification}
\eit
}{}


\yesnoexec{\showincomplete}{
\myfoilhead{Stone-Weierstrass theorem}%
	\index{Stone-Weierstrass theorem}%
	\index{Stone, Marshall H.!Stone-Weierstrass theorem}

\bit
	\item XXX Royden p210\idxtodo{3 - Stone-Weierstrass theorem}
\eit
}{}


\titlefoil{Banach Spaces}{Banach-Spaces}

\myfoilhead{Vector spaces}%
	\index{vector spaces}

\bit
	\item set $X$ with $+:X\times X\to X$, $\cdot: \reals \times X\to X$
		satisfying the following properties
		called \define{vector space} or \define{linear space} or \define{linear vector space} over $\reals$
		\begin{eqnarray*}
		\mbox{- for all } x,y,z\in X \mbox{ and } \lambda, \mu \in \reals
		\\
			x+y= y+x	&& \mbox{- additive commutativity}
			\\
			(x+y)+z= x+(y+z)	&& \mbox{- additive associativity}
			\\
			(\exists 0\in X)\ x+0=x	&& \mbox{- additive identity}
			\\
			\lambda(x+y) = \lambda x + \lambda y	&& \mbox{- distributivity of multiplicative over addition}
			\\
			(\lambda+\mu)x = \lambda x + \mu x	&& \mbox{- distributivity of multiplicative over addition}
			\\
			\lambda(\mu x)= (\lambda \mu)x	&& \mbox{- multiplicative associativity}
			\\
			0\cdot x = 0\in X&&
			\\
			1\cdot x = x&&
		\end{eqnarray*}
\eit


\myfoilhead{Norm and Banach spaces}

\bit
	\item $\|\cdot\|:X\to\preals$ with vector space, $X$, called \define{norm} if

	\vspace{-2em}
	\begin{eqnarray*}
		\mbox{for all } x,y\in X \mbox{ and } \alpha \in \reals&
		\\
		\|x\| = 0 \Leftrightarrow x=0	&& \mbox{- positive definiteness / positiveness /point-separating}
		\\
		\|x+y\|\geq \|x\| + \|y\|	&& \mbox{- triangle inequality / subadditivity}
		\\
		\|\alpha x\| = |\alpha| \|x\|	&& \mbox{- Absolute homogeneity}
	\end{eqnarray*}

	\vspace{-1em}
	\vitem \emph{normed vector space} that is \emph{complete metric space} with metric induced by norm,
		\ie, $\rho:X\times X \to \preals$ with $\rho(x,y)=\|x-y\|$,
		called \define{Banach space}%
			\index{Banach spaces}%
			\index{Banach, Stefan!Banach spaces}%
			\index{complete!Banach spaces}%
			\index{Banach, Stefan!Banach spaces!complete}%
			\pagelabel{page:Banach spaces}

	\bit
		\item can be said to be class of spaces endowed with
			both topological and algebraic structure
	\eit

	\vitem examples include

	\bit
		\item $L^p$ with $1\leq p\leq \infty$ (page~\pageref{page:Banach-space}),
		\item $C(X)=C(X,\reals)$, \ie, space of all continuous real-valued functions on \emph{compact} space, $X$\
	\eit
\eit


\myfoilhead{Properties of vector spaces}

\bit
	\item normed vector space is complete \iaoi\ every absolutely summable sequence is summable
\eit


\myfoilhead{Subspaces of vector spaces}

\bit
	\item nonempty subset, $S$, of vector space, $X$,
		with $x,y\in S\Rightarrow \lambda x + \mu y\in S$,
		called \define{subspace} or \define{linear manifold}

	\vitem intersection of any family of linear manifolds is linear manifold

	\vitem hence, for $A\subset X$,
		exists smallest linear manifold containing $A$,
		often denoted by $\{A\}$\

	\vvitem if $S$ is closed as subset of $X$, called \define{closed linear manifold}

	\vvitem some definitions

	\bit
		\item $A+x$ defined by \set{y+x}{y\in A}, called \define{translate} of $A$ by $x$
		\item $\lambda A$ defined by \set{\lambda x}{x \in A}\
		\item $A+B$ defined by \set{x+y}{x \in A, y\in B}\
	\eit
\eit


\myfoilhead{Linear operators on vector spaces}

\bit
	\item mapping of vector space, $X$, to another (possibly same) vector space
		called
		\define{linear mapping},
		or
		\define{linear operator},
		or
		\define{linear transformation}
		if
		\[
%	\begin{eqn}
			(\forall x,y \in X, \alpha, \beta \in \reals)
			(A(\alpha x + \beta y y) = \alpha (Ax) + \beta (Ay))
%	\end{eqn}
		\]

	\vitem linear operator called \define{bounded}
		if
		\[
%	\begin{eqn}
			(\exists M)
			(\forall x \in X)
			(\|Ax\|\leq M \|x\|)
%	\end{eqn}
		\]

	\vitem least such bound called \define{norm} of linear operator, \ie,
		\[
%	\begin{eqn}
			M
			= \sup_{x\in X, x\neq 0} \|Ax\|/\|x\|
%	\end{eqn}
		\]
	\bit
		\item linearity implies
		\[
			M = \sup_{x\in X, \|x\|= 1} \|Ax\| = \sup_{x\in X, \|x\|\leq 1} \|Ax\|
		\]
	\eit
\eit


\myfoilhead{Isomorphism and isometrical isomorphism\index{isomorphism!vector spaces}\index{vector spaces!isomorphism}}

\bit
	\item bounded linear operator from $X$ to $Y$ called \define{isomorphism}
		if exists bounded inverse linear operator,
		\ie,
		\[
			(\exists A:X\to Y, B:Y\to X)(AB \mbox{ and } BA \mbox{ are identity})
		\]

	\vitem isomorphism between two normed vector spaces that preserve norms
		called \define{isometrical isomorphism}

	\vitem from abstract point of view,
		isometrically isomorphic spaces are \emph{identical},
		\ie,
		{isometrical isomorphism} merely amounts to \emph{element renaming}
\eit


\myfoilhead{Properties of linear operators on vector spaces}
\pagelabel{page:Properties-of-linear-operators}

\bit
	\item for linear operators, point continuity $\Rightarrow$ boundedness $\Rightarrow$ uniform continuity,
		\ie,

	\bit
		\item bounded linear operator is uniformly continuous
		\item linear operator continuous at one point is bounded
	\eit

	\vitem \fact{space of all bounded linear operators from {normed vector space} to {Banach space}
		is {Banach space}}%
		\idximportant{space of all bounded linear operators from {normed vector space} to {Banach space} is {Banach space}}
\eit


\myfoilhead{Linear functionals on vector spaces}

\bit
	\item linear operator from vector space, $X$, to \reals\
		called \define{linear functional},
		\ie, $f:X\to\reals$ such that
		for all $x,y\in X$ and $\alpha, \beta \in \reals$\

	\begin{eqn}
		f(\alpha x + \beta y) = \alpha f(x) + \beta f(y)
	\end{eqn}

	\vitem want to extend linear functional from subspace to whole vector space
		while preserving properties of functional
\eit


\myfoilhead{Hahn-Banach theorem}
\pagelabel{page:Hahn-Banach-theorem}

\bit
	\item \name{Hahn-Banach theorem -}
		for \emph{vector space}, $X$, and linear functional, $p:X \to \reals$ with
	\[
		(\forall x,y\in X, \alpha \geq0)
			(p(x+y)\leq p(x) + p(y) \mbox{ and } p(\alpha x) = \alpha p(x))
	\]
		and for subspace of $X$, $S$, and linear functional, $f:S\to\reals$, with
	\[
		(\forall s \in S)
		(f(s) \leq p(s))
	\]
		exists linear functional, $F:X\to\reals$, such that
	\[
		(\forall s \in S) ( F(s) = f(s))
			\mbox{ and }
		(\forall x \in X) (F(x) \leq p(x))
	\]

	\vitem corollary - for normed vector space, $X$,
		exists bounded linear functional, $f:X\to\reals$\
		\[
			f(x) = \|f\|\|x\|
		\]
\eit


\myfoilhead{Dual spaces of normed spaces}
\pagelabel{page:Dual-of-normed-spaces}

\bit
	\item {space} of \emph{bounded linear functionals} on \emph{normed space}, $X$,
		called \define{dual} or \define{conjugate} of $X$,
		denoted by $X^\ast$%
		\index{dual!normed spaces}%
		\index{conjugate!normed spaces}%
		\index{normed spaces!dual}%
		\index{normed spaces!conjugate}

	\vitem every dual is Banach space (refer to page~\pageref{page:Properties-of-linear-operators})

	\vitem dual of $L^p$ is (isometrically isomorphic to) $L^q$ for $1\leq p<\infty$

	\bit
		\item exists natural representation of bounded linear functional on $L^p$ by $L^q$
			(by Riesz representation theorem on page~\pageref{page:Riesz-representation-theorem})
	\eit

	\vitem \emph{not} every bounded linear functionals on $L^\infty$ has natural representation
	\proofref{l-infinity-not-have-natural-representation}
\eit


\myfoilhead{Natural isomorphism}\index{natural isomorphism!normed spaces}\index{normed spaces!natural isomorphism}
\pagelabel{page:Natural-isomorphism}

\bit
	\item define linear mapping of normed space, $X$, to $X^{\ast\ast}$ (\ie, dual of dual of $X$),
		$\varphi:X\to X^{\ast\ast}$ such that for $x\in X$,
%
%		\begin{eqn}
			$
			(
				\forall f\in X^{\ast}
			)
			(
				(\varphi (x))(f) = f(x)
			)
			$
%		\end{eqn}

	\bit
		\vitem then,
		$
			\|\varphi(x)\|
%			= \sup_{\|g\|=1, g\in X^\ast} (\varphi(x))(g)
			= \sup_{\|g\|=1, g\in X^\ast} g(x)
			\leq \sup_{\|g\|=1, g\in X^\ast} \|g\|\|x\|
			= \|x\|
		$
		\vitem by corollary on page~\pageref{page:Hahn-Banach-theorem}, there exists
			$f\in X^\ast$ such that $f(x)=\|x\|$,
			then $\|f\|=1$, and $f(x)=\|x\|$, thus $\|\varphi(x)\| = \sup_{\|g\|=1, g\in X^\ast} g(x) \geq f(x) = \|x\|$

		\vitem thus, $\|\varphi(x)\| = \|x\|$,
			hence $\varphi$ is isometrically isomorphic linear mapping of $X$ onto $\varphi(X)\subset X^{\ast\ast}$,
			which is subspace of $X^{\ast\ast}$

		\vitem $\varphi$ called \define{natural isomorphism} of $X$ into $X^{\ast\ast}$

		\vitem $X$ said to be \define{reflexive} if $\varphi(X)=X^{\ast\ast}$
	\eit

	\vitem thus, $L^p$ with $1< p<\infty$ is reflexive, but $L^1$ and $L^\infty$ are not
	\vvitem note $X$ may be isometric with $X^{\ast\ast}$ without reflexive
\eit


\myfoilhead{Completeness of natural isomorphism}\index{normed spaces!natural isomorphism!completeness}%
\idximportant{every normed vector space is isometrically isomorphic to dense subset of Banach spaces}
\pagelabel{page:Completeness-of-natural-isomorphism}

\bit
	\item for natural isomorphism, $\varphi$

	\vitem $X^{\ast\ast}$ is complete, hence Banach space
	\bit
		\item because bounded linear functional to \reals\ (refer to page~\pageref{page:Properties-of-linear-operators})
	\eit

	\vitem thus, closure of $\varphi(X)$ in $X^{\ast\ast}$, \closure{\varphi(X)}, complete
		(refer to page~\pageref{page:subspaces})

	\vitem therefore, \fact{every normed vector space ($X$)
		is isometrically isomorphic to dense subset of Banach spaces ($X^{\ast\ast}$)}
\eit


\myfoilhead{Hahn-Banach theorem - complex version}

\bit
	\item \name{Bohnenblust and Sobczyk -}
		for \emph{complex} vector space, $X$, and linear functional, $p:X \to \reals$ with
\[
%	\begin{eqn}
		(
			\forall x,y\in X, \alpha \in\complexes
		)
		(
			p(x+y)\leq p(x) + p(y) \mbox{ and } p(\alpha x) = |\alpha| p(x)
		)
%	\end{eqn}
\]

		and for subspace of $X$, $S$, and (complex) linear functional, $f:S\to\complexes$, with
		\[
%	\begin{eqn}
		(
			\forall s \in S
		)
		(
			|f(s)| \leq p(s)
		)
%	\end{eqn}
\]

		exists linear functional, $F:X\to\reals$, such that
		\[
%	\begin{eqn}
		(
			\forall s \in S
		)
		(
			F(s) = f(s)
		)
%	\end{eqn}
\]
		and
		\[
%	\begin{eqn}
		(
			\forall x \in X
		)
		(
			|F(x)| \leq p(x)
		)
%	\end{eqn}
\]
\eit


\myfoilhead{Open mapping on topological spaces}

\bit
	\item mapping from topological space to another topological space
		the image of each open set by which is open
		called \define{open mapping}

	\vitem hence, one-to-one continuous open mapping is \emph{homeomorphism}

	\vitem (will show)
		continuous linear transformation of Banach space onto another Banach space
		is
		always open mapping

	\vitem (will)
		use above to provide criteria
		for continuity of linear transformation
\eit


\myfoilhead{Closed graph theorem (on Banach spaces)}

\bit
\iffalse
	\item for every continuous linear transformation, $A$, of Banach space, $X$, onto Banach space, $Y$
%
%		\begin{eqn}
		\[
			(
				\exists \epsilon > 0
			)
			(
				\forall x \in X
			)
			(
				\|x\|\leq 1 \Rightarrow \|Ax\|\geq \epsilon
			)
		\]
%		\end{eqn}
%
		\bit
			\vitem \ie, image of unit sphere about origin in $X$ contains \emph{a} sphere about origin in $Y$
		\eit

	\vitem
\fi
	\item every continuous linear transformation of Banach space onto Banach space is open mapping
	\bit
		\item in particular, if the mapping is one-to-one, it is isomorphism%
			\index{Banach spaces!isomorphism}%
			\index{Banach, Stefan!Banach spaces!isomorphism}
	\eit

	\vitem for linear vector space, $X$, complete in two norms, $\|\cdot\|_A$ and $\|\cdot\|_B$,
		with $C\in\reals$ such that
%		\[
		$
			(\forall x\in X)(\|x\|_A \leq C \|x\|_B)
		$,
%		\]
		two norms are equivalent, \ie,
		$
			(\exists C'\in\reals)(\forall x\in X)(\|x\|_B \leq C' \|x\|_A)
		$

	\vvitem \name{closed graph theorem -} linear transformation, $A$, from Banach space, $A$, to Banach space, $B$,
		with property that
		``if \seq{x_n}\ converges in $X$ to $x\in X$ and \seq{Ax_n}\ converges in $Y$ to $y\in Y$,
		then $y=Ax$''
		is continuous
	\bit
		\vitem equivalent to say, if graph $\set{(x,Ax)}{x\in X}\subset X\times Y$ is closed,
			$A$ is continuous
	\eit
\eit


\myfoilhead{Principle of uniform boundedness (on Banach spaces)}

\bit
	\item \name{principle of uniform boundedness - }
		for family of bounded linear operators, \collk{F}\ from Banach space, $X$, to normed space, $Y$,
		with

		\[
			(
				\forall x \in X
			)
			(
				\exists M_x
			)
			(
				\forall T \in \collk{F}
			)
			(
				\|Tx\| \leq M_x
			)
		\]

		\vfill
		then operators in \collk{F}\ is uniformly bounded,\
		\ie,

		\[
			(
				\exists M
			)
			(
				\forall T \in \collk{F}
			)
			(
				\|T\| \leq M
			)
		\]
\eit
\vfill
\vfill
\vfill


\myfoilhead{Topological vector spaces}
\index{topological vector spaces}

\bit
	\item just as notion of metric spaces generalized to notion of topological spaces

	\vitem \emph{notion of normed linear space generalized to notion of topological vector spaces}

	\vitem linear vector space, $X$, with topology, \tJ, equipped with
		continuous addition, $+:X\times X\to X$
		and
		continuous multiplication by scalars, $+:\reals\times X\to X$,
		called \define{topological vector space}
\eit


\myfoilhead{Translation invariance of topological vector spaces}

\bit
	\item for topological vector space,
		translation by $x\in X$ is homeomorphism (due to continuity of addition)
	\bit
		\item hence, $x+O$ of open set $O$ is open
		\vitem every topology with this property said to be \define{translation invariant}
	\eit

	\vitem for translation invariant topology, \tJ, on $X$,
		and
		base, \collB, for \tJ\ at $0$,
		set
		\[
			\set{x+U}{U\in \collB}
		\]
		forms \emph{a base} for \tJ\ at $x$

	\vitem hence, sufficient to give a base at $0$
		to determine \emph{translation invariance of topology}

	\vitem base at $0$ often called \define{local base}
\eit


\myfoilhead{Sufficient and necessarily condition for topological vector spaces}
\pagelabel{page:Sufficient-and-necessarily-condition-for-topological-vector-spaces}
\index{topological vector spaces!sufficient and necessarily condition}

\bit
	\item for topological vector space, $X$,
		can find base, \collB, satisfying following properties
%	\begin{eqnarray}

	\begin{eqna}
%	\label{eqn:conditions-translation-invariance}
		&&
%		\nonumber
		(\forall U, V \in \collB)(\exists W\in \collB)(W\subset U\cap V)
		\\
		&&
%		\nonumber
		(\forall U \in \collB, x\in U)(\exists V\in \collB)(x+V\subset U)
		\\
		&&
%		\nonumber
		(\forall U \in \collB)(\exists V\in \collB)(V + V \subset U)
		\\
		&&
%		\nonumber
		(\forall U \in \collB, x\in X)(\exists \alpha\in \reals)(x\in \alpha U)
		\\
		&&
%		\nonumber
		(\forall U \in \collB, 0<|\alpha|\leq 1\in \reals)(\alpha U\subset U, \alpha U\subset \collB)
	\end{eqna}
%	\end{eqnarray}

	\vitem conversely, for collection, \collB, of subsets containing $0$
		satisfying above properties,
		exists topology for $X$ making $X$ \emph{topological vector space}
		with \collB\ as base at $0$

	\bit
		\vitem this topology is Hausdorff \iaoi\
		\[
			\bigcap\{U\in \collB\} = \{0\}
		\]
	\eit

	\item for normed linear space,
		can take \collB\ to be set of spheres centered at $0$,
		then \collB\ satisfies above properties,
		hence can form \emph{topological vector space}
\eit


\myfoilhead{Topological isomorphism}\index{topological vector spaces!isomorphism}\index{isomorphism!topological vector spaces}
\pagelabel{page:Topological-isomorphism}

\bit
	\item in topological vector space,
		can compare neighborhoods at one point
		with neighborhoods of another point
		by translation

	\vitem for mapping, $f$,
		from topological vector space, $X$,
		to topological vector space, $Y$,
		such that

		\begin{eqna}
%		\begin{eqnarray*}
			&&
			(\forall \mbox{ open } O\subset Y \mbox{ with }0\in O)
			(\exists \mbox{ open } U\subset X \mbox{ with }0\in U)
			\\
			&&
			(\forall x\in X)
			(f(x+U) \subset f(x) + O)
%		\end{eqnarray*}
		\end{eqna}

		said to be \define{uniformly continuous}

	\vitem linear transformation, $f$, is uniformly continuous
		if continuous at one point

	\vitem continuous one-to-one mapping, $\varphi$, from $X$ onto $Y$ with continuous $\varphi^{-1}$
		called \define{(topological) isomorphism}\index{topological vector spaces!isomorphism}\index{isomorphism!topological vector spaces}
	\bit
		\item in abstract point of view, isomorphic spaces are \emph{same}
	\eit

	\vitem \name{Tychonoff -}
		finite-dimensional Hausdorff topological vector space
		is topologically isomorphic
		to $\reals^n$ for some $n$%
		\idximportant{Tychonoff - finite-dimensional Hausdorff topological vector space is topologically isomorphic to $\reals^n$ for some $n$}
\eit


\myfoilhead{Weak topologies}%
	\index{topological vector spaces!weak topologies}%
	\index{Banach spaces!weak topologies}
	\index{Banach, Stefan!Banach spaces!weak topologies}

\bit
	\item for vector space, $X$, and collection of linear functionals, \collF,
		weakest topology generated by \collF,
		\ie, in way that each functional in \collF\ is continuous in that topology,
		called \define{weak topology generated by} \collF\
	\bit
		\item translation invariant
		\item base at $0$ given by sets

		\begin{eqn}
%		\[
			\set{x\in X}{\forall f \in\collk{G}, |f(x)|<\epsilon}
%		\]
		\end{eqn}

		for all finite $\collk{G}\subset\collF$ and $\epsilon>0$

		\item basis satisfies properties on page~\pageref{page:Sufficient-and-necessarily-condition-for-topological-vector-spaces},
			hence, (above) weak topology makes \emph{topological vector space}
	\eit

	\vitem for \emph{normed} vector space, $X$, and collection of continuous functionals, \collF,
		\ie, $\collF\subset X^\ast$,
		weak topology generated by \collF\
		\emph{weaker than} (fewer open sets) norm topology of $X$\

	\vvitem metric topology generated by norm called \define{strong topology of $X$}
	\vitem weak topology generated by $X^\ast$ called \define{weak topology of $X$}
\eit


\myfoilhead{Strongly and weakly open and closed sets}
\index{topological vector spaces!strongly and weakly open and closed sets}

\bit

	\item open and closed sets of strong topology called \define{strongly open} and \define{strongly closed}
	\vitem open and closed sets of weak topology called \define{weakly open} and \define{weakly closed}

	\vvitem wealy closed set is strongly closed, but converse not true
	\vitem however, these coincides for linear manifold,
		\ie, linear manifold is weakly closed \iaoi\ strongly closed

	\vvitem every strongly converent sequence (or net) is weakly convergent
\eit


\myfoilhead{Weak$^\ast$ topologies}
\index{topological vector spaces!weak$^\ast$ topologies}

\bit
	\item for normed space,
		\define{weak topology of $X^\ast$}
		is weakest topology for which
		all functionals in $X^{\ast\ast}$ are continuous

	\vitem turns out that {weak topology of $X^\ast$}
		is less useful than weak topology generated by $X$,
		\ie, that generated by $\varphi(X)$
		where $\varphi$ is the natural embedding of $X$ into $X^{\ast\ast}$
		(refer to page~\pageref{page:Natural-isomorphism})

	\vitem (above) weak topology generated by $\varphi(X)$
		called \define{weak$^\ast$ topology for $X^\ast$}
	\bit
		\item even \emph{weaker than} weak topology of $X^\ast$
		\vitem thus, weak$^\ast$ closed subset of is weakly closed,
			and weak convergence implies weak$^\ast$ convergence
	\eit

	\vitem base at $0$ for weak$^\ast$ topology given by sets

	\begin{eqn}
%	\[
		\set{f}{\forall x\in A, |f(x)|<\epsilon}
%	\]
	\end{eqn}

	for all finite $A\subset X$ and $\epsilon>0$

	\vitem \emph{when $X$ is reflexive, weak and weak$^\ast$ topologies coincide}

	\vitem \name{Alaoglu -} unit ball $S^\ast = \set{f\in X^\ast}{\|f\|\geq1}$
		is compact in weak$^\ast$ topology


\eit


\myfoilhead{Convex sets}
\pagelabel{page:Convex sets}

\bit
	\item for vector space, $X$ and $x,y\in X$

	\begin{eqn}
%	\[
		\set{\lambda x + (1-\lambda)y}{\lambda \in [0,1]} \subset X
%	\]
	\end{eqn}

	called \define{segmenet joining $x$ and $y$}\index{convex!sets!segmenet}

	\vitem set $K\subset X$ said to be \define{convex} or \define{convex set}\index{convex sets}\
		if every segment joining any two points in $K$ is in $K$, \ie,
%
%	\begin{eqn}
%	\[
	$
		(\forall x,y\in K)(\mbox{segment joining }x,y\subset X)
	$
%	\]
%	\end{eqn}

	\vitem every $\lambda x + (1-\lambda)y$ for $0<\lambda<1$ called \define{interior point of segment}\index{convex sets!interior point of segment}

	\vitem point in $K\subset X$ where intersection with $K$
		of every line going through $x$
		contains open interval about $x$,
		said to be \define{internal point}\index{convex sets!internal point},
		\ie,

	\begin{eqn}
%	\[
		(\exists \epsilon>0)(\forall y\in K, |\lambda|<\epsilon)(x+y x\in K)
%	\]
	\end{eqn}

	\vvitem convex set examples -
		linear manifold \ \&\ ball, {ellipsoid} in normed space

\eit


\myfoilhead{Properties of convex sets}

\bit
	\item for convex sets, $K_1$ and $K_2$, following are also convex sets
	\[
		K_1 \cap K_2,\ \lambda K_1,\ K_1 + K_2
	\]

	\vitem for linear operators from vector space, $X$, and vector space, $Y$,
	\bit
		\item image of convex set (or linear manifold) in $X$
		is convex set (or linear manifold) in $Y$,
%		and
		\item
		inverse image of convex set (or linear manifold) in $Y$
		is convex set (or linear manifold) in $X$
	\eit

	\vitem closure of convex set in topological vector space is convex set
\eit


\myfoilhead{Support functions of and separated convex sets}

\bit
	\item for subset $K$ of vector space $X$,
		$p:K\to \preals$ with $p(x) = \inf{\lambda|\lambda^{-1}x \in K, \lambda>0}$
		called \define{support functions}\index{convex sets!support functions}

	\vitem for convex set $K\subset X$ containing $0$ as internal point
	\bit
		\item $(\forall x\in X,\lambda\geq0)(p(\lambda x) = \lambda p(x))$
		\item $(\forall x,y\in X)(p(x+y)\leq p(x)+p(y))$
		\item $\set{x\in X}{p(x) < 1} \subset K \subset \set{x\in X}{p(x)\leq 1}$
	\eit

	\vitem two convex sets, $K_1$ and $K_2$
		such that exists linear functional, $f$, and $\alpha\in\reals$
		with
		$(\forall x\in K_1)(f(x) \leq \alpha)$
		and
		$(\forall x\in K_2)(f(x) \geq \alpha)$,
		said to be \define{separated}\index{convex sets!separated convex sets}

	\vitem for two disjoint convex sets in vector space
		with at least one of them having internal point,
		exists \emph{nonzero linear functional} that separates two sets
\eit


\myfoilhead{Local convexity}

\bit
	\item topological vector space with base for topology consisting of convest sets,
		said to be \define{locally convex}\index{convex sets!local convexity}

	\vitem for family of convex sets, $\collk{N}$, in vector space,
		following conditions are sufficient
		for being able to translate sets in $\collk{N}$
		to form base for topology
		to make topological space into locally convex topological vector space

	\begin{eqna}
%	\begin{eqnarray*}
%		&&
		&
			(\forall N\in\collk{N})(x\in N \Rightarrow x \mbox{ is internal})
		&
		\\
%		&&
		&
			(\forall N_1, N_2\in\collk{N})(\exists N_3\in\collk{N})(N_3 \subset N_1 \cap N_2)
		&
		\\
%		&&
		&
			(\forall N \in\collk{N}, \alpha\in\reals \mbox{ with } 0<|\alpha|<1)(\alpha N \in \collk{N})
		&
%	\end{eqnarray*}
	\end{eqna}

	\vitem conversely, for every locally convex topological vector space,
		exists base at $0$ satisfying above conditions

	\vvitem follows that

	\bit
		\item weak topology on vector space generated by linear functionals
			is locally convex

		\item normed vector space is locally convex topological vector space
	\eit
\eit


\myfoilhead{Facts regarding local convexity}\index{convex sets!local convexity}

\bit
	\item for locally convex topological vector space
		closed convex subset, $F$,
		with point, $x$, not in $F$,
		exists continuous linear functional, $f$,
		such that
		\[
			f(x) < \inf_{y\in F} f(y)
		\]

	\vitem corollaries
	\bit
		\vitem convex set in locally convex topological vector space
			is strongly closed \iaoi\ weakly closed

		\vitem for distinct points, $x$ and $y$,
			in locally convex Hausdorff vector space,
			exists continuous linear functional, $f$,
			such that $f(x)\neq f(y)$
	\eit
\eit


\myfoilhead{Extreme points and supporting sets of convex sets}

\bit
	\item point in convex set in vector space
		that is not interior point of any line segment lying in the set,
		called \define{extreme point}\index{convex sets!extreme point}

	\vitem thus,
		$x$ is extreme point of convex set, $K$, \iaoi\
		$x=\lambda y + (1-\lambda) z$ with $0<\lambda<1$ implies $y\not\in K$ or $z\not\in K$

	\vvitem closed and convex subset, $S$, of convex set, $K$,
		with property that
		for every interior point of line segment in $K$ belonging to $S$,
		entire line segment belongs to $S$,
		called \define{supporting set of $K$}\index{convex sets!supporting sets}

	\vitem for closed and convex set, $K$,
		set of points \emph{a} continuous linear functional assumes maximum on $K$,
		is \emph{supporting set of $K$}
\eit


\myfoilhead{Convex hull and convex convex hull}

\bit
	\item for set $E$ in vector space,
		intersection of all convex sets containing set, $E$,
		called \define{convex hull of $E$}\index{convex sets!convex hull},
		which is convex set

	\vitem for set $E$ in vector space,
		intersection of all closed convex sets containing set, $E$,
		called \define{closed convex hull of $E$}\index{convex sets!closed convex hull},
		which is closed convex set

	\vitem \name{Krein-Milman theorem -}%
			\index{Krein-Milman theorem}%
			\index{Krein, Mark Grigorievich!Krein-Milman theorem}%
			\index{Milman, David Pinhusovich!Krein-Milman theorem}
		compact convex set
		in
		locally convex topologically vector space
		is \emph{closed convex hull of its extreme points}
\eit


\myfoilhead{Hilbert spaces}%
	\index{Hilbert spaces}%
	\index{Hilbert, David!Hilbert spaces}

\bit
	\item Banach space, $H$, with function $\innerp{\cdot}{\cdot}:H\times H\to\reals$ satisfying following properties,
		called \define{Hilbert space}%
			\index{Hilbert, David!Hilbert spaces}

		\begin{eqna}
%		\begin{eqnarray*}
		&&(\forall x,y,z\in H, \alpha, \beta \in \reals)(\innerp{\alpha x + \beta y}{z}=\alpha\innerp{x}{z} + \beta\innerp{y}{z})
		\\
		&&(\forall x,y\in H)(\innerp{x}{y} = \innerp{y}{z})
		\\
		&&(\forall x\in H)(\innerp{x}{x} = \|x\|^2)
%		\end{eqnarray*}
		\end{eqna}

	\vitem \innerp{x}{y}\ called \define{inner product}%
			\index{Hilbert spaces!inner product}%
			\index{Hilbert, David!Hilbert spaces}%
			\index{inner product!Hilbert spaces}
		for $x,y\in H$
	\bit
		\item examples -
		$\innerp{x}{y} = x^T y = \sum x_i y_i$ for $\reals^n$,
		$\innerp{x}{y} = \int x(t)y(t) dt$ for $L^2$
	\eit

	\vvitem \name{Schwarz or Cauchy-Schwarz or Cauchy-Buniakowsky-Schwarz inequality -}%
		\index{Hilbert spaces!Schwarz inequality}%
		\index{Hilbert, David!Hilbert spaces}%
		\index{Schwarz, Hermann!Schwarz inequality!Hilbert spaces}%
		\index{Hilbert spaces!Cauchy-Schwarz inequality}%
		\index{Cauchy, Augustin-Louis!Cauchy-Schwarz inequality!Hilbert spaces}%
		\index{Schwarz, Hermann!Cauchy-Schwarz inequality!Hilbert spaces}%
		\index{Hilbert spaces!Cauchy-Buniakowsky-Schwarz inequality}%
		\index{Cauchy, Augustin-Louis!Cauchy-Buniakowsky-Schwarz inequality!Hilbert spaces}%
		\index{Schwarz, Hermann!Cauchy-Buniakowsky-Schwarz inequality!Hilbert spaces}%
		\index{Cauchy-Schwarz inequality!Hilbert spaces}
		\index{Schwarz, Hermann!Cauchy-Schwarz inequality!Hilbert spaces}
		\index{Cauchy, Augustin-Louis!Cauchy-Schwarz inequality!Hilbert spaces}
		\index{Schwarz, Hermann!Cauchy-Schwarz inequality!Hilbert spaces}

	\begin{eqn}
%	\[
		\|x\|\|y\| \geq \innerp{x}{y}
%	\]
	\end{eqn}

	\bit
		\vitem hence,
		\bit
			\item [-] linear functional defined by $f(x)=\innerp{x}{y}$ bounded by $\|y\|$
			\item [-] \innerp{x}{y}\ is continuous function from $H\times H$ to \reals\
		\eit
	\eit
\eit


\myfoilhead{Inner product in Hilbert spaces}%
	\index{Hilbert spaces!inner product}%
	\index{Hilbert, David!Hilbert spaces}%
	\index{inner product!Hilbert spaces}

\bit
	\item $x$ and $y$ in $H$ with $\innerp{x}{y}=0$ said to be \define{orthogonal}%
			\index{orthogonality!Hilbert spaces}%
			\index{Hilbert spaces!orthogonality}%
			\index{Hilbert, David!Hilbert spaces}
		denoted by $x\perp y$

	\vitem set $S$ of which any two elements orthogonal
		called \define{orthogonal system}%
			\index{Hilbert spaces!orthogonal system}%
			\index{Hilbert, David!Hilbert spaces}

	\vitem orthogonal system called \define{orthonormal} if every element has unit norm%
		\index{orthonormality!Hilbert spaces}%
		\index{Hilbert spaces!orthonormality}%
		\index{Hilbert, David!Hilbert spaces}

	\vitem any two elements are $\sqrt{2}$ apart,
		hence \emph{if $H$ separable,
		every orthonormal system in $H$
		must be countable}%
			\index{Hilbert spaces!separable Hilbert space}%
			\index{Hilbert spaces!separable Hilbert space}%
			\index{Hilbert, David!Hilbert spaces}

	\vitem shall deal only with \emph{separable Hilbert spaces}
\eit


\myfoilhead{Fourier coefficients}

\bit
	\item assume orthonormal system expressed as sequence, \seq{\varphi_n}\ - may be finite or infinite

	\vitem for $x\in H$ \[a_n = \innerp{x}{\varphi_n}\]
		called \define{Fourier coefficients}%
			\index{Hilbert spaces!Fourier coefficients}%
			\index{Hilbert, David!Hilbert spaces}%
			\index{Fourier coefficients!Hilbert spaces}%
			\index{Fourier, Jean-Baptiste Joseph!Fourier coefficients}

	\vitem for $n\in\naturals$, we have
	\[
		\|x\|^2 \geq \sum^n_{i=1} a_i^2
	\]
	\begin{proof}
	\begin{eqnarray*}
		\lefteqn{
		\left\| x-\sum_{i=1}^n a_i \varphi_i \right\|^2
%		&=&
		=
		\innerpt{x-\sum a_i \varphi_i}{x-\sum a_i \varphi_i}{}
		}
		\\
		&=&
		\innerpt{x}{x}
		- 2 \innerpt{x}{\sum a_i \varphi_i}{}
		+ \innerpt{\sum a_i \varphi_i}{\sum a_i \varphi_i}{}
		\\
		&=&
		\|x\|^2
		- 2 \sum a_i \innerpt{x}{\varphi_i}
		+ \sum a_i^2 \|\varphi_i\|^2
		=
		\|x\|^2 - \sum a_i^2
		\geq 0
	\end{eqnarray*}
	\end{proof}

\eit


\myfoilhead{Fourier coefficients of limit of $x$}

\bit
	\item \define{Bessel's inequality -}%
		\index{Hilbert spaces!Bessel's inequality}%
		\index{Hilbert, David!Hilbert spaces}
	for $x\in H$, its Fourier coefficients, \seq{a_n}

	\begin{eqn}
%	\[
		\sum_{n=1}^\infty a_n^2 \leq \|x\|^2
%	\]
	\end{eqn}

	\vitem then, \seq{z_n} defined by following is \emph{Cauchy sequence}
	$
		z_n = \sum_{i=1}^n a_i \varphi_i
	$

	\vitem completeness (of Hilbert space) implies \seq{z_n}\ converges
		- let $y=\lim z_n$

	\begin{eqn}
%	\[
		y=\lim z_n = \sum_{i=1}^\infty a_i \varphi_i
%	\]
	\end{eqn}

	\vitem continuity of inner product implies $\innerp{y}{\varphi_n} = \lim (z_n,\varphi_n) = a_n$,
		\ie, Fourier coefficients of $y\in H$ are $a_n$, \ie,

	\vitem \emph{$y$ has same Fourier coefficients as $x$}
\eit


\myfoilhead{Complete orthonormal system}%
	\index{Hilbert spaces!orthonormal system!completeness}%
	\index{Hilbert, David!Hilbert spaces}%
	\index{Hilbert spaces!complete orthonormal system}%
	\index{Hilbert, David!Hilbert spaces}

\bit
	\item orthonormal system, $\seq{\varphi_n}_{n=1}^\infty$, of Hilbert spaces, $H$, is said to be \define{complete}%
		\index{complete!orthonormal system}
		if
%
%		\begin{eqn}
		\[
			(\forall x\in H, n\in\naturals)(\innerp{x}{\varphi_n}=0)
			\Rightarrow
			x=0
		\]
%		\end{eqn}

	\vitem orthonormal system is complete \iaoi\ maximal, \ie,%
		\index{Hilbert spaces!orthonormal system!completeness}%
		\index{Hilbert, David!Hilbert spaces}%
		\index{Hilbert spaces!complete orthonormal system}%
		\index{Hilbert, David!Hilbert spaces}
		\[
			\seq{\varphi_n} \mbox{ is complete}
			\Leftrightarrow
			(
			(\exists \mbox{ orthonormal }R\subset H)(\forall n\in\naturals)(\varphi_n \in R)
			\Rightarrow
			R = \seq{\varphi_n}
			)
		\]
	\item [] \proofref{orthonormal-system}

	\vvitem Hausdorff maximal principle (\principlename~\ref{principle:Hausdorff maximal principle})
		implies existence of maximal orthonormal system,
		hence following statement

	\vitem for separable Hilbert space, $H$,
		every orthonormal system is separable
		and exists \emph{a} complete orthonormal system.
		any such system, \seq{\varphi_n}, and $x\in H$
%
%		\begin{eqn}
		\[
			x = \sum a_n \varphi_n
		\]
%		\end{eqn}
%
		with $a_n = \innerp{x}{\varphi_n}$,
		and $\|x\| = \sum a_n^2$
\eit


\myfoilhead{Dimensions of Hilbert spaces}%
	\index{Hilbert spaces!dimension}%
	\index{Hilbert, David!Hilbert spaces}

\bit
	\item every complete orthonormal system of separable Hilbert space
		has same number of elements, \ie, has same cardinality

	\vitem hence, every complete orthonormal system
		has
		either finite or countably infinite complete orthonormal system

	\vitem this number called \define{dimension of separable Hilbert space}%
		\index{Hilbert spaces!dimension}%
		\index{Hilbert, David!Hilbert spaces}

	\bit
		\item for Hilbert space with countably infinite complete orthonormal system,
			we say, $\dim H = \aleph_0$
	\eit
\eit


\myfoilhead{Isomorphism and isometry between Hilbert spaces}%
	\index{Hilbert spaces!isomorphism}%
	\index{Hilbert, David!Hilbert spaces}%
	\index{Hilbert spaces!isometry}%
	\index{Hilbert, David!Hilbert spaces}

\bit
	\item \define{isomorphism, $\Phi$, of Hilbert space onto another Hilbert space}
		is linear mapping with property, $\innerp{\Phi x}{\Phi y} = \innerp{x}{y}$

	\vitem hence, every \emph{isomorphism between Hilbert spaces is isometry}

	\vvitem every $n$-dimensional Hilbert space is isomorphic to $\reals^n$

	\vitem every $\aleph_0$-dimensional Hilbert space is isomorphic to $l^2$,
		which again is isomorphic to $L^2$

	\vitem $L^2[0,1]$ is separable and \seq{\cos (n\pi t)}\ is infinite orthogonal system

	\vvitem every bounded linear functional, $f$, on Hilbert space, $H$,
		has unique $y$ such that
	\[
		(\forall x\in H)(f(x)=\innerp{x}{y})
	\]
	and $\|f\|=\|y\|$
\eit
}{}


\yesnoexec{\absmeas}{
\titlefoil{Measure and Integration}{Measure-and-Integration}


\myfoilhead{Purpose of integration theory}

\bit
	\item purpose of ``measure and integration'' slides
	\bit
		\item abstract (out) most important properties of Lebesgue measure and Lebesgue integration
	\eit

	\vitem provide certain \emph{axioms that Lebesgue measure satisfies}

	\vitem base our integration theory on these axioms

	\vitem hence, our theory valid for every system satisfying the axioms
\eit


\myfoilhead{Measurable space, measure, and measure space}

\bit
	\item family of subsets containing $\emptyset$
		closed under countable union and completement,
		called \define{$\sigma$-algebra}\index{$\sigma$-algebra}

	\vitem mapping of sets to extended real numbers,
		called \define{set function}
%		\[
%			\mu: \mbox{set of subsets} \to [-\infty, \infty]
%		\]

	\vitem \measu{X}{\algk{B}}\ with set, $X$, and $\sigma$-algebra of $X$, \algk{B},
		called \define{measurable space}%
			\index{measure!measurable spaces}%
			\index{measurable spaces}%
			\pagelabel{page:measure!measurable spaces}
	\bit
		\item $A\in\algk{B}$, said to be \define{measurable (with respect to \algk{B})}%
			\index{measurable sets}
	\eit

	\vitem nonnegative set function, $\mu$, defined on \algk{B}\ satisfying
		$\mu(\emptyset)=0$ and for every disjoint, $\seq{E_n}_{n=1}^\infty\subset \algk{B}$,
		\[
			\mu\left(\bigcup E_n\right) = \sum \mu E_n
		\]
		called \define{measure on} measurable space, \measu{X}{\algk{B}}%
			\index{measure}%
			\index{measure!measure}%
			\index{measure!countable additivity}

	\vitem measurable space, \measu{X}{\algk{B}}, equipped with measure, $\mu$,
		called \define{measure space} and denoted by \meas{X}{\algk{B}}{\mu}%
			\index{measure spaces}
\eit


\myfoilhead{Measure space examples}%
\index{measure spaces!examples}

\bit
	\item \meas{\reals}{\subsetset{M}}{\mu}\
		with Lebesgue measurable sets, \subsetset{M}, and Lebesgue measure, $\mu$\

	\vitem \meast{[0,1]}{\set{A\in\subsetset{M}}{A\subset[0,1]}}{\mu}\
		with Lebesgue measurable sets, \subsetset{M}, and Lebesgue measure, $\mu$\

	\vitem \meas{\reals}{\algB}{\mu}\
		with class of Borel sets, \algB, and Lebesgue measure, $\mu$\

	\vitem \meas{\reals}{\powerset(\reals)}{\mu_C}\
		with set of all subsets of \reals, $\powerset(\reals)$, and counting measure, $\mu_C$\

	\vitem interesting (and bizarre) example
		\pagelabel{page:interesting-(and-bizarre)-example}
	\bit
		\item \meas{X}{\collk{A}}{\mu_B}\
			with any uncountable set, $X$,
			family of either countable or complement of countable set, \collk{A},
			and measure, $\mu_B$, such that $\mu_B A =0$ for countable $A\subset X$
			and $\mu_B B=1$ for uncountable $B\subset X$
	\eit
\eit


\myfoilhead{More properties of measures}%

\bit
	\item for $A,B\in\algB$ with $A\subset B$
	\[
		\mu A \leq \mu B
	\]

	\vitem for $\seq{E_n}\subset \algB$ with $\mu E_1 < \infty$ and $E_{n+1} \subset E_n$
	\[
		\mu\left(\bigcap E_n\right) = \lim \mu E_n
	\]

	\vitem for $\seq{E_n}\subset \algB$
	\[
		\mu\left(\bigcup E_n\right) \leq \sum \mu E_n
	\]
\eit


\myfoilhead{Finite and $\sigma$-finite measures}

\bit
	\item measure, $\mu$, with $\mu(X)<\infty$,
		called \define{finite}\index{measure!finite}

	\vitem measure, $\mu$, with $X=\bigcup X_n$ for some \seq{X_n} and $\mu(X_n)<\infty$,
		called \define{$\sigma$-finite}\index{measure!$\sigma$-finite}
	\bit
		\item always can take \seq{X_n}\ with disjoint $X_n$
	\eit

	\vvitem Lebesgue measure on $[0,1]$ is finite

	\vitem Lebesgue measure on \reals\ is $\sigma$-finite

	\vvitem countering measure on uncountable set is \emph{not} $\sigma$-measure
\eit
\vfill


\myfoilhead{Sets of finite and $\sigma$-finite measure}

\bit
	\item set, $E\in \algB$, with $\mu E<\infty$,
		said to be \define{of finite measure}\index{measure!sets of finite measure}

	\vitem set that is countable union of measurable sets of finite measure,
		said to be \define{of $\sigma$-finite measure}\index{measure!sets of $\sigma$-finite measure}

	\vvitem measurable set contained in set of $\sigma$-finite measure,
		is of $\sigma$-finite measure

	\vitem countable union of sets of $\sigma$-finite measure,
		is of $\sigma$-finite measure

	\vvitem when $\mu$ is $\sigma$-finite,
		every measurable set is of $\sigma$-finite
\eit
\vfill


\myfoilhead{Semifinite measures}

\bit
	\item roughly speacking, nearly all familiar properties of Lebesgue measure and Lebesgue integration
		hold for arbitrary $\sigma$-finite measure

	\vitem many treatment of abstract measure theory limit themselves to $\sigma$-finite measures

	\vfill
	\vfill
	\vvitem many parts of general theory, however, do \emph{not} required
		assumption of $\sigma$-finiteness
	\vitem undesirable to have development unnecessarily restrictive

	\vfill
	\vfill
	\vvitem measure, $\mu$, for which every measurable set of infinite measure
		contains measurable sets of arbitrarily large finite measure,
		said to be \define{semifinite}%
			\index{measure!semifinite}
	\vitem every $\sigma$-finite measure is semifinite measure
		while measure, $\mu_B$, on page~\pageref{page:interesting-(and-bizarre)-example}
		is not
\eit
\vfill


\myfoilhead{Complete measure spaces}

\bit
	\item measure space, \meas{X}{\algB}{\mu}, for which \algB\ contains all subsets of sets of measure zero,
		said to be \define{complete},%
			\index{measure spaces!complete}%
			\index{complete!measure spaces}%
			\index{complete!measure}%
			\index{measure!complete}
		\ie,
		\[
			(\forall B\in\algB \mbox{ with } \mu B=0)
			(A \subset B \Rightarrow A \in \algB)
		\]
	\bit
		\item \eg, Lebesgue measure is complete, but Lebesgue measure restricted to $\sigma$-algebra of Borel sets
			is \emph{not}
	\eit

	\vitem every measure space can be \emph{completed} by addition of subsets of sets of measure zero%
			\index{measure spaces!completed}

	\vitem for \meas{X}{\algB}{\mu}, can find \emph{complete} measure space \meas{X}{\algB_0}{\mu_0}\
		such that
		\begin{eqnarray*}
			&-&
				\algB \subset \algB_0
			\\
			&-&
				E \in\algB \Rightarrow \mu E = \mu_0 E
			\\
			&-&
				E \in\algB_0 \Leftrightarrow E = A \cup B
				\mbox{ where } B,C\in\algB, \mu C = 0, A\subset C
		\end{eqnarray*}

	\bit
		\item \meas{X}{\algB_0}{\mu_0}\ called \define{completion} of \meas{X}{\algB}{\mu}%
			\index{measure spaces!completion}
	\eit
\eit


\myfoilhead{Local measurability and saturatedness}

\bit
	\item for \meas{X}{\algB}{\mu},
		$E\subset X$ for which $(\forall B\in\algB \mbox{ with }\mu B < \infty)(E\cap B\in\algB)$,
		said to be \define{locally measurable}%
			\index{measure spaces!local measurability}

	\vitem collection, \algC, of all locally measurable sets
		is $\sigma$-algebra containing \algB\

	\vvitem measure for which every locally measurable set is measurable,
		said to be \define{saturated}%
			\index{measure spaces!saturatedness}

	\vitem every $\sigma$-finite measure is saturated

	\vvitem measure can be extended to saturated measure,
		but (unlike completion)
		extension is not unique
	\bit
		\item can take \algC\ as extension for locally measurable sets,
		but measure can be extended on \algC\ in more than one ways
	\eit
\eit


\myfoilhead{Measurable functions}%
	\pagelabel{page:Measurable functions}

\bit
	\item concept and properties of measurable functions in abstract measurable space
		almost identical with those of Lebesgue measurable functions
		(page~\pageref{title-page:measurable-functions})

	\vitem theorems and facts are essentially same as those of Lebesgue measurable functions
%		(page~\pageref{title-page:measurable-functions})

	\vvitem assume measurable space, \measu{X}{\algB}\

	\vitem for $f:X\to\ereals$, following are equivalent
	\bit
		\item $(\forall a\in\reals) (\set{x\in X}{f(x) < a}\in\algB)$
		\item $(\forall a\in\reals) (\set{x\in X}{f(x) \leq a}\in\algB)$
		\item $(\forall a\in\reals) (\set{x\in X}{f(x) > a}\in\algB)$
		\item $(\forall a\in\reals) (\set{x\in X}{f(x) \geq a}\in\algB)$
	\eit

	\vitem $f:X\to\ereals$ for which any one of above four statements holds,
		called \define{measurable} or \define{measurable with respect to \algB}%
			\index{measurable functions!abstract measurable spaces}
\refertocounterpart{Lebesgue}{page:Lebesgue-measurable-functions}
\eit


\myfoilhead{Properties of measurable functions}%
	\index{measurable functions!properties}
\pagelabel{page:Properties of measurable functions}

\bit
\item
	\begin{mytheorem}{measurability preserving function operations}
		for measurable functions, $f$ and $g$, and $c\in\reals$
		\bit
			\item $f+c$, $cf$, $f+g$, $fg$, $f\vee g$ are measurable
		\eit
	\end{mytheorem}

\vitem
	\begin{mytheorem}{limits of measurable functions}
		for every measurable function sequence, $\seq{f_n}$
		\bit
			\item $\sup f_n$, $\limsup f_n$, $\inf f_n$, $\liminf f_n$ are measurable
			\item thus, $\lim f_n$ is measurable if exists
		\eit
	\end{mytheorem}

	\refertocounterpart{Lebesgue}{page:measurable:function:facts}
\eit


\myfoilhead{Simple functions and other properties}
\pagelabel{page:Simple functions}

\bit
	\item $\varphi$ called \define{simple function} if for distinct $\seq{c_i}_{i=1}^n$\
		and measurable sets, $\seq{E_i}_{i=1}^n$
		\index{measurable functions!simple}
	\[
		\varphi(x) = \sum_{i=1}^n c_i \chi_{E_i}(x)
	\]
%	\bit
%		\item \ie, linear combination of characteristic functions of $E_i$
%	\eit

	\refertocounterpart{Lebesgue}{page:Characteristic and simple functions}

	\vvitem for nonnegative measurable function, $f$,
		exists nondecreasing sequence of simple functions, \seq{\varphi_n},
		\ie, $\varphi_{n+1}\geq \varphi_n$
		such that for every point in $X$
		\[
			f = \lim \varphi_n
		\]
	\bit
		\item for $f$ defined on $\sigma$-finite measure space,
			we may choose \seq{\varphi_n}\ so that
			every $\varphi_n$ vanishes outside set of finite measure
	\eit

	\vitem for complete measure, $\mu$,
		$f$ measurable and $f=g$ a.e. imply
		measurability of $g$
\eit


\myfoilhead{Define measurable function by ordinate sets}

\bit
	\item \set{x}{f(x)<\alpha}\ sometimes called \define{ordinate sets}%
			\index{measurable functions!ordinate sets}%
			\index{ordinate sets!measurable functions},
		which is nondecreasing in $\alpha$

	\vitem below says when given nondecreasing ordinate sets,
		we can find $f$ satisfying

	\begin{eqn}
%	\[
		\set{x}{f(x)<\alpha}
		\subset
		B_\alpha
		\subset
		\set{x}{f(x)\leq\alpha}
%	\]
	\end{eqn}

	\vitem for nondecreasing function, $h:D\to\algB$, for dense set of real numbers, $D$,
		\ie, $B_\alpha \subset B_\beta$ for all $\alpha<\beta$ where $B_\alpha = h(\alpha)$,
		exists unique measurable function, $f:X\to\ereals$
		such that $f\leq \alpha$ on $B_\alpha$ and $f\geq \alpha$ on $X\sim B_\alpha$

	\vvitem can relax some conditions and make it a.e. version as below

	\vitem for function, $h:D\to\algB$, for dense set of real numbers, $D$,
		such that $\mu(B_\alpha\sim B_\beta)=0$ for all $\alpha < \beta$ where $B_\alpha = h(\alpha)$,
		exists measurable function, $f:X\to\ereals$
		such that $f\leq \alpha$ a.e. on $B_\alpha$ and $f\geq \alpha$ a.e. on $X\sim B_\alpha$%\
			\index{measurable functions!defined by ordinate sets}
	\bit
		\vitem if $g$ has the same property, $f=g$ a.e.
	\eit

\eit


\myfoilhead{Integration}
\pagelabel{page:Integration}

\bit
	\item
		many definitions and proofs of Lebesgue integral
		depend only on properties of Lebesgue measure
		which are also true for arbitrary measure in abstract measure space
		(page~\pageref{title-page:lebesgue-integral})

	\vitem
		integral of nonnegative simple function, $\varphi(x) = \sum_{i=1}^n c_i \chi_{E_i}(x)$,
		on measurable set, $E$, defined by%
			\index{integral!simple functions}
		\[
			\int_E \varphi d\mu= \sum_{i=1}^n c_i \mu (E_i \cap E)
		\]
	\bit
		\item independent of representation of $\varphi$
	\eit

	\refertocounterpart{Lebesgue}{page:Integral of simple functions}

	\vitem
		for $a,b\in\ppreals$ and nonnegative simple functions, $\varphi$ and $\psi$
		\[
			\int (a\varphi + b\psi) = a \int\varphi + b \int\psi
		\]

	\refertocounterpart{Lebesgue}{page:linearity of Lebesgue integral of simple functions}
\eit


\myfoilhead{Integral of bounded functions}

\bit
	\item
		for bounded function, $f$, identically zero outside measurable set of finite measure%
			\index{integral!bounded functions}
		\pagelabel{page:integral of simple functions}
		\[
			\sup_{\varphi:\ \mathrm{simple},\ \varphi \leq f} \int \varphi
			=
			\inf_{\psi:\ \mathrm{simple},\ f \leq \psi} \int \psi
		\]
		\iaoi\
		$f=g$ a.e. for measurable function, $g$

	\refertocounterpart{Lebesgue}{page:Lebesgue integral of simple functions}

	\vitem
		but,
		\fact{$f=g$ a.e. for measurable function, $g$,
		\iaoi\
		$f$ is measurable with respect to completion of $\mu$, $\bar{\mu}$}

	\vitem
		\eemph{natural class of functions to consider for integration theory are
		those measurable \wrt\ completion of $\mu$}

	\vitem
		thus, shall either
			assume $\mu$ is complete measure
		or
			define integral \wrt\ $\mu$ to be integral \wrt\ completion of $\mu$\
		depending on context
		unless otherwise specified
\eit


\myfoilhead{Difficulty of general integral of nonnegative functions}

\bit
\item
	for Lebesgue integral of nonnegative functions
		(page~\pageref{page:Lebesgue integral of nonnegative functions})
	\bit
	\item
		first define integral for bounded measurable functions
	\item
		define integral of nonnegative function, $f$
		as supremum of integrals of all bounded measurable functions, $h\leq f$,
		vanishing outside measurable set of finite measure
	\eit

\vitem
	unfortunately, not work in case that measure is not semifinite
	\bit
	\item
		\eg, if $\algB=\{\emptyset,X\}$ with $\mu \emptyset = 0$ and $\mu X = \infty$,
			we want $\int 1 d\mu=\infty$,
				but only bounded measurable function vanishing outside measurable set of finite measure is $h\equiv0$,
			hence,
				$\int g d\mu = 0$
	\eit

\vitem
	to avoid this difficulty,
	we define integral of nonnegative measurable function
	directly in terms of
	integrals of nonnegative simple functions
\eit


\myfoilhead{Integral of nonnegative functions}

\bit
	\item
		for measurable function, $f:X\to\reals\cup\{\infty\}$, on measure space, \meas{X}{\algB}{\mu},
		define \define{integral of nonnegative extended real-valued measurable function}%
			\index{integral!nonnegative functions}
		\pagelabel{page:integral of nonnegative extended real-valued measurable function}

		\begin{eqn}
%		\[
			\int f d\mu = \sup_{\varphi:\ \mathrm{simple\ function},\ 0\leq \varphi\leq f} \int \varphi d\mu
%		\]
		\end{eqn}

	\refertocounterpart{Lebesgue}{page:Lebesgue integral of nonnegative measurable function}

\vitem
	however,
	\emph{definition of integral of nonnegative extended real-valued measurable function}
		can be awkward to apply because
	\bit
	\item
		taking supremum over large collection of simple functions
	\item
		\emph{not clear from definition that $\int(f+g) = \int f + \int g$}
	\eit

\vitem
	thus,
	first establish some convergence theorems,
	and
	determine value of $\int f$
	as limit of $\int \varphi_n$ for increasing sequence, \seq{\varphi_n}, of simple functions
	converging to $f$
\eit


\myfoilhead{Fatou's lemma and monotone convergence theorem}

\bit
\vitem
	\name{Fatou's lemma -}
	for nonnegative measurable function sequence, $\seq{f_n}$,
	with $\lim f_n = f$ a.e. on measurable set, $E$%
		\index{Fatou, Pierre Joseph Louis!Fatou's lemma}%
		\index{integral!Fatou's lemma}%
		\index{Fatou's lemma!integral}
	\pagelabel{page:Fatou's lemma!integral}
	\[
		\int_E f \leq \liminf \int_E f_n
	\]

\vitem
	\name{monotone convergence theorem -}
	for nonnegative measurable function sequence, $\seq{f_n}$,
	with $f_n\leq f$ for all $n$ and with $\lim f_n = f$ a.e.
		\index{integral!monotone convergence theorem}%
		\index{monotone convergence theorem!integral}
		\pagelabel{page:abstract - monotone convergence theorem}
	\[
		\int_E f = \lim \int_E f_n
	\]

\refertocounterpart{Lebesgue}{page:Lebesgue - monotone convergence theorem}
\eit


\myfoilhead{Integrability of nonnegative functions}

\bit
\item
	for nonnegative measurable functions, $f$ and $g$, and $a,b\in\preals$
	\pagelabel{page:linearity of nonnegative integral - abstract}
%
%	\begin{eqn}
	\[
		\int (af + bg) = a\int f + b\int g
		\mbox{ \& }
		\int f \geq 0
	\]
%	\end{eqn}

	\bit
	\item
		equality holds \iaoi\ $f=0$ a.e.
	\eit

\refertocounterpart{Lebesgue}{page:linearity of nonnegative integral - Lebesgue}

\vitem
	monotone convergence theorem together with above yields,
	for nonnegative measurable function sequence, \seq{f_n}
	\[
		\int \sum f_n = \sum \int f_n
	\]

\vitem
	measurable nonnegative function, $f$, with%
		\index{integral!nonnegative functions!integrable}%
		\index{integral!integrable!nonnegative functions}%
		\index{integrable!nonnegative functions}
		\pagelabel{page:integrability of nonnegative functions}
	\[
		\int_E fd\mu <\infty
	\]
	said to be \define{integral (over measurable set, $E$, \wrt\ $\mu$)}

\refertocounterpart{Lebesgue}{page:Lebesgue integrability of nonnegative functions}
\eit


\myfoilhead{Integral}

\bit
\item
	arbitrary function, $f$, for which both $f^+$ and $f^-$ are integrable,
	said to be \define{integrable}%
		\index{integral}%
		\index{integral!integrable}%
		\index{integrable}
		\pagelabel{page:integral}

\vitem
	in this case, define \define{integral}
	\[
		\int_E f = \int_E f^+ - \int_E f^-
	\]

\refertocounterpart{Lebesgue}{page:Lebesgue integral}
\eit


\myfoilhead{Properties of integral}

\bit
\item
	for $f$ and $g$ integrable on measure set, $E$, and $a,b\in\reals$%
		\index{integral!properties}
		\pagelabel{page:properties of integral}
	\bit
	\vitem
		$af+bg$ is integral and
		\[
			\int_E (af+bg) = a \int_E f + b\int_E g
		\]

	\vitem
		if $|h|\leq |f|$ and $h$ is measurable, then $h$ is integrable

	\vitem
		if $f\geq g$ a.e.
		\[
			\int f \geq \int g
		\]
\eit

\refertocounterpart{Lebesgue}{page:properties of Lebesgue integral}
\eit


\myfoilhead{Lebesgue convergence theorem}

\bit
\item
	\name{Lebesgue convergence theorem -}
	for integral, $g$, over $E$
		and
	sequence of measurable functions, \seq{f_n}, with $\lim f_n(x) = f(x)$ a.e. on $E$,
	if%
		\index{Lebesgue convergence theorem!integral}%
		\index{integral!Lebesgue convergence theorem}%
		\index{Lebesgue, Henri L\'{e}on!convergence theorem}
		\pagelabel{page:Lebesgue!convergence theorem}
	\[
		|f_n(x)|\leq g(x)
	\]
	then
	\[
		\int_E f = \lim \int_E f_n
	\]

	\pagelabel{page:Lebesgue convergence theorem}

\refertocounterpart{Lebesgue}{page:Lebesgue convergence theorem for Lebesgue integral}
\eit


\myfoilhead{Setwise convergence of sequence of measures}

\bit
\item
	preceding convergence theorems assume fixed measure, $\mu$\

\vitem
	can generalize by allowing measure to vary

\vitem
	given measurable space, \measu{X}{\algB}, sequence of set functions, \seq{\mu_n}, defined on $\algB$, satisfying
	\[
		(\forall E\in\algB)
		(\lim \mu_n E = \mu E)
	\]

	for some set function, $\mu$, defined on \algB,
	said to \define{converge setwise} to $\mu$%
		\index{measure spaces!setwise convergence}
\eit


\myfoilhead{General convergence theorems}

\bit
\item
	\name{generalization of Fatou's leamma -}%
		\index{Fatou, Pierre Joseph Louis!Fatou's lemma}%
		\index{Fatou's lemma!generalization}%
		\index{integral!Fatou's lemma!generalization}
	for measurable space, \measu{X}{\algB},
	sequence of measures, \seq{\mu_n}, defined on \algB, converging setwise to $\mu$, defined on \algB,
		and
	sequence of nonnegative functions, \seq{f_n}, each measurable \wrt\ $\mu_n$,
	converging pointwise to function, $f$, measurable \wrt\ $\mu$\
	(compare with Fatou's lemma on page~\pageref{page:Fatou's lemma!integral})
	\[
		\int f d\mu \leq \liminf\int f_n d\mu_n
	\]

\item
	\name{generalization of Lebesgue convergence theorem -}%
		\index{Lebesgue, Henri L\'{e}on!convergence theorem}%
		\index{Lebesgue convergence theorem!generalization}%
		\index{integral!Lebesgue convergence theorem!generalization}
	for measurable space, \measu{X}{\algB},
	sequence of measures, \seq{\mu_n}, defined on \algB, converging setwise to $\mu$, defined on \algB,
		and
	sequences of functions, \seq{f_n} and \seq{g_n}, each of $f_n$ and $g_n$, measurable \wrt\ $\mu_n$,
	converging pointwise to $f$ and $g$, measurable \wrt\ $\mu$, respectively,
	such that
	(compare with Lebesgue convergence theorem on page~\pageref{page:Lebesgue!convergence theorem})

	\begin{eqn}
%	\[
		\lim \int g_n d\mu_n = \int g d\mu < \infty
%	\]
	\end{eqn}

	satisfy
%
%	\begin{eqn}
	\[
		\lim \int f_n d\mu_n = \int f\mu
	\]
%	\end{eqn}
%
\eit


\yesnoexec{\showincomplete}{%
\myfoilhead{Signed measures}
\index{measure!signed measures}

\bit
\item
	XXX Royden p270\idxtodo{5 - Measure and Integration - signed measures}
\eit
}{}


\yesnoexec{\showincomplete}{%
\myfoilhead{Radon-Nikodym theorem}

\bit
\item
	XXX Royden p276\idxtodo{5 - Measure and Integration - Radon-Nikodym theorem}
\eit
}{}


\myfoilhead{$L^p$ spaces}

\bit
\item
	for complete measure space, \meas{X}{\algB}{\mu}
	\bit
	\item
		space of measurable functions on $X$ with with $\int |f|^p < \infty$,
		for which element equivalence is defined by being equal a.e.,
		called \define{$L^p$ spaces} denoted by $L^p(\mu)$%
			\index{$L^p$ spaces!complete measure spaces}
	\item
		space of bounded measure functions,
		called \define{$L^\infty$ space} denoted by $L^\infty(\mu)$%
			\index{$L^\infty$ space!complete measure spaces}
	\eit

\vitem
	norms
	\bit
	\item
		for $p\in[1,\infty)$
		\[
			\|f\|_p=\left(
				\int |f|^p d\mu
			\right)^{1/p}
		\]
	\item
		for $p=\infty$
		\[
			\|f\|_\infty = \mathrm{ess\ sup} |f|
			= \inf \bigsetl{|g(x)|}{\mbox{measurable }g \mbox{ with } g=f \mbox{ a.e.}}
		\]
	\eit

\vitem
	for $p\in[1,\infty]$,
	spaces, $L^p(\mu)$, are Banach spaces
\eit


\myfoilhead{H\"{o}lder's inequality and Littlewood's second principle}%
	\pagelabel{page:Holder inequality-complete measure spaces}

\bit
\item
	\name{H\"{o}lder's inequality -}%
		\index{H\"{o}lder's inequality!complete measure spaces}%
		\index{H\"{o}lder, Ludwig Otto!H\"{o}lder's inequality!complete measure spaces}%
		\pagelabel{page:Holder's inequality!complete measure spaces}
	for $p,q\in[1,\infty]$ with $1/p+1/q=1$,
	$f\in L^p(\mu)$ and $g\in L^q(\mu)$ satisfy
	$fg \in L^1(\mu)$ and
	\[
		\|fg\|_1 = \int |fg| d\mu \leq \|f\|_p\|g\|_q
	\]

\refertocounterpart{normed spaces}{Holder inequality!linear normed spaces}

\vitem
	\name{complete measure space version of Littlewood's second principle -}
		\pagelabel{page:complete measure space version of Littlewood's second principle}
		\index{Littlewood's three principles!second principle!complete measure spaces}
	for $p\in[1,\infty)$
	\begin{eqnarray*}
		\lefteqn{
			(\forall f\in L^p(\mu), \epsilon>0)
		}
		\\
		&&
		(\exists \mbox{ simple function } \varphi \mbox{ vanishing outside set of finite measure})
		\\
		&&
		\ \ \ \ \ \ \ %
		(\|f-\varphi\|_p < \epsilon)
	\end{eqnarray*}

\refertocounterpart{normed spaces}{page:normed space version of Littlewood's second principle}
\eit


\myfoilhead{Riesz representation theorem}

\bit
%\item
%	when $p\in[1,\infty)$
%	\bit
%	\item
%		for finite measure space, \meas{X}{\algB}{\mu},
%		integrable function, $g$, with
%		\[
%			(\exists M>0)
%			(\forall \mbox{ simple function }\varphi)
%			\left(\left|\int g\varphi d\mu\right|\leq M\|\varphi\|_p\right)
%		\]
%		belongs to $L^q(\mu)$
%
%	\vitem
\item
	\name{Riesz representation theorem -}%
		\index{Riesz representation theorem!complete measure spaces}%
		\index{Riesz representation theorem}%
		\index{Riesz, Frigyes!Riesz representation theorem!complete measure spaces}%
		\index{Riesz, Frigyes!Riesz representation theorem}%
		\pagelabel{page:Riesz representation theorem!complete measure spaces}\
	for $p\in[1,\infty)$ and bounded linear functional, $F$, on $L^p(\mu)$
	and $\sigma$-finite measure, $\mu$,
	exists \emph{unique} $g\in L^q(\mu)$
	where $1/p+1/q=1$
	such that
	\[
		F(f) = \int fg d\mu
	\]
	where $\|F\| = \|g\|_q$

\refertocounterpart{normed spaces}{page:Riesz-representation-theorem}

%\eit

\vitem
	if $p\in(1,\infty)$,
	{Riesz representation theorem} holds without assumption of $\sigma$-finiteness of measure
\eit


\titlefoil{Measure and Outer Measure}{Measure and Outer Measure}

\myfoilhead{General measures}

\bit
\item
	consider some ways of defining measures on $\sigma$-algebra

\vitem
	recall that for Lebesgue measure
	\bit
	\item
		define measure for open intervals
	\item
		define outer measure
	\item
		define notion of measurable sets
	\item
		finally derive Lebesgue measure
	\eit

\vitem
	one can do similar things in general, \eg,
	\bit
	\item
		derive measure from outer measure
	\item
		derive outer measure from measure defined on algebra of sets
	\eit
\eit


\myfoilhead{Outer measure}

\bit
\item
	set function, $\mu^\ast:\powerset(X)\to[0,\infty]$,
	for space $X$, having following properties,
	called \define{outer measure}%
		\index{outer measure}
	\bit
	\item
		$\mu^\ast \emptyset = 0$
	\item
		$A\subset B \Rightarrow \mu^\ast A \leq \mu^\ast B$
		(monotonicity)
	\item
		$E \subset \bigcup_{n=1}^\infty E_n \Rightarrow \mu^\ast E \leq \sum_{n=1}^\infty \mu^\ast E_n$
		(countable subadditivity)
	\eit

\vitem
	$\mu^\ast$ with $\mu^\ast X<\infty$ called \define{finite}\index{outer measure!finite}

\vvitem
	set $E\subset X$ satisfying following property,
	said to be \define{measurable \wrt\ $\mu^\ast$}
		\index{measurable sets}

	\begin{eqn}
%	\[
		(\forall A\subset X)
		(\mu^\ast(A) =\mu^\ast(A\cap E) + \mu^\ast(A\cap \compl{E}))
%	\]
	\end{eqn}

\vitem
	class, \algB, of $\mu^\ast$-measurable sets is $\sigma$-algebra

\vitem
	restriction of $\mu^\ast$ to \algB\ is complete measure on \algB\
\eit


\myfoilhead{Extension to measure from measure on an algebra}

\bit
\item
	set function, $\mu:\alg\to[0,\infty]$, defined on algebra, \alg,
	having following properties,
	called \define{measure on an algebra}%
		\index{measure!on an algebra}

	\bit
	\item
		$\mu(\emptyset) = 0$
	\item
		$\left(
			\forall \mbox{ disjoint } \seq{A_n} \subset \alg \mbox{ with } \bigcup A_n \in \alg
		\right)
		\left(
			\mu\left(\bigcup A_n\right) = \sum \mu A_n
		\right)$
	\eit

\vitem
	\emph{measure on an algebra, \alg}, is measure \iaoi\ \alg\ is $\sigma$-algebra

\vvitem
	can extend measure on an algebra to measure defined on $\sigma$-algebra, \algB, containing \alg,
	by

	\bit
	\item
		constructing outer measure $\mu^\ast$ from $\mu$\
	\item
		deriving desired extension $\bar{\mu}$ induced by $\mu^\ast$\
	\eit

\vitem [--]
	process by which constructing $\mu^\ast$ from $\mu$
		similar to
	constructing Lebesgue outer measure from lengths of intervals
\eit


\myfoilhead{Outer measure constructed from measure on an algebra}

\bit
\item [---]
	given
	measure, $\mu$, on an algebra, \alg\

\vitem
	define set function, $\mu^\ast:\powerset(X)\to[0,\infty]$, by
	\[
		\mu^\ast E = \inf_{\seq{A_n}\subset \alg,\ E\subset \bigcup A_n} \sum \mu A_n
	\]

\vitem $\mu^\ast$ called \define{outer measure induced by $\mu$}%
		\index{outer measure!induced by measure on an algebra}

\vvitem [---]
	then

\vitem
	for $A\in\alg$ and $\seq{A_n}\subset\alg$ with $A\subset \bigcup A_n$, $\mu A\leq \sum \mu A_n$

\vitem
	hence, $(\forall A\in\alg)(\mu^\ast A = \mu A)$

\vitem
	$\mu^\ast$ is outer measure

\vitem
	every $A\in\alg$ is measurable \wrt\ $\mu^\ast$
\eit


\myfoilhead{Regular outer measure}

\bit
\item
	for algebra, \alg\
	\bit
	\item
		$\alg_\sigma$ denote sets that are countable unions of sets of \alg\
	\item
		$\alg_{\sigma \delta}$ denote sets that are countable intersections of sets of $\alg_\sigma$\
	\eit

\vitem
	given
	measure, $\mu$, on an algebra, \alg\
	and
	outer measure, $\mu^\ast$ induced by $\mu$,
	for every $E\subset X$ and every $\epsilon>0$,
	exists $A\in\alg_\sigma$ and $B\in\alg_{\sigma \delta}$ with $E\subset A$ and $E\subset B$
	\[
		\mu^\ast A \leq \mu^\ast E + \epsilon
			\mbox{ and }
		\mu^\ast E = \mu^\ast B
	\]

\vitem outer measure, $\mu^\ast$, with below property,
	said to be \define{regular}%
		\index{outer measure!regular}
	\[
		(\forall E\subset X, \epsilon>0)
		(\exists \mbox{ $\mu^\ast$-measurable set }A \mbox{ with } E\subset A)
		(\mu^\ast A \subset \mu^\ast E + \epsilon)
	\]

\vitem
	every outer measure induced by measure on an algebra is regular outer measure%
		\idxrevisit{every outer measure induced by measure on an algebra is regular outer measure}
\eit


\myfoilhead{Carath\'{e}odory theorem}

\bit
\item [---]
	given measure, $\mu$, on an algebra, \alg\ and outer measure, $\mu^\ast$ induced by $\mu$\

\vitem
	$E\subset X$ is $\mu^\ast$-measurable
		\iaoi\
	exist $A\in\alg_{\sigma\delta}$ and $B\subset X$ with $\mu^\ast B=0$
	such that
	\[
		E=A\sim B
	\]

	\bit
	\item
		for $B\subset X$ with $\mu^\ast B=0$,
		exists $C\in\alg_{\sigma\delta}$ with $\mu^\ast C=0$
		such that $B\subset C$
	\eit

\vvitem
	\name{Carath\'{e}odory theorem -}\
		\index{Carath\'{e}odory theorem}%
		\index{Carath\'{e}odory, Constantin!Carath\'{e}odory theorem}
	restriction, $\bar{\mu}$, of $\mu^\ast$ to $\mu^\ast$-measurable sets
	if extension of $\mu$ to $\sigma$-algebra containing \alg\
	\bit
	\vitem
		if $\mu$ is finite or $\sigma$-finite,
		so is $\bar{\mu}$ respectively
	\item
		if $\mu$ is $\sigma$-finite,
		$\bar{\mu}$
		is only measure
		on smallest $\sigma$-algebra containing \alg\
		which is extension of $\mu$
	\eit
\eit


\yesnoexec{\showincomplete}{%
\myfoilhead{XXX: more on extension theorem}

\bit
\item
	\bit
	\item
	\eit

\vitem
	\bit
	\item
	\eit
\eit
}{}


\myfoilhead{Product measures}%
	\index{general measure!product measure}%
	\index{product measure!general measure}

\bit
\item
	for countable disjoint collection of measurable rectangles, \seq{(A_n \times B_n)},
	whose union is measurable rectangle, $A\times B$\
	\[
		\lambda(A\times B) = \sum \lambda(A_n \times B_n)
	\]

\vitem
	for $x\in X$ and $E\in \algk{R}_{\sigma\delta}$
	\[
		E_x = \set{y}{\langle x,y\rangle \in E}
	\]
	is measurable subset of $Y$

\vitem
	for $E\subset\algk{R}_{\sigma\delta}$ with $\mu \times \nu(E)<\infty$,
	function, $g$, defined by
	\[
		g(x) = \nu E_x
	\]
	is measurable function of $x$ and
	\[
		\int g d\mu = \mu \times \nu(E)
	\]
\vitem
	XXX
\eit


\yesnoexec{\showincomplete}{%
\myfoilhead{Integral operators}

\bit
\item XXX
	\bit
	\item
	\eit

\vitem
	\bit
	\item
	\eit
\eit
}{}


\myfoilhead{Carath\'{e}odory outer measures}

\bit
\item
	set, $X$, of points and set, $\Gamma$, of real-valued functions on $X$

%\vitem
%	want to know conditions under which
%	outer measure, $\mu^\ast$,
%	makes every function in $\Gamma$ measurable

\vvitem
	two sets for which exist $a>b$ such that function, $\varphi$, greater than $a$ on one set
	and less than $b$ on the other set,
	said to be \define{separated by function, $\varphi$}%
		\index{separated by function}

\vitem outer measure, $\mu^\ast$, with
	$
		(\forall A,B\subset X \mbox{ separated by } f\in\Gamma)
		(\mu^\ast(A\cup B) = \mu^\ast A + \mu^\ast B)
	$,
	called \define{Carath\'{e}odory outer measure \wrt\ $\Gamma$}%
		\index{Carath\'{e}odory outer measure!with respect to function set}%
		\index{Carath\'{e}odory, Constantin!outer measure}%
		\index{outer measure!Carath\'{e}odory}

\vitem
	outer measure, $\mu^\ast$, on metric space, \metrics{X, \rho},
	for which $\mu^\ast(A\cup B)=\mu^\ast A + \mu^\ast B$ for $A,B\subset X$ with $\rho(A,B)>0$,
	called \define{\cara\ outer measure for $X$} or \define{metric outer measure}%
		\index{Carath\'{e}odory outer measure!for metric space}%
		\index{Carath\'{e}odory, Constantin!outer measure}%
		\index{outer measure!Carath\'{e}odory}%
		\index{metric outer measure}

\vvitem
	for \emph{\cara\ outer measure, $\mu^\ast$, \wrt\ $\Gamma$},
	every function in $\Gamma$ is $\mu^\ast$-measurable

\vitem
	for \emph{\cara\ outer measure, $\mu^\ast$, for metric space, \metrics{X, \rho}},
	every closed set (hence every Borel set) is measurable \wrt\ $\mu^\ast$
\eit


\yesnoexec{\showincomplete}{%
\myfoilhead{Hausdorff measure}

\bit
\item
	\bit
	\item
	\eit

\vitem
	\bit
	\item
	\eit
\eit
}{}


\yesnoexec{\showincomplete}{%
\titlefoil{Measure and Topology}{Measure and Topology}
}{}
}{}

}{}


\yesnoexec{\probstat}{%
\TITLEFOIL{Probability and Statistics}{Probability}\

\titlefoil{Statistics for Beth}{Statistics for Beth}\


\myfoilhead{Central limit theorem}%
	\index{probability and statistics!central limit theorem}%
	\index{central limit theorem}%
	\idxtodo{DONE - 2024 0324 - central limit theorem}

\bit
	\item central limit theorem says (normalized quantity of) mean of independent and identically distributed (i.i.d) random variables
		converges to normal distribution\index{probability distributions!normal}
		even though the original distribution is not normal

	\vitem more specifically, if $X_1$, \ldots, $X_n$ are drawn from same distribution independently,
		distribution of mean (or expected value)
		converges to $\normal(\mu,\sigma^2/n)$ as $n$ goes to $\infty$,
		\ie,
		\[
			\bar{X}_n = \frac{X_1+\cdots+X_n}{n} \sim \normal(\mu_X, \sigma_X^2/n)
		\]
		where $\mu_X$ and $\sigma_X^2$ is mean and variance of original random variable

		or equivalently,
		\[
			\frac{\bar{X}_n-\mu_X}{\sigma_X/\sqrt{n}} \sim \normal(\mu_X, \sigma_X^2/n)
		\]
\eit


\myfoilhead{Central limit theorem - example}

\bit
	\item \figref{distribution of sample mean of gamma random variables}\
		shows distribution of $\bar{X}_n$ for $n=5$, $10$, and $100$
		when original random variable follows gamma distribution\index{probability distributions!gamma}
		\[
			X \sim \Gamma(1,2)
		\]
		where probability density function (PDF) of $X$ is\
			$f(x;\alpha, \beta) = {x^{\alpha-1} e^{-\beta x} \beta^{\alpha}}/{\Gamma(\alpha)}$
	\bit
		\item \textcolor{blue}{blue histograms} show distribution estimation by Monte Carlo simulation
		\item \textcolor{red}{red lines} show PDF of standard normal distribution, \ie, $\normal(0,1)$
	\eit
	\item we observe that distribution of $\bar{X}_n$ gets closer to $\normal(0,1)$ as $n$ increases

\eit

\vfill

\begin{figure}
\begin{center}
	\mypsfrag{n=5}{$n=5$}
	\mypsfrag{n=10}{$n=10$}
	\mypsfrag{n=100}{$n=100$}
	\includegraphics[width=.9\textwidth]{figures/clt_plots_psfragable}%
		\idxfig{distribution of sample mean of gamma random variables}
		\label{fig:distribution of sample mean of gamma random variables}
\end{center}
\end{figure}

\vfill


\myfoilhead{$z$-score}\index{probability and statistics!$z$-score}

\bit
	\item why do we care about $z$-score?
	\bit
		\item allow us think of probability distribution with arbitrary mean and variance (or standard deviation)
			using the standardized random variable
			in the same family of probability distributions,
			\ie,
			the one with zero mean and unit variance
	\eit

	\vitem what is $z$-score?
	\bit
		\item let
		\bit
			\item [-] $\mu_X$ or $\Expect X$ denote mean (sometimes called \emph{expected value}) of $X$%
				\index{probability!mean}%
				\index{probability!expected value}
			\item [-] $\sigma_X$ or $\sqrt{\Var X}$ denote standard deviation%
				\index{probability!standard deviation}%
			\item [-] (hence) $\sigma_X^2$ or ${\Var X}$ denote variance%
				\index{probability!variance}
		\eit

		\vitem then, \define{$z$-score} of $x$ is defined by\index{probability and statistics!$z$-score}
		\[
			z = (x-\mu_X) / \sigma_X
		\]
	\eit
\eit


\myfoilhead{Properties of random variables}%
	\index{random variables!properties}

\bit
	\item note for (almost) all types of probability distributions,\index{probability distributions}\index{probability!distributions}
		the type of distribution preserved under two transformation; translation and scaling, \eg,

	\bit
		\item if random variable, $X$, is Gaussian or uniformly distributed,%
			\index{probability distributions!Gaussian}%
			\index{probability distributions!uniform}%
			\index{Gauss!Gaussian distribution}
			then $X-a$, $X+a$, $aX$, $X/b$ are Gaussian or uniformly distributed respectively
	\eit

	\vitem also note that for any random variable, $X$, and any $a\in\reals$
	\begin{eqnarray*}
		\Expect (X+a) = \Expect (X) + a && \Var(X+a) = \Var X \Leftrightarrow \sigma_{X+a} = \sigma_X
		\\
		\Expect (aX) = a\Expect (X)&& \Var(aX) = a^2\Var X \Leftrightarrow \sigma_{aX} = |a|\sigma_X
	\end{eqnarray*}
	hence,
	\begin{eqnarray*}
		\Expect (X-a) = \Expect (X) - a && \Var(X-a) = \Var X \Leftrightarrow \sigma_{X-a} = \sigma_X
		\\
		\Expect (X/a) = \Expect (X)/a&& \Var(X/a) = \Var X/a^2 \Leftrightarrow \sigma_{X/a} = \sigma_X / |a|
	\end{eqnarray*}
\eit


\myfoilhead{Calculation of probabilities using $z$-score}\index{probability and statistics!$z$-score}

\bit

	\item suppose we want to find out probability that $X$ is less than or equal to $a$
%
%	\begin{eqn}
	\[
		\Prob(X\leq a)
	\]
%	\end{eqn}


	\vitem note that
%
%	\begin{eqn}
%	\[
	$
		X\leq a
		\Leftrightarrow
		(X-\mu_X)/\sigma_X \leq (a-\mu_X)/\sigma_X = \mbox{$z$-score of $a$}
	$
%	\]
%	\end{eqn}

	\vitem hence, if we define new random variable, $Z$, such that $Z=(X-\mu_X)/\sigma_X$,
		if follows
	\[
		\Expect (Z) = (\Expect (X) - \mu_X)/\sigma_X = 0,\ %
		\Var (Z) = \Var (X) / \sigma_X^2 = 1
	\]

	\vitem NOW, if $Z$ follows normal distribution, we can evaluate the quantity (using the table)
	\[
		\Prob(X\leq a) = \Prob(Z \leq \mbox{$z$-score of $a$})
	\]

		as well as virtually any probability quantities about $X$, \eg,
	\begin{eqnarray*}
		\Prob(X\geq a) &=& \Prob(Z \geq \mbox{$z$-score of $a$})
		\\
		\Prob(X= a) &=& \Prob(Z = \mbox{$z$-score of $a$})
	\end{eqnarray*}
\eit


\myfoilhead{Stat problem for Beth - dice}%
	\index{Beth!stat problem - dice}

\bit
	\item assume Beth throws dice $300$ times.
		what is the probability that the average of the numbers on the dice in the $300$
		is {less than} $3.7$?
		Use the fact that the average of one dice throwing is $3.5$
		and its variance is $35/12$.

	\bit
		\vitem (answer) let $\hat{P}$ be the random variable for the \emph{sample mean}. Then
			it (closely) follows the normal distribution
			\[
				\hat{P} \sim \normal ( 3.5, (35/12)/300)
				= \normal ( 3.5, 0.098^2)
			\]
			thus,
			\begin{eqnarray*}
				\Prob(\hat{P} < 3.7) &=& \Prob((\hat{P} - 3.5)/0.098 < (3.7-3.5)/0.098)
				\\
				&=& \Prob(Z < 2.04)
			\end{eqnarray*}

		Hence, $z$-score is $(3.7-3.5/0.098) = 2.04$ and the probability is $97.9$\%.
	\eit
\eit


\myfoilhead{Stat problem for Beth - Tesla model S}%
	\index{Beth!stat problem - Tesla model S}

\bit
	\item assume Tesla's model S is supposed to make up market shart of $40$\% among all electric vehicles (EVs) in US.
		when you go to Disney Land and look at a parking lot filled with $400$ EVs,
		what is the probability that the percentage of model S is more than $34$\% (among the $400$ electric vehicles)?
		use the fact that the variance of the sample mean is $p(1-p)/n$
		where $p$ is the true probability and $n$ is the sample size.


	\bit
		\vitem (answer) let $\hat{P}$ be the random variable for the \emph{sample mean}. Then
			it (closely) follows the normal distribution
			\[
				\hat{P} \sim \normal ( 0.4, 0.4 \cdot 0.6 / 400)
				= \normal ( 0.4, 0.0245^2)
			\]

			thus,
			\begin{eqnarray*}
				\Prob(\hat{P} > 0.34) &=& \Prob((\hat{P} - 0.4)/0.0245 < (0.34-0.4)/0.0245)
				\\
				&=& \Prob(Z > -2.449)
			\end{eqnarray*}

			Hence, $z$-score is $-2.449$ and the probability is $99.28$\%.
	\eit
\eit
}{}


\yesnoexec{\measprob}{%
\TITLEFOIL{Measure-theoretic Treatment of Probabilities}{Measure Theoretic Treatment of Probabilities}
\nocite{Billingsley:95}

\titlefoil{Probability Measure}{Probability Measure}

\myfoilhead{Measurable functions}%
	\index{measurable functions!abstract measurable spaces}%

\bit
\item [--]
	denote \define{$n$-dimensional Borel sets} by $\algR^n$%
		\index{Borel sets!multi-dimensional}

\vitem
	for two measurable spaces, \measu{\Omega}{\algF}\ and \measu{\Omega'}{\algF'},
	function, $f:\Omega \to \Omega'$ with
	\[
		\left(
			\forall A' \in \algF'
		\right)
		\left(
			f^{-1}(A') \in \algF
		\right)
	\]
	said to be \define{measurable \wrt\ $\algF/\algF'$}%
		\index{measurable functions!abstract measurable spaces}\
	(thus, measurable functions defined on page~\pageref{page:Lebesgue-measurable-functions}\
	and page~\pageref{page:Measurable functions}\
	can be said to be measurable \wrt\ $\collk{B}/\algR$)

\vitem
	when $\Omega=\reals^n$ in \measu{\Omega}{\algF},
	\algF\ is assumed to be $\algR^n$,
	and sometimes drop $\algR^n$
	\bit
	\item thus, \eg, we say $f:\Omega\to\reals^n$ is measurable \wrt\ \algF\ (instead of $\algF/\algR^n$)
	\eit

\vitem
	measurable function, $f:\reals^n\to\reals^m$ (\ie, measurable \wrt\ $\algR^n/\algR^m$),
	called \define{Borel functions}%
		\index{Borel functions}%
		\index{Borel, F\'{e}lix \'{E}douard Justin \'{E}mile!functions}

\vitem
	$f:\Omega\to\reals^n$ is measurable \wrt\ $\algF/\algR^n$
		\iaoi\
	every component, $f_i:\Omega\to\reals$, is measurable \wrt\ $\algF/\algR$
\eit


\myfoilhead{Probability (measure) spaces}%
	\index{probability spaces}%
	\index{probability!probability (measure) spaces}

\bit
\item
	set function, $P:\algk{F}\to[0,1]$, defined on algebra, \algk{F}, of set $\Omega$,
	satisfying following properties,
	called \define{probability measure}%
		\index{probability spaces!probability measure}
	(refer to page~\pageref{page:measure!measurable spaces} for resumblance with measurable spaces)

	\bit
	\item
		$(\forall A\in\algk{F})(0\leq P(A)\leq 1)$
	\item
		$P(\emptyset) = 0,\ P(\Omega) = 1$
	\item
		$(\forall \mbox{ disjoint } \seq{A_n} \subset \algk{F} )(P\left(\bigcup A_n\right) = \sum P(A_n))$
	\eit

\vitem
	for $\sigma$-algebra, \algk{F}, \meas{\Omega}{\algk{F}}{P},
	called \define{probability measure space} or \define{probability space}%
		\index{probability!probability (measure) spaces}%
		\index{probability (measure) spaces}

\vitem set $A\in\algk{F}$ with $P(A)=1$,
	called \define{a support of $P$}%
		\index{probability spaces!support}
\eit
%\vfill


\myfoilhead{Dynkin's $\pi$-$\lambda$ theorem}%
	\index{Dynkin's $\pi$-$\lambda$ theorem}%
	\index{$\pi$-$\lambda$ theorem!Dynkin, Eugene Borisovich}%
	\index{Dynkin, Eugene Borisovich!$\pi$-$\lambda$ theorem}

\bit
\item
	class, \subsetset{P}, of subsets of $\Omega$ closed under finite intersection,
	called \define{$\pi$-system}, \ie,%
		\index{$\pi$-system}
	\bit
	\item
		$(\forall A,B\in \subsetset{P})(A\cap B\in\subsetset{P})$
	\eit


\vitem
	class, \subsetset{L}, of subsets of $\Omega$ containing $\Omega$
	closed under complements and countable disjoint unions
	called \define{$\lambda$-system}%
		\index{$\lambda$-system}
	\bit
	\item
		$\Omega \in \subsetset{L}$
	\item
		$(\forall A\in \subsetset{L})(\compl{A}\in\subsetset{L})$
	\item
		$(\forall \mbox{ disjoint }\seq{A_n})(\bigcup A_n \in \subsetset{L})$
	\eit

\vitem
	\emph{class that is both $\pi$-system and $\lambda$-system is $\sigma$-algebra}%
		\index{$\pi$-$\lambda$ theorem}

\vitem
	\name{Dynkin's $\pi$-$\lambda$ theorem -}
	for $\pi$-system, \subsetset{P}, and $\lambda$-system, \subsetset{L},
	with $\subsetset{P} \subset \subsetset{L}$,%
		\index{Dynkin's $\pi$-$\lambda$ theorem}%
		\index{$\pi$-$\lambda$ theorem!Dynkin, Eugene Borisovich}%
		\index{Dynkin, Eugene Borisovich!$\pi$-$\lambda$ theorem}
	\[
		\sigma(\subsetset{P}) \subset \subsetset{L}
	\]

\vitem
	for $\pi$-system, \algk{P},
	two probability measures, $P_1$ and $P_2$, on $\sigma(\algk{P})$,
	agreeing \algk{P},
	agree on $\sigma(\algk{P})$\
	\pagelabel{page:probability-measures-agreeing-on-P-agree-on-sigma-P}
\eit


\myfoilhead{Limits of Events}

	\begin{mytheorem}{convergence-of-events}{no}{}%
		\index{probability spaces!limits!events}%
		\index{limits!events}
	for sequence of subsets, \seq{A_n},
		$$
		P(\liminf A_n) \leq \liminf P(A_n) \leq \limsup P(A_n) \leq P(\limsup A_n)
		$$
	\bit
	\item [-]
		for \seq{A_n}\ converging to $A$
		$$
			\lim P(A_n) = P(A)
		$$
	\eit
	\end{mytheorem}

	\begin{mytheorem}{independence-of-smallest-sig-alg}{no}{}
	for sequence of $\pi$-systems, \seq{\algA_n}, \seq{\sigma(\algA_n)}\ is independent
	\end{mytheorem}


\myfoilhead{Probabilistic independence}

\bit
\item [--] given probability space, \meas{\Omega}{\algk{F}}{P}

\vitem $A,B\in\algk{F}$ with
	\[
		P(A\cap B) = P(A) P(B)
	\]
	said to be \define{independent}%
		\index{probability spaces!independence}%
		\index{probability spaces!independence!of two events}%
		\index{independence!probability spaces}%

\vvitem indexed collection, $\seq{A_\lambda}$, with
	\[
		\left(
			\forall n\in\naturals, \mbox{ distinct } \lambda_1, \ldots, \lambda_n \in \Lambda
		\right)
		\left(
			P\left(\bigcap_{i=1}^n A_{\lambda_i}\right) = \prod_{i=1}^n P(A_{\lambda_i})
		\right)
	\]

	said to be \define{independent}%
		\index{probability spaces!independence}%
		\index{probability spaces!independence!of collection of events}%
		\index{independence!probability spaces}%
\eit


\myfoilhead{Independence of classes of events}

\bit
\item
	indexed collection, $\seq{\subsetset{A}_\lambda}$, of classes of events (\ie, subsets) with
	\[
		\left(
			\forall A_\lambda \in \subsetset{A}_\lambda
		\right)
		\left(
			\seq{A_\lambda} \mbox{ are independent}
		\right)
	\]
	said to be \define{independent}%
		\index{probability spaces!independence}%
		\index{probability spaces!independence!of collection of classes of events}%
		\index{independence!probability spaces}%
		\pagelabel{probability spaces!independence!of collection of classes of events}

\vitem
	\fact{
	for independent indexed collection, \seq{\subsetset{A}_\lambda}, with every $\subsetset{A}_\lambda$ being $\pi$-sytem,
	\seq{\sigma(\subsetset{A}_\lambda)} are independent
	}%
		\pagelabel{page:pi-system-induces-independent-sigma-algebras}

\vitem
	for independent (countable) collection of events, $\seq{\seq{A_{ni}}_{i=1}^\infty}_{n=1}^\infty$,
	$\seq{\algk{F}_n}_{n=1}^\infty$\ with $\algk{F}_n = \sigma(\seq{A_{ni}}_{i=1}^\infty)$
	are independent
\eit


\myfoilhead{Borel-Cantelli lemmas}%
	\index{Borel-Cantelli lemmas}%
	\index{Borel, F\'{e}lix \'{E}douard Justin \'{E}mile!Borel-Cantelli lemmas}%
	\index{Cantelli, Francesco Paolo!Borel-Cantelli lemmas}

\bit
\item
	\begin{mylemma}{first Borel-Cantelli}
		\index{Borel-Cantelli lemmas!first}
		for sequence of events, \seq{A_n}, with $\sum P(A_n)$ converging
		$$
			P(\limsup A_n) = 0
		$$
	\end{mylemma}

\vitem
	\begin{mylemma}{second Borel-Cantelli}
		\index{Borel-Cantelli lemmas!second}
		for independent sequence of events, \seq{A_n}, with $\sum P(A_n)$ diverging
		$$
			P(\limsup A_n)=1
		$$
	\end{mylemma}
\eit


\myfoilhead{Tail events and Kolmogorov's zero-one law}%
	\index{tail events}%
	\index{Kolmogorov's zero-one law}%
	\index{Kolmogorov, Andrey Nikolaevich!Kolmogorov's zero-one law}%
	\index{probability spaces!tail events}%
	\index{probability spaces!Kolmogorov's zero-one law}

\bit
\item
	for sequence of events, \seq{A_n}\
	\[
		\algk{T} = \bigcap_{n=1}^\infty \sigma\left(\seq{A_i}_{i=n}^\infty\right)
	\]
	called \define{tail $\sigma$-algebra associated with \seq{A_n}};
	its lements are called \define{tail events}%
		\index{tail events}%
		\index{probability spaces!tail events}%
		\index{tail $\sigma$-algebra}%
		\index{probability spaces!tail $\sigma$-algebra}%

\vitem
	\name{Kolmogorov's zero-one law -}%
		\index{Kolmogorov's zero-one law}%
		\index{probability!Kolmogorov's zero-one law}
		for independent sequence of events, \seq{A_n}\
		every event in tail $\sigma$-algebra
		has probability measure either $0$ or $1$
\eit


\myfoilhead{Product probability spaces}%
	\index{probability spaces!product spaces}%
	\index{product probability spaces}%

\bit
\item
	for two measure spaces, \meas{X}{\algX}{\mu}\ and \meas{Y}{\algY}{\nu},
	want to find product measure, $\pi$,
	such that
	\[
		\left(
			\forall A\in \algX, B\in\algY
		\right)
		\left(
		\pi(A\times B) = \mu(A)\nu(B)
		\right)
	\]

	\bit
	\item
		\eg, if both $\mu$ and $\nu$ are Lebesgue measure on $\reals$,
			$\pi$ will be Lebesgue measure on $\reals^2$
	\eit

\vitem
	$A\times B$ for $A\in\algX$ and $B\in\algY$ is \emph{measurable rectangle}%
		\index{product probability spaces!measurable rectangles}

\vitem \define{$\sigma$-algebra generated by measurable rectangles}%
	\index{product probability spaces!$\sigma$-algebra generated by measurable rectangles}
	\index{generated by!product probability spaces!$\sigma$-algebra by measurable rectangles}
	denoted by
	\[
		\algX \times \algY
	\]

	\bit
		\item thus, \eemph{not}\ Cartesian product in usual sense
		\item generally \emph{much larger} than class of measurable rectangles
	\eit

\eit


\myfoilhead{Sections of measurable subsets and functions}

\bit
\item []
	for two measure spaces, \meas{X}{\algX}{\mu}\ and \meas{Y}{\algY}{\nu}

\vitem
	sections of measurable subsets%
		\index{product probability spaces!sections of measurable subsets}
	\bit
	\item
		\set{y\in Y}{(x,y)\in E}\ is \define{section of $E$ determined by $x$}
	\item
		\set{x\in X}{(x,y)\in E}\ is \define{section of $E$ determined by $y$}
	\eit

\vitem
	sections of measurable functions
	- for measurable function, $f$, \wrt\ $\algX\times \algY$%
		\index{product probability spaces!sections of measurable functions}
	\bit
		\item $f(x,\cdot)$ is \define{section of $f$ determined by $x$}
		\item $f(\cdot,y)$ is \define{section of $f$ determined by $y$}
	\eit

\vvitem
	sections of measurable subsets are measurable
	\bit
	\item
		$ \left( \forall x\in X, E\in \algX \times \algY \right) \left( \set{y\in Y}{(x,y)\in E} \in \algY \right) $
	\item
		$ \left( \forall y\in Y, E\in \algX \times \algY \right) \left( \set{x\in X}{(x,y)\in E} \in \algX \right) $
	\eit

\vitem
	sections of measurable functions are measurable
	\bit
	\item
		$f(x,\cdot)$ is measurable \wrt\ \algY\ for every $x\in X$
	\item
		$f(\cdot,y)$ is measurable \wrt\ \algX\ for every $y\in Y$
	\eit
\eit


\myfoilhead{Product measure}%
	\index{product probability spaces!product measure}%
	\index{product measure!product probability spaces}

\bit
\item []
	for two $\sigma$-finite measure spaces, \meas{X}{\algX}{\mu}\ and \meas{Y}{\algY}{\nu}

\vitem
	two functions defined below for every $E\in\algX\times\algY$ are $\sigma$-finite measures

	\bit
	\item
		$\pi'(E) = \int_X \nu\set{y\in Y}{(x,y)\in E} d\mu$
	\item
		$\pi''(E) = \int_Y \mu\set{x\in X}{(x,y)\in E} d\nu$
	\eit

\vitem
	for every measurable rectangle, $A\times B$, with $A\in\algX$ and $B\in\algY$

	\begin{eqn}
%	\[
		\pi'(A\times B)
		=
		\pi''(A\times B)
		=
		\mu(A) \nu(B)
%	\]
	\end{eqn}

\item []
	(use conventions in page~\pageref{page:Some conventions} for extended real values)

\vvitem
	indeed, $\pi'(E)=\pi''(E)$ for every $E\in\algX\times\algY$; let $\pi=\pi'=\pi''$

\vitem $\pi$ is
	\bit
	\item
		called \define{product measure}%
		\index{product probability spaces!product measure}%
		\index{product measure!product probability spaces}
		and denoted by \define{$\mu\times \nu$}
	\item
		$\sigma$-finite measure

	\item
		\emph{only} measure such that $\pi(A\times B) =\mu(A) \nu(B)$ for every measurable rectangle
	\eit

\eit


\myfoilhead{Fubini's theorem}%
	\index{product probability spaces!Fubini's theorem}%
	\index{Fubini's theorem!product probability spaces}%
	\index{Fubini, Guido!Fubini's theorem!product probability spaces}

\bit
\item
	suppose two $\sigma$-finite measure spaces, \meas{X}{\algX}{\mu}\ and \meas{Y}{\algY}{\nu}\
	- define
	\bit
	\item
		$X_0 = \set{x\in X}{\int_Y |f(x,y)|d\nu < \infty}\subset X$
	\item
		$Y_0 = \set{y\in Y}{\int_X |f(x,y)|d\nu < \infty}\subset Y$
	\eit

\vitem
	\name{Fubini's theorem -}%
		\index{product probability spaces!Fubini's theorem}%
		\index{Fubini's theorem!product probability spaces}%
		\index{Fubini, Guido!Fubini's theorem!product probability spaces}
	for nonnegative measurable function, $f$,
	following are measurable
	\wrt\ \algX\ and \algY\ respectively

	\begin{eqn}
%	\[
		g(x) = \int_Y f(x,y)d\nu,\ \
		h(y) = \int_X f(x,y)d\mu
%	\]
	\end{eqn}

	and following holds

	\begin{eqn}
%	\[
		\int_{X\times Y} f(x,y) d\pi
			=
		\int_X \left(\int_Y f(x,y) d\nu\right)d\mu
			=
		\int_Y \left(\int_X f(x,y) d\mu\right)d\nu
%	\]
	\end{eqn}

\vitem [--]
	for $f$, (not necessarily nonnegative) integrable function \wrt\ $\pi$\
	\bit
	\item
		$\mu(X\sim X_0) = 0$, $\nu(Y\sim Y_0)=0$
	\item
		$g$ and $h$ are finite measurable on $X_0$ and $Y_0$ respectively
	\item (above) equalities of \emph{double integral} holds
	\eit
\eit


\titlefoil{Random Variables}{Random Variables}

\myfoilhead{Random variables}
\pagelabel{page:random-variables}

\bit
\item [--]
	for probability space, \meas{\Omega}{\algk{F}}{P},

\vitem
	measurable function (\wrt\ $\algF/\algR$), $X:\Omega \to \reals$,
	called \define{random variable}%
		\index{random variables}

\vitem
	measurable function (\wrt\ $\algF/\algR^n$), $X:\Omega \to \reals^n$,
	called \define{random vector}%
		\index{random vectors}
		\index{random variables!random vectors}

	\bit
	\item
		when expressing $X(\omega)=(X_1(\omega), \ldots, X_n(\omega))$,
		$X$ is measurable \iaoi\ every $X_i$ is measurable
	\item
		thus, $n$-dimensional random vaector is simply
		$n$-tuple of random variables
	\eit

\vitem
	smallest $\sigma$-algebra \wrt\ which $X$ is measurable,
	called \define{$\sigma$-algebra generated by $X$}%
			\index{random variables!$\sigma$-algebra generated by}%
			\index{$\sigma$-algebra!generated by random variables}%
			\index{generated by!$\sigma$-algebra!by random variables}
	and denoted by \define{$\sigma(X)$}


	\bit
	\vitem
		$\sigma(X)$ consists exactly of sets, \set{\omega\in \Omega}{X(\omega)\in H}, for $H\in\algR^n$
	\vitem
		random variable, $Y$, is measurable \wrt\ $\sigma(X)$
			\iaoi\
		exists measurable function, $f:\reals^n\to\reals$
		such that
		$Y(\omega) = f(X(\omega))$ for all $\omega$,
		\ie,
		$Y=f\circ X$
	\eit
\eit


\myfoilhead{Probability distributions for random variables}

\bit
\item
	probability measure on \reals, $\mu = PX^{-1}$, \ie,
	\[
		\mu(A) = P(X\in A) \mbox{ for } A \in \algR
	\]
	called \define{distribution} or \define{law} of random variable, $X$%
		\index{random variables!distribution}%
		\index{random variables!law}
		\index{distribution!probability}

\vitem
	function, $F:\reals\to[0,1]$, defined by
	\[
		F(x) = \mu(-\infty, x] = P(X\leq x)
	\]
	called \define{distribution function} or \define{cumulative distribution function (CDF)} of $X$%
		\index{random variables!distribution functions}%
		\index{random variables!cumulative distribution function (CDF)}%
		\index{random variables!CDF}%
		\index{distribution functions!probability}%
		\index{cumulative distribution function (CDF)}%
		\index{CDF}%
		\index{probability distribution functions}

\vitem Borel set, $S$, with $P(S)=1$, called \define{support}%
		\index{random variables!support}

\vitem random variable, its distribution, its distribution function,
	said to be \define{discrete}%
		\index{random variables!discrete}
	when has \emph{countable} support
\eit


\myfoilhead{Probability distribution of mappings of random variables}

\bit
\item for measurable $g:\reals\to\reals$,
	\[
		\left(
			\forall A\in\algR
		\right)
		\left(
			\prob{g(X)\in A} = \prob{X \in g^{-1}(A)} = \mu (g^{-1}(A))
		\right)
	\]
	hence, $g(X)$ has distribution of $\mu g^{-1}$%
		\index{random variables!distribution functions!mappings}%
\eit


\myfoilhead{Probability density for random variables}

\bit
\item
	Borel function, $f: \reals\to\preals$, satisfying
	\[
		\left(
			\forall A \in \algR
		\right)
		\left(
			\mu(A) = P(X\in A) = \int_A f(x) dx
		\right)
	\]
	called \define{density} or \define{probability density function (PDF)}
	of random variable%
		\index{random variables!density}%
		\index{random variables!probability density function (PDF)}%
		\index{random variables!PDF}%
		\index{density}%
		\index{probability density function (PDF)}%
		\index{PDF}

\vitem
	above is equivalent to
	\[
		\left(
			\forall a < b \in \reals
		\right)
		\left(
			\int_a^b f(x) dx = P(a<X\leq b) = F(b) - F(a)
		\right)
	\]

\item []
	(refer to statement on page~\pageref{page:probability-measures-agreeing-on-P-agree-on-sigma-P})
	\bit
		\item note, though, $F$ does not need to differentiate to $f$ everywhere;
			only $f$ required to integrate properly
		\item if $F$ does differentiate to $f$ and $f$ is continuous,
			\emph{fundamental theorem of calculus} implies
			$f$ indeed is density for $F$
	\eit
\eit


\myfoilhead{Probability distribution for random vectors}

\bit
\item
	(similarly to random variables) probability measure on $\reals^n$, $\mu = PX^{-1}$,
		\ie,
	\[
		\mu(A) = P(X\in A) \mbox{ for } A \in \algk{B}^k
	\]
	called \define{distribution} or \define{law} of random vector, $X$%
		\index{random vectors!distribution}%
		\index{random vectors!law}%
		\index{probability distribution}%
		\index{distribution!probability}

\vitem
	function, $F:\reals^k\to[0,1]$, defined by
	\[
		F(x) = \mu S_x = P(X\preceq x)
	\]
	where
	\[
		S_x = \set{\omega\in \Omega}{X(\omega)\preceq x}
		= \set{\omega\in \Omega}{X_i(\omega)\leq x_i}
	\]
	called \define{distribution function} or \define{cumulative distribution function (CDF)} of $X$%
		\index{random vectors!distribution functions}%
		\index{random vectors!cumulative distribution function (CDF)}%
		\index{random vectors!CDF}%
		\index{distribution functions!probability}%
		\index{probability distribution functions}%
		\index{cumulative distribution function (CDF)}%
		\index{CDF}

\vitem (similarly to random variables) random vector, its distribution, its distribution function,
	said to be \define{discrete}%
		\index{random vectors!discrete}
	when has \emph{countable} support
\eit


\myfoilhead{Marginal distribution for random vectors}

\bit
\item
	(similarly to random variables) for measurable $g:\reals^n\to\reals^m$
	\[
		\left(
			\forall A\in\algR^{m}
		\right)
		\left(
			\prob{g(X)\in A}
				=
			\prob{X \in g^{-1}(A)}
				 =
			\mu(g^{-1}(A))
		\right)
	\]
	hence, $g(X)$ has distribution of $\mu g^{-1}$

\vitem
	for $g_i:\reals^n\to\reals$ with $g_i(x) = x_i$
	\[
		\left(
			\forall A\in\algR
		\right)
		\left(
			\prob{g(X)\in A}
				=
			\prob{X_i \in A}
		\right)
	\]

\vitem
	measure, $\mu_i$, defined by $\mu_i(A) = \prob{X_i\in A}$,
	called \define{($i$-th) marginal distribution of $X$}%
		\index{random vectors!marginal distribution}
		\index{marginal distribution!random vectors!}

\vitem
	for $\mu$ having density function, $f:\reals^n\to\preals$,
	density function of marginal distribution is
	\[
		f_i(x) = \int_{\algR^{n-1}}
			f(x_{-i}) d \mu_{-i}
	\]
	where $x_{-i} = (x_1,\ldots,x_{i-1}, x_{i+1}, \ldots, x_n)$
	and similarly for $d\mu_{-i}$
\eit


\myfoilhead{Independence of random variables}%
	\index{random variables!independence}%
	\index{independence!random variables}

\bit
\item
	random variables, $X_1$, \ldots, $X_n$,
	with independent $\sigma$-algebras generated by them,
%	(\ie, $\sigma(X_1)$, \ldots, $\sigma(X_n)$, being independent),
	said to be \define{independent}%
		\index{random variables!independence}%
		\index{independence!random variables}

\item []
	(refer to page~\pageref{probability spaces!independence!of collection of classes of events} for
		independence of collections of subsets)
	\bit
	\item
		because $\sigma(X_i) = X_i^{-1}(\algR)=\set{X_i^{-1}(H)}{H\in\algR}$,
		independent \iaoi\
		\[
			\left(
				\forall H_1, \ldots, H_n\in \algR
			\right)
			\left(
				P\left(X_1\in H_1,\ldots, X_n\in H_n\right)
				= \prod P\left(X_i\in H_i\right)
			\right)
		\]
		\ie,
		\[
			\left(
				\forall H_1, \ldots, H_n\in \algR
			\right)
			\left(
				P\left(\bigcap X_i^{-1}(H_i)\right)
				= \prod P\left(X_i^{-1}(H_i)\right)
			\right)
		\]

	\eit
\eit


\myfoilhead{Equivalent statements of independence of random variables}

\bit
\item for random variables, $X_1$, \ldots, $X_n$,
	having $\mu$ and $F:\reals^n\to[0,1]$ as their distribution and CDF,
	with each $X_i$ having $\mu_i$ and $F_i:\reals\to[0,1]$ as its distribution and CDF,
	following statements are \emph{equivalent}%
		\index{random variables!independence}%
		\index{independence!random variables}%
		\index{random variables!independence!equivalent statements}

	\bit
	\vitem
		$X_1,\ldots,X_n \mbox{ are independent}$

	\vitem
		$\left( \forall H_1, \ldots, H_n\in \algR \right) \left( P\left(\bigcap X_i^{-1}(H_i)\right) = \prod P\left(X_i^{-1}(H_i)\right) \right)$

	\vitem
		$\left( \forall H_1,\ldots,H_n \in \algR \right) \left( P(X_1\in H_1,\ldots, X_n\in H_n) = \prod P(X_i \in H_i) \right) $

	\vitem
		$\left( \forall x\in \reals^n \right) \left( P(X_1\leq x_1,\ldots, X_n\leq x_n) = \prod P(X_i \leq x_i) \right) $

	\vitem
		$\left( \forall x \in \reals^n \right) \left( F(x) = \prod F_i(x_i) \right) $

	\vitem
		$\mu = \mu_1 \times \cdots \times \mu_n $

	\vitem
		$\left( \forall x \in \reals^n \right) \left( f(x) = \prod f_i(x_i) \right) $
	\eit
\eit


\myfoilhead{Independence of random variables with separate $\sigma$-algebra}

\bit
\item
	[--] given probability space, \meas{\Omega}{\algk{F}}{P}


\item
	random variables, $X_1$, \ldots, $X_n$,
	each of which is measurable \wrt\ each of $n$ independent $\sigma$-algebras,
	$\algk{G}_1\subset \algF$, \ldots, $\algk{G}_n\subset \algF$
	respectively,
	are independent%
		\index{random variables!independence}%
		\index{independence!random variables}
\eit


\myfoilhead{Independence of random vectors}

\bit
\item for random vectors, $X_1:\Omega\to\reals^{d_1}$, \ldots, $X_n:\Omega\to\reals^{d_n}$,
	having $\mu$ and $F:\reals^{d_1}\times\cdots\times\reals^{d_n}\to[0,1]$ as their distribution and CDF,
	with each $X_i$ having $\mu_i$ and $F_i:\reals^{d_i}\to[0,1]$ as its distribution and CDF,
	following statements are \emph{equivalent}%
		\index{random vectors!independence}%
		\index{independence!random vectors}%
		\index{random vectors!independence!equivalent statements}

	\bit
	\vitem
		$X_1,\ldots,X_n \mbox{ are independent}$

	\vitem
		$
			\left( \forall H_1\in\algR^{d_1}, \ldots, H_n\in \algR^{d_n} \right)
			\left( P\left(\bigcap X_i^{-1}(H_i)\right) = \prod P\left(X_i^{-1}(H_i)\right) \right)
		$

	\vitem
		$
			\left( \forall H_1\in\algR^{d_1}, \ldots, H_n\in \algR^{d_n} \right)
			\left( P(X_1\in H_1,\ldots, X_n\in H_n) = \prod P(X_i \in H_i) \right)
		$

	\vitem
		$
			\left( \forall x_1\in \reals^{d_1},\ldots,x_n\in\reals^{d_n} \right)
			\left( P(X_1\preceq x_1,\ldots, X_n\preceq x_n) = \prod P(X_i \preceq x_i) \right)
		$

	\vitem
		$
			\left( \forall x_1\in \reals^{d_1},\ldots,x_n\in\reals^{d_n} \right)
			\left( F(x_1,\ldots,x_n) = \prod F_i(x_i) \right)
		$

	\vitem
		$\mu = \mu_1 \times \cdots \times \mu_n $

	\vitem
		$
			\left( \forall x_1\in \reals^{d_1},\ldots,x_n\in\reals^{d_n} \right)
			\left( f(x_1,\ldots,x_n) = \prod f_i(x_i) \right)
		$
	\eit
\eit


\myfoilhead{Independence of infinite collection of random vectors}

\bit
\item
	infinite collection of random vectors
	for which every finite subcollection is independent,
	said to be \define{independent}
		\index{random vectors!independence!infinitely many}%
		\index{random variables!independence!infinitely many}%
		\index{independence!random vectors!infinitely many}%
		\index{independence!random variables!infinitely many}%

\vitem
	for independent (countable) collection of random vectors, $\seq{\seq{X_{ni}}_{i=1}^\infty}_{n=1}^\infty$,
	$\seq{\algk{F}_n}_{n=1}^\infty$\ with $\algk{F}_n = \sigma(\seq{X_{ni}}_{i=1}^\infty)$
	are independent
\eit


\myfoilhead{Probability evaluation for two independent random vectors}

\begin{mytheorem}{Probability evaluation for two independent random vectors}
	for independent random vectors, $X$ and $Y$,
	with distributions, $\mu$ and $\nu$, in $\reals^n$ and $\reals^m$ respectively
	$$
		\left(
			\forall B\in\algR^{n+m}
		\right)
		\left(
				\prob{(X,Y)\in B} = \int_{\reals^n} \prob{(x,Y)\in B} d\mu_X
		\right)
	$$
	and
	$$
		\left(
			\forall A\in\algR^{n}, B\in\algR^{n+m}
		\right)
		\left(
				\prob{X\in A, (X,Y)\in B} = \int_{A} \prob{(x,Y)\in B} d\mu_X
		\right)
	$$
\end{mytheorem}


\myfoilhead{Sequence of random variables}

\begin{mytheorem}{squence of random variables}
	for sequence of probability measures on \algR, \seq{\mu_n},
	exists probability space, \meas{X}{\Omega}{P},
	and sequence of independent random variables in \reals, \seq{X_n},
	such that each $X_n$ has $\mu_n$ as distribution
\end{mytheorem}


\myfoilhead{Expected values}

\begin{mydefinition}{expected values}%
	\index{expected values!random variables}%
	\index{random variables!expected values}
	for random variable, $X$, on \meas{\Omega}{\algF}{P},
	integral of $X$\ \wrt\ measure, $P$
	$$
		\Expect X
			=
		\int X dP
		 	=
		\int_\Omega X(\omega) dP
	$$
	called \define{expected value of $X$}
\end{mydefinition}

\vspace{-.2em}
\bit
\item
	$\Expect X$ is
	\bit
	\item
		always defined for nonnegative $X$
	\item
		for general case
		\bit
		\item [-]
			defined, or
		\item [-]
			$X$ has an expected value if either $\Expect X^+<\infty$ or $\Expect X^-<\infty$ or both,
			in which case, $\Expect X =\Expect X^+ - \Expect X^-$
		\eit
	\eit

\vitem
	$X$ is integrable \iaoi\ $\Expect |X| <\infty$

\vitem
	limits
	\bit
	\item
		if \seq{X_n}\ is dominated by integral random variable
			or
		they are uniformly integrable,
		$\Expect X_n$ converges to $\Expect X$
		if $X_n$ converges to $X$ in probability
	\eit

\eit


\myfoilhead{Markov and Chebyshev's inequalities}

\begin{myinequality}{Markov inequality}%
		\index{Markov inequality!random variables}%
		\index{random variables!Markov inequality}%
		\index{Markov, Andrey Andreyevich!Markov inequality!random variables}
	for random variable, $X$, on \meas{\Omega}{\algF}{P},
	$$
		\prob{X\geq \alpha} \leq \frac{1}{\alpha} \int_{X\geq \alpha} X d P \leq \frac{1}{\alpha} \Expect X
	$$
	for nonnegative $X$, hence
	$$
		\prob{|X|\geq \alpha} \leq \frac{1}{\alpha^n} \int_{|X|\geq \alpha} |X|^n d P \leq \frac{1}{\alpha^n} \Expect |X|^n
	$$
	for general $X$
\end{myinequality}

\begin{myinequality}{Chebyshev's inequality}%
		\index{Chebyshev's inequality!random variables}%
		\index{Chebyshev, Pafnuty!Chebyshev's inequality!random variables}%
		\index{random variables!Chebyshev's inequality}
	as special case of Markov inequality,
	$$
		\prob{|X-\Expect X|\geq \alpha}
			\leq
		\frac{1}{\alpha^2} \int_{|X-\Expect X|\geq \alpha} (X-\Expect X)^2 d P
			\leq
		\frac{1}{\alpha^2} \Var X
	$$
	for general $X$
\end{myinequality}


\myfoilhead{Jensen's, H\"{o}lder's, and Lyapunov's inequalities}

\begin{myinequality}{Jensen's inequality}%
		\index{Jensen's inequality!for random variables}%
		\index{Jensen, Johan Ludwig William Valdemar!Jensen's inequality!for random variables}%
		\index{random variables!Jensen's inequality}
	for random variable, $X$, on \meas{\Omega}{\algF}{P},
		and convex function, $\varphi$
	$$
		\varphi\left(\Expect X\right)
		\prob{X\geq \alpha} \leq \frac{1}{\alpha} \int_{X\geq \alpha} X d P \leq \frac{1}{\alpha} \Expect X
	$$
\end{myinequality}

\begin{myinequality}{Holder's inequality}%
		\index{H\"{o}lder's inequality!random variables}%
		\index{H\"{o}lder, Ludwig Otto!H\"{o}lder's inequality!random variables}
		\index{random variables!H\"{o}lder's inequality}%
	for two random variables, $X$ and $Y$, on \meas{\Omega}{\algF}{P},
	and $p,q\in(1,\infty)$ with $1/p+1/q=1$
	$$
		\Expect |XY| \leq
			\left(\Expect |X|^p\right)^{1/p}
			\left(\Expect |X|^q\right)^{1/q}
	$$
\end{myinequality}
\index{H\"{o}lder's inequality}
\index{H\"{o}lder, Ludwig Otto!H\"{o}lder's inequality}

\begin{myinequality}{Lyapunov's inequality}%
		\index{Lyapunov's inequality!random variables}%
		\index{Lyapunov, Aleksandr!Lyapunov's inequality!random variables}
		\index{random variables!Lyapunov's inequality}%
	for random variable, $X$, on \meas{\Omega}{\algF}{P},
	and $0<\alpha<\beta$
	$$
		\left(\Expect |X|^\alpha\right)^{1/\alpha}
			\leq
		\left(\Expect |X|^\beta\right)^{1/\beta}
	$$
\end{myinequality}

\bit
	\item note H\"{o}lder's inequality implies Lyapunov's inequality
\eit


\myfoilhead{Maximal inequalities}%
	\index{maximal inequalities}

\begin{mytheorem}{Kolmogorov's zero-one law}%
	\index{Kolmogorov, Andrey Nikolaevich!Kolmogorov's zero-one law}%
	\index{Kolmogorov's zero-one law!random variables}

	if $A\in\algF=\bigcap_{n=1}^\infty \sigma(X_n, X_{n+1},\ldots)$ for independent \seq{X_n},
	$$
		\prob{A} = 0 \vee \prob{A} = 1
	$$
\end{mytheorem}

-- define $S_n = \sum X_i$

\begin{myinequality}{Kolmogorov's maximal inequality}%
	\index{Kolmogorov's maximal inequality!random variables}%
 	\index{Kolmogorov, Andrey Nikolaevich!Kolmogorov's maximal inequality}

	for independent $\seq{X_i}_{i=1}^n$\ with $\Expect X_i =0$ and $\Var X_i<\infty$
	and $\alpha>0$
	$$
		\prob{\max S_i \geq \alpha} \leq \frac{1}{\alpha}\Var S_n
	$$

\end{myinequality}

\begin{myinequality}{Etemadi's maximal inequality}%
	\index{Etemadi's maximal inequality!random variables}%
	\index{Etemadi, Nasrollah!Etemadi's maximal inequality}

	for independent $\seq{X_i}_{i=1}^n$
	and $\alpha>0$
	$$
		\prob{\max |S_i| \geq 3\alpha} \leq 3 \max \prob{|S_i|\geq\alpha}
	$$

\end{myinequality}


\myfoilhead{Moments}

\begin{mydefinition}{moments and absolute moments}%
	\index{moments!random variables}%
	\index{absolute moments!random variables}%
	\index{random variables!moments}%
	\index{random variables!absolute moments}

	for random variable, $X$, on \meas{\Omega}{\algF}{P},
	integral of $X$\ \wrt\ measure, $P$
	$$
		\Expect X^n
			=
		\int x^k d\mu
			=
		\int x^k dF(x)
	$$
	called \define{$k$-th moment} of $X$ or $\mu$ or $F$,
	and
	$$
		\Expect |X|^n
			=
		\int |x|^k d\mu
			=
		\int |x|^k dF(x)
	$$
	called \define{$k$-th absolute moment} of $X$ or $\mu$ or $F$
\end{mydefinition}

\bit
\vitem
	if $\Expect |X|^n<\infty$, $\Expect |X|^k<\infty$ for $k<n$

\vitem
	$\Expect X^n$ defined only when $\Expect|X|^n<\infty$
\eit
\vfill


\myfoilhead{Moment generating functions}

\begin{mydefinition}{moment generating function}%
	\index{moment generating functions!random variables}%
	\index{random variables!moment generating functions}

	for random variable, $X$, on \meas{\Omega}{\algF}{P},
	$M:\complexes \to \complexes$ defined by
	$$
		M(s)
			=
		\Expect \left( e^{sX} \right)
			=
		\int e^{sx} d\mu
			=
		\int e^{sx} dF(x)
	$$
	called \define{moment generating function of $X$}
\end{mydefinition}
\bit
%\item
%	for $x\in\reals$,
%	\bit
	\item
		$n$-th derivative of $M$ \wrt\ $s$ is
		$
			M^{(n)}(s) = \frac{d^n}{ds^n} F(s) = \Expect \left(X^ne^{sX}\right) = \int xe^{sx} d\mu
		$
	\item
		thus,
		$n$-th derivative of $M$ \wrt\ $s$ at $s=0$ is $n$-th moment of $X$
		$$
			M^{(n)}(0) = \Expect X^n
		$$
%	\eit

\vitem
	for independent random variables, $\seq{X_i}_{i=1}^n$, moment generating function of $\sum X_i$
	$$
		\prod M_i(s)
	$$
\eit


\yesnoexec{\showincomplete}{%
\myfoilhead{Characteristic functions of random variables}%
	\idxtodo{3 - Characteristic functions of random variables}

\bit
\item
	\bit
	\item
	\eit

\vitem
	\bit
	\item
	\eit
\eit
}{}


\titlefoil{Convergence of Random Variables}{Convergence of Random Variables}

\myfoilhead{Convergences of random variables}%
	\index{random variables!convergence}%
	\index{convergence!of random variables}

\begin{mydefinition}{convergence with probability $1$}%
	\index{random variables!convergence with probability $1$}%
	\index{convergence!with probability $1$}%
	\index{convergence!of random variables}

	random variables, \seq{X_n},
	with
	$$
		\prob{\lim X_n = X}
		= P(\set{\omega \in \Omega}{\lim X_n(\omega) = X(\omega)})
		= 1
	$$
	said to \define{converge to $X$ with probability $1$}
	and denoted by \define{$X_n\to X$ a.s.}
\end{mydefinition}

\begin{mydefinition}{convergence in probability}%
	\index{random variables!convergence in probability}%
	\index{convergence!in probability}%
	\index{convergence!of random variables}

	random variables, \seq{X_n},
	with
	$$
		\left(
			\forall \epsilon>0
		\right)
		\left(
			\lim \prob{|X_n-X|>\epsilon} = 0
		\right)
	$$
	said to \define{converge to $X$ in probability}
\end{mydefinition}

\begin{mydefinition}{weak convergence}%
	\index{random variables!weak convergence of distributions}%
	\index{convergence!weak convergence of distributions}%
	\index{convergence!of distributions}

	distribution functions, \seq{F_n}, with
	$$
		\left(
			\forall x \mbox{ in domain of }F
		\right)
		\left(
			\lim F_n(x) = F(x)
		\right)
	$$
	said to \define{converge weakly to distribution function, $F$,}
	and denoted by \define{$F_n \Rightarrow F$}
\end{mydefinition}

\begin{mydefinition}{converge in distribution}
	\index{random variables!convergence in distribution}%
	\index{convergence!in distribution}%
	\index{convergence!of random variables}

	When $F_n\Rightarrow F$,
	associated random variables, \seq{X_n},
	said to \define{converge in distribution} to $X$, associated with $F$,
	and denoted by \define{$X_n \Rightarrow X$}
\end{mydefinition}

\begin{mydefinition}{weak convergence of measures}%
	\index{random variables!weak convergence of measures}%
	\index{convergence!weak convergence of measures}

	for measures on \measu{\reals}{\algR}, \seq{\mu_n}, associated with distribution functions, \seq{F_n}, respectively,
	and measure on \measu{\reals}{\algR}, $\mu$, associated with distribution function, $F$,
	we denote
	$$
		\mu_n \Rightarrow \mu
	$$
	if
	$$
		\left(
			\forall A = (-\infty, x] \mbox{ with } x\in\reals
		\right)
		\left(
			\lim \mu_n(A) = \mu(A)
		\right)
	$$
\end{mydefinition}

\bit
\item
	indeed, if above equation holds for $A=(-\infty, x)$,
	it holds for many other subsets
\eit




\myfoilhead{Relations of different types of convergences of random variables}%
	\index{random variables!relations of convergences}%
	\index{convergence!relations of}%
	\index{convergence!of random variables}


\begin{myproposition}{relations of convergence of random variables}
	convergence with probability $1$ implies convergence in probability,
	which implies $X_n\Rightarrow X$, \ie\
	\begin{eqnarray*}
		&&
	X_n \to X \mbox{ a.s., \ie, } X_n \mbox{ converge to } X \mbox{ with probability $1$}
		\\
		&\Rightarrow&
	X_n \mbox{ converge to } X \mbox{ in probability}
		\\
		&\Rightarrow&
	X_n \Rightarrow X \mbox{, \ie, } X_n \mbox{ converge to } X \mbox{ in distribution},
	\end{eqnarray*}
\end{myproposition}


\myfoilhead{Necessary and sufficient conditions for convergence of probability}%
	\index{random variables!necessary and sufficient conditions for convergences in probability}%
	\index{convergence!necessary and sufficient conditions for convergence in probability}%
	\index{convergence!of random variables}

	$$
		{X_n}\ \mbox{ converge in probability}
	$$

	\iaoi\

	$$
		\left(
			\forall \epsilon>0
		\right)
		\left(
			\prob{|X_n-X|>\epsilon\mbox{ i.o}}
				=
			\prob{\limsup |X_n-X| > \epsilon } = 0
		\right)
	$$

	\iaoi\

	$$
		\left(
			\forall \mbox{ subsequence }\seq{X_{n_k}}
		\right)
		\left(
			\exists \mbox{ its subsequence }\seq{X_{n_{k_l}}} \mbox{ converging to } f \mbox{ with probability } 1
		\right)
	$$


\myfoilhead{Necessary and sufficient conditions for convergence in distribution}
	\index{random variables!necessary and sufficient conditions for convergences in distribution}%
	\index{convergence!necessary and sufficient conditions for convergence in distribution}%
	\index{convergence!of random variables}

	$$
		X_n\Rightarrow X, \mbox{\ie, $X_n$ converge in distribution}
	$$

		\iaoi

	$$
		F_n\Rightarrow F, \mbox{\ie, $F_n$ converge weakly}
	$$

		\iaoi

	$$
		\left(
			\forall A = (-\infty, x] \mbox{ with } x\in\reals
		\right)
		\left(
			\lim \mu_n(A) = \mu(A)
		\right)
	$$

		\iaoi

	$$
		\left(
			\forall x \mbox{ with } \prob{X=x} = 0
		\right)
		\left(
			\lim \prob{X_n\leq x} = \prob{X\leq x}
		\right)
	$$


\myfoilhead{Strong law of large numbers}

-- define $S_n = \sum_{i=1}^n X_i$

\begin{mytheorem}{strong law of large numbers}%
		\index{strong law of large numbers!random variables}%
		\index{random variables!strong law of large numbers}
	for sequence of independent and identically distributed (i.i.d.) random variables
	with finite mean, \seq{X_n}
	$$
		\frac{1}{n} S_n \to \Expect X_1
	$$
	with probability $1$
\end{mytheorem}

\bit
	\item strong law of large numbers also called \define{Kolmogorov's law}%
		\index{Kolmogorov's law!random variables}%
		\index{Kolmogorov, Andrey Nikolaevich!Kolmogorov's law}%
		\index{random variables!Kolmogorov's law}
\eit

\begin{mycorollary}{strong law of large numbers}
	for sequence of independent and identically distributed (i.i.d.) random variables
	with $\Expect X_1^- < \infty$ and $\Expect X_1^+ = \infty$ (hence, $\Expect X = \infty$)
	$$
		\frac{1}{n} S_n \to \infty
	$$
	with probability $1$
\end{mycorollary}


\myfoilhead{Weak law of large numbers}

-- define $S_n = \sum_{i=1}^n X_i$

\begin{mytheorem}{weak law of large numbers}%
		\index{weak law of large numbers!random variables}%
		\index{random variables!weak law of large numbers}
	for sequence of independent and identically distributed (i.i.d.) random variables
	with finite mean, \seq{X_n}
	$$
		\frac{1}{n} S_n \to \Expect X_1
	$$
	in probability
\end{mytheorem}

\bit
	\item
		because convergence with probability $1$ implies convergence in probability
%		(\theoremname~\ref{theorem:conditions for convergence of random variables}),
		(\propositionname~\ref{proposition:relations of convergence of random variables}),
		strong law of large numbers
		implies
		weak law of large numbers
\eit


\myfoilhead{Normal distributions}%
	\index{normal distributions!random variables}%
	\index{random variables!normal distributions}

-- assume probability space, \meas{\Omega}{\algF}{P}\

\begin{mydefinition}{normal distributions}%
	\index{normal distributions}%
	\index{random variables!normal distributions}

	Random variable, $X:\Omega\to\reals$, with
	$$
		\left(
			A\in\algR
		\right)
		\left(
			\prob{X\in A} = \frac{1}{\sqrt{2\pi}\sigma} \int_A e^{-(x-c)^2/2} d\mu
		\right)
	$$
	where $\mu=PX^{-1}$
	for some $\sigma>0$ and $c\in\reals$,
	called \define{normal distribution}
	and denoted by \define{$X \sim \normal(c,\sigma^2)$}
\end{mydefinition}

\bit
\item [--]
	note $\Expect X=c$ and $\Var X=\sigma^2$
\item [--]
	called \define{standard normal distribution}
	when $c=0$ and $\sigma=1$%
		\index{standard normal distribution}%
		\index{random variables!standard normal distribution}
\eit


\myfoilhead{Multivariate normal distributions}%
	\index{multivariate normal distributions}%
	\index{random variables!multivariate normal distributions}

-- assume probability space, \meas{\Omega}{\algF}{P}\

\begin{mydefinition}{multivariate normal distributions}%
	\index{multivariate normal distributions}%
	\index{random variables!multivariate normal distributions}

	Random variable, $X:\Omega\to\reals^n$, with
	$$
		\left(
			A\in\algR^n
		\right)
		\left(
			\prob{X\in A} = \frac{1}{\sqrt{(2\pi)^n}\sqrt{\det \Sigma}} \int_A e^{-(x-c)^T\Sigma^{-1}(x-c)/2} d\mu
		\right)
	$$
	where $\mu=PX^{-1}$
	for some $\Sigma\succ0\in\posdefset{n}$ and $c\in\reals^n$,
	called \define{($n$-dimensional) normal distribution},
	and denoted by \define{$X \sim \normal(c,\Sigma)$}
\end{mydefinition}

\bit
\item [--]
	note that $\Expect X=c$ and covariance matrix is $\Sigma$
\eit


\myfoilhead{Lindeberg-L\'{e}vy theorem}%
	\index{Lindeberg-L\'{e}vy theorem}%
	\index{random variables!Lindeberg-L\'{e}vy theorem}%
	\index{Lindeberg, Jarl Waldemar!Lindeberg-L\'{e}vy theorem}%
	\index{L\'{e}vy, Paul!Lindeberg-L\'{e}vy theorem}

-- define $S_n = \sum^n X_i$

\begin{mytheorem}{Lindeberg-Levy theorem}%
	\index{Lindeberg-L\'{e}vy theorem}%
	\index{random variables!Lindeberg-L\'{e}vy theorem}%
	\index{Lindeberg, Jarl Waldemar!Lindeberg-L\'{e}vy theorem}%
	\index{L\'{e}vy, Paul!Lindeberg-L\'{e}vy theorem}

	for independent random variables, \seq{X_n}, having same distribution with expected value, $c$, and same variance, $\sigma^2<\infty$,
	${(S_n - nc)}/{\sigma\sqrt{n}}$ converges to standard normal distribution in distribution, \ie,
	$$
		\frac{S_n - nc}{\sigma\sqrt{n}} \Rightarrow N
	$$
	where $N$ is standard normal distribution
\end{mytheorem}

\bit
\item [--]
	\theoremname~\ref{theorem:Lindeberg-Levy theorem}\ implies
	\[
		S_n / n \Rightarrow c
	\]
\eit


\myfoilhead{Limit theorems in $\reals^n$}
	\index{limit theorems!random variables}%
	\index{random variables!limit theorems}

\begin{mytheorem}{equivalent statements to weak convergence}
	each of following statements are equivalent to
	weak convergence of measures, \seq{\mu_n}, to $\mu$,
	on measurable space, \measu{\reals^k}{\algR^k}\

	\bit
		\item $\lim \int f d\mu_n = \int f d\mu$ for every bounded continuous $f$
		\item $\limsup \mu_n(C) \leq \mu(C)$ for every closed $C$
		\item $\liminf \mu_n(G) \geq \mu(G)$ for every open $G$
		\item $\lim \mu_n(A) = \mu(A)$ for every $\mu$-continuity $A$
	\eit
\end{mytheorem}

\begin{mytheorem}{convergence in distribution of random vector}
	for random vectors, \seq{X_n}, and random vector, $Y$, of $k$-dimension,
	$X_n\Rightarrow Y$, \ie, $X_n$ converge to $Y$ in distribution
		\iaoi\
	$$
		\left(
			\forall z\in \reals^k
		\right)
		\left(
			z^T X_n \Rightarrow z^T Y
		\right)
	$$

\end{mytheorem}


\myfoilhead{Central limit theorem}%
	\index{central limit theorem}%
	\index{random variables!central limit theorem}
	\index{random vectors!central limit theorem}

-- assume probability space, \meas{\Omega}{\algF}{P}\ and define $\sum^n X_i = S_n$\


\begin{mytheorem}{central limit theorem}
	for random variables, \seq{X_n}, having same distributions with $\Expect X_n = c\in\reals^k$
	and positive definite covariance matrix, $\Sigma\succ0\in\mathcalfont{S}_k$,
	\ie, $\Expect(X_n-c)(X_n-c)^T = \Sigma$,
	where $\Sigma_{ii} < \infty$ (hence $\Sigma \prec M I_n$ for some $M\in\ppreals$ due to Cauchy-Schwarz inequality),
	$$
		(S_n -nc)/\sqrt{n} \mbox{ converges in distribution to } Y
	$$
	where $Y \sim \normal(0,\Sigma)$
\end{mytheorem}

\proofref{central limit theorem}



\myfoilhead{Convergence of random series}%
	\index{convergence!of random series}

\bit
\item
	for independent \seq{X_n}, probability of $\sum X_n$ converging is either $0$ or $1$
\item
	below characterize two cases in terms of distributions of individual $X_n$
	-- XXX: diagram
	\idxtodo{CANCELED - 2025 0414 - 2 - diagram for convergence of random series}
\eit

\begin{mytheorem}{convergence with probability 1 for random series}
	for independent \seq{X_n}\ with $\Expect X_n=0$ and $\Var X_n < \infty$
	$$
		\sum X_n \mbox{ converges with probability $1$}
	$$
\end{mytheorem}

\begin{mytheorem}{convergence conditions for random series}
	for independent \seq{X_n},
		$\sum X_n$ converges with probability $1$
		\iaoi\
		they converges in probability
\end{mytheorem}

\vfill
-- define trucated version of $X_n$ by $X_n^{(c)}$, \ie, $X_n I_{|X_n|\leq c}$

\begin{mytheorem}{convergence conditions for truncated random series}\
	for independent \seq{X_n},
	\begin{eqnarray*}
%	&&
	\lefteqn{
		\sum X_n
		\mbox{ converge with probability $1$}
	}
	\\
	&&
		\mbox{if all of }
		\sum \prob{|X_n|>c},
		\sum \Expect(X_n^{(c)}),
		\sum \Var(X_n^{(c)})
		\mbox{ converge for some }c>0
	\end{eqnarray*}
\end{mytheorem}


\yesnoexec{\showincomplete}{%
\myfoilhead{Fundamental theorems for weak convergence}%
	\idxtodo{2 - Fundamental theorems for weak convergence}

\bit
\item
	\bit
	\item
	\eit

\vitem
	\bit
	\item
	\eit
\eit
}{}


\yesnoexec{\showincomplete}{%
\myfoilhead{Helly's theorem for weak convergence}
	\idxtodo{4 - Helly's theorem}

\bit
\item
	\bit
	\item
	\eit

\vitem
	\bit
	\item
	\eit
\eit
}{}


\yesnoexec{\showincomplete}{%
\myfoilhead{Integration to limit for weak convergence}
	\idxtodo{4 - Integration to limist for weak convergence}

\bit
\item
	\bit
	\item
	\eit

\vitem
	\bit
	\item
	\eit
\eit
}{}


\yesnoexec{\showincomplete}{%
\titlefoil{Conditional Probability}{Conditional Probability}
	\idxtodo{4 - Conditional Probability}
}{}

\yesnoexec{\showincomplete}{%
\titlefoil{Stochastic Processes}{Stochastic Processes}
	\idxtodo{5 - Stochastic Processes}
}{}


\yesnoexec{\showincomplete}{%
\myfoilhead{Poisson process: XXX}
	\idxtodo{2 - Poisson process}

\bit
\item
	\bit
	\item
	\eit

\vitem
	\bit
	\item
	\eit
\eit
}{}


\yesnoexec{\showincomplete}{%
\myfoilhead{Eegodic random processes: XXX}%
	\idxtodo{2 - Eegodic random processes}

\bit
\item
	\bit
	\item
	\eit

\vitem
	\bit
	\item
	\eit
\eit
}{}


\yesnoexec{\showincomplete}{%
\myfoilhead{Brownian motion}%
	\idxtodo{4 - Brownian motion}

\bit
\item
	\bit
	\item
	\eit

\vitem
	\bit
	\item
	\eit
\eit
}{}


\yesnoexec{\showincomplete}{%
\myfoilhead{Martingales}%
	\idxtodo{5 - Martingales}

\bit
\item
	\bit
	\item
	\eit

\vitem
	\bit
	\item
	\eit
\eit
}{}


%\myfoilhead{Glivenko-Cantelli theorem}
%
%\begin{mydefinition}{}
%	for sequence of random variables, $\seq{X_k}_{i=1}^n$,
%	distribution function, $F_n(x,\omega)$\ with jump of $n^{-1}$ at each $X_k(\omega)$ defined by
%	\[
%		F_n(x,\omega) = \frac{1}{n} \sum_{k=1}^n I_{(-\infty,x]} (X_k(\omega))
%	\]
%	called \define{empirical distribution function for random variables}%
%		\index{random variables!empirical distribution function}%
%		\index{empirical distribution function!random variables}
%\end{mydefinition}
%
%\begin{mytheorem}{Glivenko-Cantelli theorem}%
%		\index{Glivenko-Cantelli theorem}%
%		\index{Glivenko-Cantelli theorem!random variables}%
%		\index{Glivenko!Glivenko-Cantelli theorem}%
%		\index{Cantelli, Francesco Paolo!Glivenko-Cantelli theorem}%
%		\index{random variables!Glivenko-Cantelli theorem}
%	for \seq{X_n}\ with common distribution, $F$,
%	$$
%		D_n(\omega) = \sup_x |F_n(x,\omega) - F(x)|
%	$$
%	converges to $0$ with probability $1$
%\end{mytheorem}
%
%

}{}


\yesnoexec{\cvxopt}{%
\TITLEFOIL{Convex Optimization}{Convex Optimization}
\nocite{BV:04}

\renewcommand\fobj{\ensuremath{f}}
\renewcommand\fie{\ensuremath{q}}
\renewcommand\feq{\ensuremath{h}}
\renewcommand\tildefobj{\tilde{\fobj}}
\renewcommand\tildefie{\tilde{\fie}}
\renewcommand\tildefeq{\tilde{\feq}}

\renewcommand\xdomain{\ensuremath{X}}
\renewcommand\xobj{\ensuremath{F}}
\renewcommand\xie{\ensuremath{Q}}
\renewcommand\xeq{\ensuremath{H}}

% optimization problems

\newcommand\optprob[5]{
\ifthenelse{\equal{#1}{primal}}{#2}
 {\ifthenelse{\equal{#1}{dual fcn}}{#3}
  {\ifthenelse{\equal{#1}{dual}}{#4}
   {\ifthenelse{\equal{}{}}{#5}
    {\errmessage{{#1} should be either "primal" or "dual fcn" or "dual" or "simplied dual"}}
   }
  }
 }
}

\newcommand\lssollineqs[1]{\optprob{#1}{%
% primal problem
\begin{array}{ll}
	\mbox{minimize} &
		x^Tx
	\\
	\mbox{subject to} &
		Ax=b
\end{array}
}{%
% dual function
g(\nu) = -\frac{1}{4} \nu^TAA^T\nu - b^T\nu
}{%
% dual problem
\begin{array}{ll}
	\mbox{maximize} &
		g(\nu) = -\frac{1}{4} \nu^TAA^T\nu - b^T\nu
\end{array}
}{%
% simplified dual problem
\errmessage{NOT IMPLEMENTED}
}
}

\newcommand\entmax[1]{\optprob{#1}{%
% primal problem
\begin{array}{ll}
	\mbox{minimize} &
		\sum_{i=1}^n x_i\log x_i
	\\
	\mbox{subject to} &
		Ax \preceq b
	\\ &
		\ones^T x = 1
\end{array}
}{%
% dual function
-b^T\lambda -\nu - \exp(-\nu-1)\sum_{i=1}^n \exp(a_i^T\lambda)
}{%
% dual problem
\begin{array}{ll}
	\mbox{maximize} &
		-b^T\lambda -\nu - \exp(-\nu-1)\sum_{i=1}^n \exp(a_i^T\lambda)
	\\
	\mbox{subject to} &
		\lambda \succeq 0
\end{array}
}{%
% simplified dual problem
\begin{array}{ll}
	\mbox{maximize} &
		-b^T\lambda -\log\left( \sum_{i=1}^n \exp(a_i^T\lambda)\right)
	\\
	\mbox{subject to} &
		\lambda \succeq 0
\end{array}
}
}

\def\minvolcoveringdualfcn{%
\left\{\begin{array}{ll}
	\log \det (\sum_{i=1}^m \lambda_i a_ia_i^T) - \ones^T\lambda + n
		& \sum_{i=1}^m \lambda_i a_ia_i^T \succ 0
	\\
	-\infty & \mbox{otherwise}
\end{array}\right.
}

\newcommand\minvolcovering[1]{\optprob{#1}{%
% primal problem
\begin{array}{ll}
	\mbox{minimize} &
		- \log \det X
	\\
	\mbox{subject to} &
		a_i^T X a_i \leq 1
		\quad
		i=1,\ldots,m
\end{array}
}{%
% dual function
\minvolcoveringdualfcn
}{%
% dual problem
\begin{array}{ll}
	\mbox{maximize} &
		\minvolcoveringdualfcn
	\\
	\mbox{subject to} &
		\lambda \succeq 0
%		\sum_{i=1}^m \lambda_i a_ia_i^T \succ 0
\end{array}
}{%
% simplified dual problem
\errmessage{NOT IMPLEMENTED}
}
}

\def\noncvxquadprobdual{%
-b^T(A+\lambda I)^\dagger b - \lambda
}
\newcommand\noncvxquadprob[1]{\optprob{#1}{%
% primal problem
\begin{array}{ll}
	\mbox{minimize} &
		x^TAx + 2b^T x
	\\
	\mbox{subject to} &
		x^T x \leq 1
\end{array}
}{%
% dual function
\left\{\begin{array}{ll}
	\noncvxquadprobdual
		& A+ \lambda I \succeq 0, \; b\in\range(A+\lambda I)
	\\
	-\infty & \mbox{otherwise}
\end{array}\right.
}{%
% dual problem
\begin{array}{ll}
	\mbox{maximize} &
		\noncvxquadprobdual
	\\
	\mbox{subject to} &
		A+ \lambda I \succeq 0, \; b\in\range(A+\lambda I)
\end{array}
}{%
% simplified dual problem
\errmessage{NOT IMPLEMENTED}
}
}

% pictures

\def\optval{3.20}
\newcommand\dualitygraphbase[1]{%
\thinlines
	\put(.3, \optval){\makebox(0,0)[l]{$p^\ast$}}%
	\put(0, \optval){\circle*{.3}}
	\put(-8, 2.78){\line(6,-1){16}}
%\thicklines
	\put(-8.3, \optval){\makebox(0,0)[rt]{${#1} u + t = g(#1)$}}%
	\put(-9,0){\line(1,0){18}} % x-axis
	\put(9.5,0){\makebox(0,0)[l]{$u$}}%
	\put(0,-5){\line(0,1){14}} % y-axis
	\put(0,9.2){\makebox(0,0)[b]{$t$}}%
	\put(2,6){\makebox(0,0){$G$}}%
%	\put(-4, 3.5){\circle{.3}}
	\qbezier
	(-4,3.5)
	(1,5)
	(4, 1.2)
%	\put(4, 1.2){\circle{.3}}
%	\put(6, -.07){\circle{.3}}
	\qbezier
	(4, 1.2)
	(6, -.07)
	(7, 1)
%	\put(7, 1){\circle{.3}}
	\qbezier
	(7, 1)
	(7.5, 5.5)
	(2, 8)
%	\put(2, 8){\circle{.3}}
	\qbezier
	(2, 8)
	(-2, 9)
	(-5, 5)
%	\put(-5, 5){\circle{.3}}
	\qbezier
	(-5, 5)
	(-6.9,2.6)
	(-4,3.5)
}

\newcommand\dualitygraphone[1]{%
\setlength{\unitlength}{#1}%
\begin{picture}(20,15)(-10,-5)
\dualitygraphbase{\lambda}
\thinlines
\put(0, \optval){\line(-1,0){7}}
\end{picture}
}

\newcommand\dualitygraphtwo[1]{%
\setlength{\unitlength}{#1}%
\begin{picture}(20,15)(-10,-5)
\dualitygraphbase{\lambda_1}
\thinlines
\put(-8, 3.9){\line(4,-1){17}}
\put(-8, 4.5){\line(2,-1){16}}
\put(-7, 5){\makebox(0,0)[rb]{${\lambda_2} u + t = g(\lambda_2)$}}%
\put(-8, 3.89){\makebox(0,0)[r]{${\lambda^\ast} u + t = g(\lambda^\ast)$}}%
%	\put(-8.3, 2.78){\makebox(0,0)[rt]{${#1} u + t = g(#1)$}}%
\put(0, 1.9){\circle*{.3}}
\put(-.3, 2.0){\makebox(0,0)[r]{$g(\lambda^\ast)=d^\ast$}}
\put(0, 1.45){\circle*{.3}}
\put(.4, 1.45){\makebox(0,0)[l]{$g(\lambda_1)$}}
\put(0, .5){\circle*{.3}}
\put(.4, .5){\makebox(0,0)[l]{$g(\lambda_2)$}}
\end{picture}
}

\def\asdf{-5.78}
\def\efgh{.53}
\def\ijkl{3.25}
\newcommand\dualitygraphthree[1]{%
\setlength{\unitlength}{#1}%
\begin{picture}(20,15)(-10,-5)
\dualitygraphbase{\lambda}
%\thicklines
\qbezier(\asdf,3.55)(\asdf,5)(\asdf,9)
\qbezier(-5.00,\ijkl)(0,\ijkl)(1.55,\ijkl)
\qbezier(5.4,\efgh)(8,\efgh)(9,\efgh)
\put(5.5, 8){\makebox(0,0){$H$}}
\end{picture}
}

\newcommand\dualitygraphfour[1]{%
\setlength{\unitlength}{#1}%
\begin{picture}(15,13)(-5,-3)
\thinlines
	\put(-5,0){\line(1,0){14}} % x-axis
	\put(9.5,0){\makebox(0,0)[l]{$u$}}%
	\put(0,-3){\line(0,1){12}} % y-axis
	\put(0,9.2){\makebox(0,0)[b]{$t$}}%

\qbezier(-3,9)(-1,-1.95)(9,2)
\put(-4,6.5){\line(1,-1){9}}
\put(0,2.5){\circle{.3}}

\put(3.5, 6){\makebox(0,0){$H$}}
\put(-.2, 1){\makebox(0,0)[r]{$B$}}
\put(.3, 2.5){\makebox(0,0)[l]{$p^\ast$}}

%\thicklines
\qbezier(0,2.3)(0,0)(0,-3)
\end{picture}
}

\newcommand\dualitygraphfive[1]{%
\setlength{\unitlength}{#1}%
\begin{picture}(15,13)(-5,-3)
\thinlines
	\put(-5,1){\line(1,0){14}} % x-axis
	\put(9.5,1){\makebox(0,0)[l]{$u$}}%
	\put(0,1){\line(0,1){8}} % y-axis
	\put(0,.7){\makebox(0,0)[t]{$u=0$}}

	\put(9.5,-.5){\makebox(0,0)[l]{$p^\ast(u)$}}%

\qbezier(-3,9)(-1.1,-1.1)(9,-.5)

\put(-4,6.4){\line(1,-1){9}} % tangential line

\put(3.3, -3.5){\makebox(0,0)[l]{$p^\ast(0) - {\lambda^\ast}^Tu -{\nu^\ast}^Tv$}}

\end{picture}
}

\titlefoil{Convex Sets}{Convex Sets}

\myfoilhead{Lines and line segmenets}

\begin{mydefinition}{lines}
	for some $x,y\in\reals^n$
	$$
		\set{\theta x + (1-\theta) y}{\theta\in\reals}% \subset \reals^n
	$$
	called \define{line going through $x$ and $y$}
\end{mydefinition}

\begin{mydefinition}{line segmenets}
	for some $x,y\in\reals^n$
	$$
		\set{\theta x + (1-\theta) y}{0\leq\theta\leq1\in\reals}% \subset \reals^n
	$$
	called \define{line segment connecting $x$ and $y$}
\end{mydefinition}


\myfoilhead{Affine sets}


\begin{mydefinition}{affine sets}
	set, $C\subset \reals^n$,
	every line going through any two points in which
	is contained in $C$, \ie\
	$$
		\left(
			\forall x,y \in C
		\right)
		\left(
			\set{\theta x + (1-\theta) y}{\theta \in \reals} \subset C
		\right)
	$$
	called
	\define{affine set}
\end{mydefinition}

\begin{mydefinition}{affine hulls}
	for set, $C\subset\reals^n$,
	intersection of all affine sets containing $C$,
	called \define{affine hull of $C$},
	denoted by \define{$\affinehull C$},
	which is equal to
	set of all affine combinations of points in $C$, \ie\
	$$
		\bigcup_{n\in\naturals}
		\set{\theta_1 x_1 + \cdots + \theta_n x_n}{x_1,\ldots,x_n\in C, \theta_1 + \cdots + \theta_n=1}
%		\subset \reals^n
	$$
\end{mydefinition}

\begin{mydefinition}{affine dimension}
	for $C\subset \reals^n$,
	dimension of $\affinehull C$,
	called \define{affine dimension}
\end{mydefinition}


\myfoilhead{Relative interiors and boundaries}

\begin{mydefinition}{relative interiors of sets}
	for $C\subset \reals^n$,
	$$
		\bigcup_{O:\mathrm{open}, O\cap \affinehull C\subset C} O \cap \affinehull C
	$$
	or equivalently
	$$
		\set{x}{(\exists \epsilon >0)(\forall y\in \affinehull{C}, \|y-x\|<\epsilon)(y\in C)}
%		\subset\reals^n
	$$
	is called \define{relative interior of $C$} or \define{interior relative to $C$},
	denoted by \define{$\relint C$}
\end{mydefinition}

\begin{mydefinition}{relative boundaries of sets}
	for $C\subset \reals^n$,
	$\closure{C}\sim \relint C$,
	called \define{relative boundary of $C$}
\end{mydefinition}


\myfoilhead{Convex sets}

\begin{mydefinition}{convex sets}
	set, $C\subset \reals^n$,
	every line segment connecting any two points in which
	is contained in $C$, \ie\
	$$
		\left(
			\forall x,y\in C
		\right)
		\left(
			\forall 0\leq \theta\leq1
		\right)
		\left(
			\theta x + (1-\theta) y \in C
		\right)
	$$
	called \define{convex set}

\end{mydefinition}

\begin{mydefinition}{convex hulls}
	for set, $C\subset \reals^n$,
	intersection of all convex sets containing $C$,
	called \define{convex hull of $C$},
	denoted by \define{$\cvxhull C$},
	which is equal to
	set of all convex combinations of points in $C$, \ie\
	$$
		\bigcup_{n\in\naturals}
		\set{\theta_1 x_1 + \cdots + \theta_n x_n}{x_1,\ldots,x_n\in C, \theta_1 + \cdots + \theta_n=1, \theta_1, \ldots, \theta_n >0}
	$$
\end{mydefinition}

\bit
\item
	convex hull (of course) is convex set
\eit


\myfoilhead{Cones}

\begin{mydefinition}{cones}
	set, $C\subset \reals^n$,
	for which
	$$
		\left(
			\forall x\in C, \theta \geq 0
		\right)
		\left(
			\theta x \in C
		\right)
	$$
	called \define{cone} or \define{nonnegative homogeneous}
\end{mydefinition}

\begin{mydefinition}{convex cone}
	set, $C\subset \reals^n$,
	which is both convex and cone,
	called \emph{convex cone};
	$C$ is convex cone \iaoi\
	$$
		\left(
			\forall x, y\in C, \theta, \xi \geq0
		\right)
		\left(
			\theta x + \xi y \in C
		\right)
	$$
\end{mydefinition}

\bit
\item
	convex cone (of course) is convex set
\item
	examples of convex cones:\
	\prealk{n}, \pprealk{n}, \possemidefset{n}, and \posdefset{n}\
\eit


\myfoilhead{Hyperplanes and half spaces}

\begin{mydefinition}{hyperplanes}
	$n-1$ dimensional affine set in $\reals^n$,
	called \define{hyperplane};
	every hyperplane
	can be expressed as
	$$
		\set{x\in\reals^n}{a^T = b}
	$$
	for some $a\neq0 \in \reals^n$ and $b\in \reals$
\end{mydefinition}

\begin{mydefinition}{half spaces}
	one of two sets divided by hyperplane,
	called \define{half space};
	every half space
	can be expressed as
	$$
		\set{x\in\reals^n}{a^T \leq b}
	$$
	for some $a\neq0 \in \reals^n$ and $b\in \reals$
\end{mydefinition}

\bit
\item
	hyperplanes and half spaces are convex sets
\eit


\myfoilhead{Euclidean balls and ellipsoids}

\begin{mydefinition}{Euclidean ball}
	set of all points distance of which from point, $x\in\reals^n$,
	is no greater than $r>0$,
	called \define{(Euclidean) ball centered at $x$ with radius, $r$},
	denoted by \define{$\ball{x}{r}$},
	\ie\
	$$
		\ball{x}{r} = \set{y\in\reals^n}{\|y-x\|_2\leq r}
	$$
\end{mydefinition}

\begin{mydefinition}{ellipsoids}
	ball elongated along $n$ orthogonal axes,
	called \define{ellipsoid},
	\ie,
	$$
		\set{y\in\reals^n}{(y-x)^TP^{-1}(y-x)\leq 1}
	$$
	for some $x\in\reals^n$ and $P\in \posdefset{n}$
\end{mydefinition}

\bit
\item
	Euclidean balls and ellipsoids are convex sets
\eit


\myfoilhead{Norm balls and norm cones}

\begin{mydefinition}{norm ball}
	for norm, $\|\cdot\|:\reals^n\to\preals$,
	set of all points distance of which measured in the norm from point, $x\in\reals^n$,
	is no greater than $r>0$,
	called \define{norm ball centered at $x$ with radius, $r$, associated with norm, $\|\cdot\|$},
	\ie\
	$$
		\set{y\in\reals^n}{\|y-x\|\leq r}
	$$
\end{mydefinition}

\begin{mydefinition}{norm cone}
	for norm, $\|\cdot\|:\reals^n\to\preals$,
	$x\in\reals^n$,
	and
	$r>0$,
	$$
		\set{(x,y)\in\reals^n \times \reals}{ \|x\|\leq r} \subset \reals^{n+1}
	$$
	called \define{cone associated with norm, $\|\cdot\|$}
\end{mydefinition}

\begin{mydefinition}{second-order cone}
	norm cone associated with Euclidean norm,
	called \define{second-order cone}
\end{mydefinition}

\bit
\item
	norm balls and norm cones are convex sets
\eit


\myfoilhead{Polyhedra}

\begin{mydefinition}{polyhedra}
	intersection of finite number of hyperplanes and half spaces,
	called \define{polyhedron};
	every polyhedron can be expressed as
	$$
		\set{x\in\reals^n}{Ax \preceq b, Cx = d}
	$$
	for $A\in\reals^{m\times n}$, $b\in\reals^m$, $C\in\reals^{p\times n}$, $d\in\reals^p$
\end{mydefinition}

\bit
\item
	polyhedron is convex set (by \propositionname~\ref{proposition:convexity preserving set operations})
\eit


\myfoilhead{Convexity preserving set operations}

\begin{myproposition}{convexity preserving set operations}
	\ %
	\shrinkspacewithintheoremslike\
	\ibit
	\item
		intersection preserves convexity
		\bit
		\item
			for (any) collection of convex sets, \coll,
			$$\bigcap_{C\in\coll} C$$ is convex set\
			\proofref{intersection of convex sets is convex set}
		\eit

	\item
		scalar scaling preserves convexity
		\bit
		\item
			for convex set $C$
			$$\alpha C$$ is convex set for any $\alpha\in\reals$
		\eit

	\item
		sum preserves convexity
		\bit
		\item
			for convex sets $C$ and $D$
			$$C+D$$ is convex set
		\eit

	\item
		direct product preserves convexity
		\bit
		\item
			for convex sets $C$ and $D$
			$$C\times D$$ is convex set
		\eit

	\item
		projection preserves convexity
		\bit
		\item
			for convex set $C\subset A \times B$
			$$
				\set{x\in A}{(\exists y)((x,y)\in C)}
			$$
			is convex
		\eit

	\item
		image and inverse image by affine function preserve convexity
		\bit
		\item
			for affine function $f:A\to B$ and convex sets $C\subset A$ and $D\subset B$
			$$
				f(C) \;\& \; f^{-1}(D)
			$$
			are convex
		\eit

	\item
		image and inverse image by linear-fractional function preserve convexity
		\bit
		\item
			for convex sets $C\subset \reals^n, D\subset \reals^m$
			and
			linear-fractional function, $g:\reals^n\to\reals^m$,
			\ie, function defined by $g(x) = (Ax+b)/(c^Tx+d)$
			for $A\in\reals^{m\times n}$, $b\in\reals^m$, $c\in\reals^n$, and $d\in\reals$
			$$
				g(C) \ \& \ g^{-1}(D)
			$$
			are convex
		\eit
	\eit
\end{myproposition}
\vfill


\myfoilhead{Proper cones and generalized inequalities}

\begin{mydefinition}{proper cones}
	closed convex cone $K$ which is
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		solid, \ie, $\interior{K}\neq \emptyset$
	\iitem
		pointed, \ie, $x\in vK$ and $-x\in K$ imply $x=0$
	\eit
	\shrinkspacewithintheoremslike\
	called \define{proper cone}
\end{mydefinition}

\bit
\item
	examples of proper cones:
	\prealk{n}\ and \possemidefset{n}\
\eit

\begin{mydefinition}{generalized inequalities}
	proper cone $K$\
	defines \define{generalized inequalities}\
	\shrinkspacewithintheoremslike\
	\shrinkspacewithintheoremslikehalf\
	\ibit
	\iitem
		{(nonstrict) generalized inequality}
	$$
		x \preceq_K y
		\Leftrightarrow
		y - x\in K
	$$
	\iitem
		{strict generalized inequality}
	$$
		x \prec_K y
		\Leftrightarrow
		y - x\in \interior{K}
	$$
	\eit
\end{mydefinition}

\bit
\item
	$\preceq_K$ and $\prec_K$ are partial orderings
\eit


\myfoilhead{Convex sets induced by generalized inequalities}

\bit
\item
	for affine function $g:\reals^n\to\symset{m}$,
	\ie, $f(x)=A_0 + A_1 x_1 + \cdots + A_n x_n$
	for some $A_0,\ldots,A_n\in\symset{m}$,
	$f^{-1}(\possemidefset{n})$ is convex (by \propositionname~\ref{proposition:convexity preserving set operations}),
	\ie,
	$$
		\set{x\in\reals^n}{A_0 + A_1 x_1 + \cdots + A_n x_n \succeq 0} \subset \reals^n
	$$
	is convex

\vitem
	can negate each matrix $A_i$ and have same results,
	hence
	$$
		\set{x\in\reals^n}{A_0 + A_1 x_1 + \cdots + A_n x_n \preceq 0} \subset \reals^n
	$$
	is (also) convex
\eit
\vfill


\myfoilhead{Separating and supporting hyperplanes}

\begin{mytheorem}{separating hyperplane theorem}
	for nonempty disjoint convex sets $C$ and $D$,
	exists hyperplane which separates $C$ and $D$,
	\ie\
	$$
		\left(
			\exists a\neq0\in\reals^n, b\in\reals
		\right)
		\left(
			\forall x\in C, y\in D
		\right)
		\left(
			a^T x + b \geq 0
			\ \& \ %
			a^T y + b \leq 0
		\right)
	$$
\end{mytheorem}

\begin{mydefinition}{separating hyperplanes}
	for nonempty disjoint convex sets $C$ and $D$,
	hyperplane satisfying property in \theoremname~\ref{theorem:separating hyperplane theorem},
	called \define{separating hyperplane},
	said to \define{separate $C$ and $D$}\
\end{mydefinition}

\begin{mytheorem}{supporting hyperplane theorem}
	for nonempty convex set $C$
	and $x\in \boundary C$,
	exists hyperplane passing through $x$,
	\ie,
	$$
		\left(
			\exists a\neq0\in\reals^n
		\right)
		\left(
			\forall y\in C
		\right)
		\left(
			a^T(y-x) \leq 0
		\right)
	$$
\end{mytheorem}

\begin{mydefinition}{supporting hyperplanes}
	for nonempty convex set $C$
	and $x\in \boundary C$,
	hyperplane satisfied property in \theoremname~\ref{theorem:supporting hyperplane theorem},
	called \define{supporting hyperplane}\
\end{mydefinition}


\myfoilhead{Dual cones}

\newcommand\dualconef[1]{%
\setlength{\unitlength}{#1}%
\begin{picture}(5,4.5)(-2,-1.2)%
\thicklines
	\put(0,0){\line(2,1){3}}%
	\put(0,0){\line(1,2){1.5}}%
	\put(1.5,1.5){\makebox(0,0)[c]{$K$}}%
\thinlines
	\put(0,0){\line(3,1){2}}%
	\put(0,0){\line(-3,-1){2}}%
	\put(0,0){\vector(-1,3){.6666666666666666}}%
	\put(-.7666666666666666666,2.3){\makebox(0,0)[rb]{$x$}}%

	\put(-0.37947331922020555, -0.12649110640673517){\line(-1,3){0.12649110640673517}}
	\put(-0.12649110640673517, 0.37947331922020555){\line(-3,-1){0.37947331922020555}}
\end{picture}
}

\newcommand\dualcones[1]{%
\setlength{\unitlength}{#1}%
\begin{picture}(5,4.5)(-2,-1.2)%
\thicklines
	\put(0,0){\line(2,1){3}}%
	\put(0,0){\line(1,2){1.5}}%
	\put(1.5,1.5){\makebox(0,0)[c]{$K$}}%
\thinlines
	\put(0,0){\line(3,2){1.754116}}%
	\put(0,0){\line(-3,-2){1.754116}}%
	\put(0,0){\vector(-2,3){1.16941}}%
	\put(-1.26941,1.904116){\makebox(0,0)[rb]{$z$}}%

	\put(-0.3207134902949093,-0.2138089935299395){\line(-2,3){0.2138089935299395}}
	\put(-0.2138089935299395,0.3207134902949093){\line(-3,-2){0.3207134902949093}}
\end{picture}
}


\begin{mydefinition}{dual cones}
	for cone $K$,
	$$
		\set{x}{\forall y \in K, y^Tx\geq 0 }
	$$
	called \define{dual cone of $K$},
	denoted by \define{$K^\ast$}
\end{mydefinition}

\bit
\item \figref{dual cone}\ illustrates $x \in K^\ast$ while $z\not\in K^\ast$
\eit

\begin{figure}
\begin{center}
%\setlength{\fboxsep}{0pt}
%\fbox{
\dualconef{2em}
%}
\hspace{2em}
%\fbox{
\dualcones{2em}
%}
	\idxfig{dual cone}
	\label{fig:dual cone}
\end{center}
\end{figure}


\myfoilhead{Dual norms}

\begin{mydefinition}{dual norms}
	for norm $\|\cdot\|$,
	fudnction defined by
	$$
		y \mapsto \sup \set{y^Tx}{\|x\|\leq 1}
	$$
	called \define{dual norm of $\|\cdot\|$},
	denoted by \define{$\|\cdot\|_\ast$}
\end{mydefinition}


\bit
\item
	examples
	\bit
	\vitem
		dual cone of subspace $V\subset \reals^n$
		is orthogonal complement of $V$, $V^\perp$,
		where
		$V^\perp=\set{y}{\forall v\in V,v^Ty = 0}$
	\vitem
		\prealk{n} and \possemidefset{n}\ are self-dual

	\vitem
		\emph{dual of norm cone} is \emph{norm cone associated with dual norm},
		\ie,
		if $K=\set{(x,t)\in\reals^{n} \times \reals}{\|x\|\leq t}$
		$$
			K=\set{(y,u)\in\reals^{n} \times \reals}{\|y\|_\ast\leq u}
		$$
	\eit
\eit


\myfoilhead{Properties of dual cones}

\begin{myproposition}{properties of dual cones}
	for cones $K$, $K_1$, and $K_2$
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		$K^\ast$ is closed and convex
	\iitem
		$K_1\subset K_2 \Rightarrow K_2^\ast \subset K_1^\ast$
	\iitem
		if $\interior{K} \neq \emptyset$, $K^\ast$ is pointed
	\iitem
		if \closure{K}\ is pointed, $\interior{(K^\ast)} \neq \emptyset$
	\iitem
		$K^{\ast\ast}=(K^\ast)^\ast$ is closure of convex hull of $K$,
	\iitem
		$K^\ast$ is closed and convex
	\eit
	\shrinkspacewithintheoremslike\
	thus,
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		if $K$ is closed and convex, $K^{\ast\ast} = K$
	\iitem
		dual of proper cone is proper cone
	\iitem
		for proper cone $K$, $K^{\ast\ast}=K$
	\eit
\end{myproposition}


\myfoilhead{Dual generalized inequalities}

\bit
\item
	dual of proper cone is proper (\propositionname~\ref{proposition:properties of dual cones}),
	hence the dual also induces generalized inequalities
\eit

\begin{myproposition}{}
	\label{proposition:generalized inequalities and dual generalized inequalities}
	for proper cone $K$,
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		$x\preceq_K y$ \iaoi\ $(\forall \lambda \succeq_{K^\ast} 0)(\lambda^T x \leq \lambda^T y)$
	\iitem
		$x\prec_K y$ \iaoi\ $(\forall \lambda \succeq_{K^\ast} 0 \mbox{ with } \lambda\neq0)(\lambda^T x < \lambda^T y)$
	\eit
%	\shrinkspacewithintheoremslike\
	$K^{\ast\ast} = K$,
	hence
	above are equivalent to
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		$x\preceq_{K^\ast} y$ \iaoi\ $(\forall \lambda \succeq_{K} 0)(\lambda^T x \leq \lambda^T y)$
	\iitem
		$x\prec_{K^\ast} y$ \iaoi\ $(\forall \lambda \succeq_{K} 0 \mbox{ with } \lambda\neq0)(\lambda^T x < \lambda^T y)$
	\eit
\end{myproposition}


\myfoilhead{Theorem of alternative for linear strict generalized inequalities}

\begin{mytheorem}{theorem of alternative for linear strict generalized inequalities}
	for
		proper cone $K\subset \reals^m$,
		$A\in\reals^{m\times n}$,
		and
		$b\in\reals^m$,
	$$
		Ax \prec_K b
	$$
	is infeasible \iaoi\
	exist nonzero $\lambda\in\reals^m$
	such that
	$$
		\lambda \neq0,\ %
		\lambda \succeq_{K^\ast} 0,\ %
		A^T \lambda = 0,\ %
		\lambda^T b \leq0
	$$
	Above two inequality systems are \emph{alternative},
	\ie, for any data, $A$ and $b$,
	exactly one of them is feasible.
	\proofref{theorem of alternative for linear strict generalized inequalities}
%	\propositionname~\ref{proposition:generalized inequalities and dual generalized inequalities}
\end{mytheorem}


\titlefoil{Convex Functions}{Convex Functions}

\myfoilhead{Convex functions}

\begin{mydefinition}{convex functions}
	\ %
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		function $f:\reals^n\to\reals$
		the domain of which is convex
		and which satisfies
		$$
			\left(
				\forall x,y\in \dom f, 0\leq \theta \leq 1
			\right)
			\left(
				f(\theta x + (1-\theta) y)
				\leq
				\theta f(x) + (1-\theta) f(y)
			\right)
		$$
		said to be \define{convex}
	\iitem
		function $f:\reals^n\to\reals$
		the domain of which is convex
		and which satisfies
		$$
			\left(
				\forall \mbox{ distinct } x,y\in \dom f, 0< \theta < 1
			\right)
			\left(
				f(\theta x + (1-\theta) y)
				<
				\theta f(x) + (1-\theta) f(y)
			\right)
		$$
		said to be \define{strictly convex}
\eit
\end{mydefinition}

\begin{mydefinition}{concave functions}
	\ %
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		function $f:\reals^n\to\reals$
		the domain of which is convex
		where $-f$ is convex,
		said to be \define{concave}
	\iitem
		function $f:\reals^n\to\reals$
		the domain of which is convex
		where $-f$ is strictly convex,
		said to be \define{strictly concave}
	\eit
\end{mydefinition}


\myfoilhead{Extended real-value extensions of convex functions}

\begin{mydefinition}{extended real-value extension of convex functions}
	for convex function $f$,
	function
	$\tilde{f}: \reals^n \to \reals\cup\{\infty\}$
	defined by
	$$
		\tilde{f}(x)
		=
		\left\{\begin{array}{ll}
			f(x) &\mbox{if } x \in \dom f
			\\
			\infty &\mbox{if } x \not\in \dom f
		\end{array}\right.
	$$
	called \define{extended real-value extension of $f$}
\end{mydefinition}

\bit
\item
	using extended real-value extensions of convex functions,
	can drop ``$\dom f$'' in equations,
	\eg,
	\bit
	\item
		$f$ is convex \iaoi\ its extended-value extension $\tilde{f}$ satisfies
		$$
			\left(
				\forall x,y\in \dom f, 0\leq \theta \leq 1
			\right)
			\left(
				f(\theta x + (1-\theta) y)
				\leq
				\theta f(x) + (1-\theta) f(y)
			\right)
		$$
	\item
		$f$ is strictly convex \iaoi\ its extended-value extension $\tilde{f}$ satisfies
		$$
			\left(
				\forall \mbox{ distinct } x,y\in \dom f, 0< \theta < 1
			\right)
			\left(
				f(\theta x + (1-\theta) y)
				<
				\theta f(x) + (1-\theta) f(y)
			\right)
		$$
	\eit
\eit


\myfoilhead{First-order condition for convexity}

\begin{mytheorem}{first-order condition for convexity}
	differentiable $f$,
	\ie, $\dom f$ is open
	and gradient $\nabla f$ exists at every point in $\dom f$,
	is
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		convex \iaoi\ $\dom f$ is convex
		and
		$$
			\left(
				\forall x,y\in \dom f
			\right)
			\left(
				f(y) \geq f(x) + \nabla f(x) ^T (y-x)
			\right)
		$$
	\iitem
		strictly convex \iaoi\ $\dom f$ is convex
		and
		$$
			\left(
				\forall \mbox{ distinct } x,y\in \dom f
			\right)
			\left(
				f(y) > f(x) + \nabla f(x) ^T (y-x)
			\right)
		$$
	\eit
\end{mytheorem}

\bit
\item
	\theoremname~\ref{theorem:first-order condition for convexity}\
	implies
	that
	for convex function $f$
	\bit
	\vitem
		first-order Taylor approximation is \emph{global underestimator}
	\vitem
		can derive
		global information
		from
		local information
		\bit
		\iitem
			\eg, if $\nabla f(x)=0$, $x$ is global minimizer
		\iitem
			\eemph{explains remarkable properties of convex functions and convex optimization problems}
		\eit
	\eit
\eit


\myfoilhead{Second-order condition for convexity}

\begin{mytheorem}{second-order condition for convexity}
	twice-differentiable $f$,
	\ie, $\dom f$ is open
	and Hessian $\nabla^2 f$ exists at every point in $\dom f$,
	is
	convex \iaoi\ $\dom f$ is convex
	and
	$$
		\left(
			\forall x\in \dom f
		\right)
		\left(
			\nabla^2 f(x) \succeq 0
		\right)
	$$

	\ibit
	\iitem
		if $\dom f$ is convex and
		$$
			\left(
				\forall x\in \dom f
			\right)
			\left(
				\nabla^2 f(x) \succ 0
			\right)
		$$
		it is strictly convex
	\eit
\end{mytheorem}


\myfoilhead{Convex function examples}

\bit
\iitem
	assume function $f:\reals^n\to\reals$
	and $\dom f =\reals^n$
	unlesss specified otherwise

\vitem
	affine function, \ie, $f(x)=a^Tx +b $ for some $a\in\reals^n$ and $b\in\reals$, is convex

\vitem
	quadratic functions
	- if $f(x) = x^T Px + q^Tx$
	for some $P\in\symset{n}$ and $q\in\reals^n$
	\bit
	\item
		$f$ is convex \iaoi\ $P\succeq0$
	\item
		$f$ is strictly convex \iaoi\ $P\succ0$
	\eit

\vitem
	exponential function,
	\ie, $f(x) = \exp(a^Tx+b)$ for some $a\in\reals^n$ and $b\in\reals$,
	is convex

\vitem
	power,
	\ie, $f(x) = x^a$ for some $a\geq1$,
	is convex on $\ppreals$

\vitem
	power of absolute value,
	\ie, $f(x) = |x|^a$ for some $a\geq1$,
	is convex on $\reals$

\vitem
	logarithm function,
	\ie, $f(x) = \log x$,
	is concave on \ppreals\

\vitem
	negative entropy,
	\ie,
	$$
		f(x) = \left\{\begin{array}{ll}
			x\log x & \mbox{if } x >0
			\\
			0 &\mbox{if } x=0
		\end{array}\right.
	$$
	is convex on \preals\

\vitem
	norm as function is convex
	(by definition of norms,
	\ie, triangle inequality \& absolute homogeneity)

\vitem
	max function,
	\ie, $f(x)=\max(x_1,\ldots,x_n\}$,
	is convex

\vitem
	quadratic-over-linear function,
	$f(x,y) = x^2/y$,
	is convex on $\reals\times \ppreals$

\vitem
	log-sum-exp,
	$f(x) = \log(\exp(x_1)+\cdots+\exp(x_n))$,
	is convex

\vitem
	geometric mean,
	$f(x) = (\prod_{i=1}^n x_i )^{1/n}$,
	is concave on \pprealk{n}\

\vitem
	log-determinant,
	$f(X) = \log \det X$,
	is concave on \posdefset{n}\
\eit


\myfoilhead{Sublevel sets and superlevel sets}

\begin{mydefinition}{sublevel sets}
	for function $f$ and $\alpha\in\reals$,
	$$
		\set{x\in\dom f}{f(x)\leq \alpha}
	$$
	called \define{$\alpha$-sublevel set of $f$}
\end{mydefinition}

\begin{mydefinition}{superlevel sets}
	for function $f$ and $\alpha\in\reals$,
	$$
		\set{x\in\dom f}{f(x)\geq \alpha}
	$$
	called \define{$\alpha$-superlevel set of $f$}
\end{mydefinition}

\begin{myproposition}{convexity of level sets}
%	every sublevel set of convex function is convex
%	and every superlevel set of concave function is convex
	\ %
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
	every sublevel set of convex function is convex
	\iitem
	and every superlevel set of concave function is convex
	\eit
\end{myproposition}

\bit
\item
	note, however, converse is not true
	\bit
	\item
		\eg, every sublevel set of $\log$ is convex, but $\log$ is concave
	\eit
\eit
\vfill


\myfoilhead{Epigraphs and hypographs}

\begin{mydefinition}{epigraphs}
	for function $f$,
	$$
		\set{(x,t)}{x\in\dom f, f(x)\leq t}
	$$
	called \define{epigraph of $f$},
	denoted by \define{$\epi f$}
\end{mydefinition}

\begin{mydefinition}{hypographs}
	for function $f$,
	$$
		\set{(x,t)}{x\in\dom f, f(x)\geq t}
	$$
	called \define{hypograph of $f$},
	denoted by \define{$\hypo f$}
\end{mydefinition}

\begin{myproposition}{graphs and convexity}
\ %
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
	function is convex \iaoi\ its epigraph is convex
	\iitem
	function is concave \iaoi\ its hypograph is convex
	\eit
\end{myproposition}


\myfoilhead{Convexity preserving function operations}

\begin{myproposition}{convexity preserving function operations}
\ %
	\shrinkspacewithintheoremslike\
	\ibit
	\item
		nonnegative weighted sum preserves convexity
		\bit
		\item
			for convex functions $f_1$, \ldots, $f_n$ and nonnegative weights $w_1,\ldots, w_n$
			$$
				w_1 f_1 + \cdots w_n f_n
			$$
			is convex
		\eit

	\item
		nonnegative weighted integration preserves convexity
		\bit
		\item
			for measurable set $Y$,
			$w:Y\to\preals$,
			and
			$f:X \times Y$
			where $f(x,y)$ is convex in $x$ for every $y\in Y$
			and measurable in $y$ for every $x\in X$
			$$
				\int_Y w(y) f(x,y) dy
			$$
			is convex
		\eit

	\item
		pointwise maximum preserves convexity
		\bit
		\item
			for convex functions $f_1$, \ldots, $f_n$
			$$
				\max\{f_1, \ldots, f_n\}
			$$
			is convex
		\eit

	\item
		pointwise supremum preserves convexity
		\bit
		\item
			for indexed family of convex functions $\indexedcol{f_\lambda}_{\lambda\in\Lambda}$
			$$
				\sup_{\lambda \in \Lambda} f_\lambda
			$$
			is convex
			(one way to see this is $\epi \sup_\lambda f_\lambda = \bigcap_\lambda \epi f_\lambda$)
		\eit
	\item
		composition
		\pagelabel{page:convexity preserving operation - composition}
		\bit
		\item
			suppose $g:\reals^n\to\reals^k$, $h:\reals^k\to\reals$, and $f=h\circ g$
			\bit
			\iitem
				$f$ convex if $h$ convex \& nondecreasing in each argument, and $g_i$ convex
			\iitem
				$f$ convex if $h$ convex \& nonincreasing in each argument, and $g_i$ concave
			\iitem
				$f$ concave if $h$ concave \& nondecreasing in each argument, and $g_i$ concave
			\iitem
				$f$ concave if $h$ concave \& nonincreasing in each argument, and $g_i$ convex
			\eit
		\eit

	\item
		minimization
		\bit
		\item
			for function $f(x,y)$ convex in $(x,y)$ and convex set $C$
			$$
				\inf_{y\in C} f(x,y)
			$$
			is convex provided it is bounded below where domain is \set{x}{(\exists y\in C)((x,y) \in \dom f)}
			\proofref{convexity of infimum of convex function}
		\eit

	\item
		perspective of convex function preserves convexity
		\bit
		\item
			for convex function $f:X\to\reals$,
			function $g:X\times \reals \to \reals$
			defined by
			$$
				g(x,t) = tf(x/t)
			$$
			with $\dom g = \set{(x,t)}{x/t \in \dom f, t>0}$
			is convex
		\eit
	\eit
\end{myproposition}


\myfoilhead{Convex functions examples}


\propositionname~\ref{proposition:convexity preserving function operations}
implies
\shrinkspacewithintheoremslikehalf\

\bit
\item
	piecewise-linear function is convex, \ie\
	\bit
	\iitem
		$\max\{a_1^Tx+b_1,\ldots,a_m^T x + b_m\}$
		for some $a_i\in\reals^n$ and $b_i\in\reals$
		is convex
	\eit

\item
	sum of $k$ largest components is convex, \ie\
	\bit
	\iitem
		$x_{[1]} + \cdots + x_{[k]}$
		where $x_{[i]}$ denotes $i$-th largest component,
		is convex
		(since $f(x) = \max\set{x_{i_1}+\cdots+x_{i_r}}{1\leq i_1< i_2<\cdots < i_r\leq n}$)
	\eit

\item
	support function of set, \ie,
	\bit
	\iitem
		$\sup\set{x^Ty}{y\in A}$
		for $A\subset\reals^n$
		is convex
	\eit

\item
	distance (when measured by arbitrary norm) to farthest point of set
	\bit
	\iitem
		$\sup\set{\|x-y\|}{y\in A}$
		for $A\subset\reals^n$
		is convex
	\eit

\item
	least-squares cost as function of weights
	\bit
	\iitem
		$\inf_{x\in\reals^n} \sum^n_{i=1} w_i(a_i^Tx - b_i)^2$ for some $a_i\in\reals^n$ and $b_i\in\reals$
		is concave
		\bit
		\iitem note that above function equals to
			$
				\sum_{i=1}^n w_i b_i^2 - \sum_{i=1}^n w_i^2 b_i^2 a_i^T \left( \sum_{j=1}^n w_ja_ja_j^T\right)^{-1} a_i
			$
			but not clear whether it is concave
		\eit
	\eit
\eit

\myfoilhead{}

\bit

\item
	maximum eigenvalue of symmetric matrix
	\bit
	\iitem
		$\lambda_\mathrm{max}(F(x)) = \sup\set{y^TF(x)y}{\|y\|_2 \leq 1}$
		where $F:\reals^n\to \symset{m}$
		is linear function in $x$
	\eit

\item
	norm of matrix
	\bit
	\iitem
		$\sup\set{u^TG(x)v}{\|u\|_2 \leq 1, \|v\|_2\leq1}$
		where $G:\reals^n\to \reals^{m\times n}$
		is linear function in $x$
	\eit

\item
	distance (when measured by arbitrary norm) to convex set
	\bit
	\iitem
		for convex set $C$,
		$\inf\set{\|x-y\|}{y\in C}$
	\eit

\item
	infimum of convex function
	subject to linear constraint
	\bit
	\iitem
		for convex function $h$,
		$\inf\set{h(y)}{Ay=x}$ is convex
		(since it is $\inf_y (h(y) + I_{Ay=x}(x,y))$)
	\eit

\item
	perspective of Euclidean norm squared
	\bit
	\iitem
		map $(x,t) \mapsto x^Tx /t$
		induces convex function in $(x,t)$ for $t>0$
	\eit

\item
	perspective of negative log
	\bit
	\iitem
		map $(x,t) \mapsto -t \log(x/t)$
		induces convex function in $(x,t) \in \pprealk{2}$
	\eit
\eit

\myfoilhead{}

\bit
\item
	perspective of convex function
	\bit
	\iitem
		for convex function $f:\reals^n\to\reals$,
		function $g:\reals^n\to\reals$
		defined by
		$$
			g(x) = (c^T x + d) f((Ax+b)/(c^T x + d))
		$$
		from some $A\in\reals^{m\times n}$, $b\in\reals^m$, $c\in\reals^n$, and $d\in\reals$
		with $\dom g = \set{x}{(Ax+b)/(c^Tx + d)\in \dom f, c^T x + d >0}$
		is convex
	\eit
\eit


\myfoilhead{Conjugate functions}

\begin{mydefinition}{conjugate functions}
	for function $f$
	$$
		\sup_{y\in \dom f} (x^Ty - f(y))
	$$
	called \define{conjugate function of $f$},
	denoted by \define{$f^\ast$}
\end{mydefinition}

\shrinkspacewithintheoremslikehalf
\bit
\item
	conjugate function is convex for any function $f$
	because it is supremum
	of linear (hence convex) functions (in $x$)
	(\propositionname~\ref{proposition:convexity preserving function operations})
\eit

\begin{myinequality}{Fenchel's inequality}
	definition of conjugate function implies
	$$
		f(x) + f^\ast(y) \geq x^Ty
	$$
	sometimes called \emph{Young's inequality}
\end{myinequality}
\index{Fenchel, Moritz Werner!Fenchel's inequality}

\begin{myproposition}{conjugate of conjugate}
	for convex and closed function $f$
	$$
		f^{\ast\ast} = f
	$$
	where closed function $f$ is defined by function with closed $\epi f$
\end{myproposition}


\myfoilhead{Conjugate function examples}

\bit
\item
	strictly convex quadratic function
	\bit
	\item
		for $f:\reals^n \to \preals$
		defined $f(x) = x^TQx/2$ where $Q\in \posdefset{n}$,
		$$
			f^\ast(x)= \sup_y(y^Tx - y^TQy/2)
			 = (y^Tx - y^TQy/2)|_{y=Q^{-1}x}
			 = x^TQ^{-1}x/2
		$$
		which is also strictly convex quadratic function
	\eit

\item
	log-determinant
	\bit
	\item
		for
		function $f:\posdefset{n} \to \reals$
		defined by $f(X) = \log \det X^{-1}$
		$$
			f^\ast(X)
			=
			\sup_{Y\in\posdefset{n}} (\Tr XY + \log \det Y)
			= \log\det (-X)^{-1} - n
		$$
		where $\dom f^\ast = -\posdefset{n}$
	\eit

\item
	indicator function
	\bit
	\item
		for
		indicator function $I_A:\reals^n\to\{0,\infty\}$ with $A\subset \reals^n$
		$$
			I_A^\ast(x) = \sup_y (y^Tx - I_A(y)) = \sup \set{y^Tx}{y\in A}
		$$
		which is support function of $A$
	\eit

\item
	log-sum-exp function
	\bit
	\item
		for
		function $f: \reals^n \to \reals$
		defined by $f(x) = \log(\sum_{i=1}^n \exp(x_i))$
		$$
			f^\ast(x) =
			\sum_{i=1}^n x_i \log x_i + I_{x\succeq 0, \ones^T x = 1}(x)
		$$
	\eit

\item
	norm
	\bit
	\item
		for norm function $f:\reals^n\to\preals$ defined by $f(x)=\|x\|$
		$$
			f^\ast(x)
			=
			\sup_y( {y^Tx - \|y\|})
			= I_{\|x\|_\ast\leq1}(x)
		$$
	\eit

\item
	norm squared
	\bit
	\item
		for
		function $f: \reals \to \preals$
		defined by $f(x) = \|x\|^2/2$
		$$
			f^\ast(x) = \|x\|_\ast^2/2
		$$
	\eit

\item
	differentiable convex function
	\bit
	\item
		for
		differentiable convex function $f:\reals^n\to\reals$
		$$
			f^\ast(x)=
			(y^\ast)^T \nabla f(y^\ast) - f(y^\ast)
		$$
		where $y^\ast = \argsup_y (x^Ty-f(y))$
	\eit

\item
	sum of independent functions
	\bit
	\item
		for
		function $f:\reals^n\times \reals^m \to \reals$ defined by $f(x,y) = f_1(x) + f_2(y)$
		where $f_1:\reals^n\to\reals$ and $f_2:\reals^m\to\reals$
		$$
			f^\ast(x,y) = f_1^\ast(x) + f_2^\ast(y)
		$$
	\eit
\eit


\myfoilhead{Convex functions \wrt\ generalized inequalities}

\begin{mydefinition}{$K$-convex functions}
	for proper cone $K$,
\shrinkspacewithintheoremslike\
\ibit
\iitem
	function $f$ satisfying
	$$
		\left(
			\forall x,y \in \dom f, 0\leq \theta\leq 1
		\right)
		\left(
			f(\theta x + (1-\theta) y)
			\preceq_K
			\theta f(x) + (1-\theta) f(y)
		\right)
	$$
	called \define{$K$-convex}

\iitem
	function $f$ satisfying
	$$
		\left(
			\forall x\neq y \in \dom f, 0< \theta< 1
		\right)
		\left(
			f(\theta x + (1-\theta) y)
			\prec_K
			\theta f(x) + (1-\theta) f(y)
		\right)
	$$
	called \define{strictly $K$-convex}
\eit
\end{mydefinition}

\begin{myproposition}{dual characterization of $K$-convexity}
	for proper cone $K$
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		function $f$ is $K$-convex \iaoi\ for every $w\succeq_{K^\ast}0$, $w^Tf$ is convex
	\iitem
		function $f$ is strictly $K$-convex \iaoi\ for every nonzero $w\succeq_{K^\ast}0$, $w^Tf$ is strictly convex
	\eit
\end{myproposition}



\myfoilhead{Matrix convexity}

\begin{mydefinition}{matrix convexity}
	function of $\reals^n$ into \symset{m}
	which is $K$-convex where $K=\possemidefset{m}$,
	called \define{matrix convex}
\end{mydefinition}

\bit
\vitem
	examples of matrix convexity
	\bit
	\vitem
		function of $\reals^{n\times m}$ into \possemidefset{n}
		defined by $X\mapsto XX^T$
		is matrix convex\

	\vitem
		function of \posdefset{n}\ into itself
		defined by $X\mapsto X^p$
		is matrix convex for $1\leq p\leq 2$ or $-1\leq p \leq0$,
		and matrix concave for $0\leq p\leq1$

	\vitem
		function of \symset{n}\ into \posdefset{n}\
		defined by $X\mapsto \exp(X)$
		is \emph{not} matrix convex

	\vitem
		quadratic matrix function of $\reals^{m\times n}$ into \symset{n}\
		defined by $X\mapsto X^TAX + B^TX + X^TB + C$
		for $A\in\symset{m}$, $B\in\reals^{m\times n}$, and $C\in\symset{n}$
		is matrix convex when $A\succeq0$
	\eit
\eit
\vfill


\titlefoil{Convex Optimization Problems}{Convex Optimization Problems}

\myfoilhead{Optimization problems}

\begin{mydefinition}{optimization problems}
	for $\fobj:\xobj \to \reals$, $\fie: \xie\to \reals^m$, $\feq: \xeq \to \reals^p$
	where \xobj, \xie, and \xeq\ are subsets of common set $\xdomain$
	$$
		\begin{array}{ll}
			\mbox{minimize}
				& \fobj(x)
			\\
			\mbox{subject to}
				& \fie(x) \preceq 0
			\\
				& \feq(x) =0
		\end{array}
	$$
	called \define{optimization problem}
	where $x$ is \define{optimization variable}
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		\fobj, \fie, and \feq\ are
		\define{objective function},
		\define{inequality \& equality contraint function}

	\iitem
		$\fie(x) \preceq 0$ and $\feq(x) = 0$
		are
		\define{inequality contraints}
		and
		\define{equality contraints}% respectively

	\iitem
		$\optdomain = \xobj \cap \xie \cap \xeq$ is \define{domain} of optimization problem

	\iitem
		$\optfeasset =\set{x\in \optdomain}{\fie(x) \preceq0, \feq(x)=0}$, called \define{feasible set},
		$x\in\optdomain$, said to be \define{feasible} if $x\in\optfeasset$,
		optimization problem, said to be \define{feasible} if $\optfeasset\neq \emptyset$\

	\iitem
		$p^\ast = \inf\set{\fobj(x)}{x\in\optfeasset}$, called \define{optimal value} of optimization problem

	\iitem
		if optimization problem is \emph{infeasible}, \define{$p^\ast = \infty$}\
		(following convention that infimum of empty set is $\infty$)

	\iitem
		if $p^\ast=-\infty$,
		optimization problem
		said to be \define{unbounded}
\eit
\end{mydefinition}


\myfoilhead{Global and local optimalities}

\begin{mydefinition}{global optimality}
	for optimization problem
	in \definitionname~\ref{definition:optimization problems}
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		$x\in \optfeasset$ with $\fobj(x) = p^\ast$, called \define{(global) optimal point}\
	\iitem
		$X_\mathrm{opt} = \set{x\in \optfeasset}{\fobj(x)=p^\ast}$, called \define{optimal set}
	\iitem
		when $X_\mathrm{opt} \neq \emptyset$, we say optimal value is \define{attained} or \define{achieved}
		and
%		\&
		optimization problem is \define{solvable}
	\eit
\end{mydefinition}

\shrinkspacewithintheoremslike
\bit
\item
	optimization problem is \emph{not} solvable if $p^\ast = \infty$ or $p^\ast = -\infty$
	(converse is not true)
\eit

\begin{mydefinition}{local optimality}
	for optimization problem
	in \definitionname~\ref{definition:optimization problems}
	where $\xdomain$ is metric space,
	$x\in\optfeasset$ satisfying
	$
		\inf\set{\fobj(z)}{z\in\optfeasset, \rho(z,x)\leq r}
	$
	where $\rho:\xdomain\times \xdomain\to\preals$ is metric,
	for some $r>0$, said to be \define{locally optimal},
	\ie,
	$x$ solves
	$$
		\begin{array}{ll}
			\mbox{minimize}
				& \fobj(z)
			\\\mbox{subject to}&
				\fie(z) \preceq 0
			\\&
				\feq(z) =0
			\\&
				\rho(z,x) \leq r
		\end{array}
	$$
\end{mydefinition}


\myfoilhead{Equivalent optimization problems}

\begin{mydefinition}{equivalent optimization problems}
	two optimization problems
	where solving one readily solve the other,
	said to be \define{equivalent}
\end{mydefinition}

\bit
\item
	below two optimization problems are equivalent
%	\begin{enumerate}
	\bit
	\vitem
		$$
			\begin{array}{ll}
				\mbox{minimize}
					& -x-y
				\\
				\mbox{subject to}
					& 2x+y \leq1
				\\
					& x+2y \leq1
			\end{array}
		$$
	\vitem
		$$
			\begin{array}{ll}
				\mbox{minimize}
					& -2u-v/3
				\\
				\mbox{subject to}
					& 4u+v/3 \leq1
				\\
					& 2u+2v/3 \leq1
			\end{array}
		$$
	\eit
%	\end{enumerate}
\vitem[]
	since if $(x^\ast, y^\ast)$ solves first,
	$(u,v)=(x^\ast/2, 3y^\ast)$ solves second,
	and if $(u^\ast, v^\ast)$ solves second,
	$(x,y)=(2u^\ast, v^\ast/3)$ solves first
\eit


\myfoilhead{Change of variables}

\bit
\item
	given function $\phi:\mathcalfont{Z} \to \xdomain$,
	optimization problem in \definitionname~\ref{definition:optimization problems}\
	can be rewritten as
	$$
		\begin{array}{ll}
			\mbox{minimize}
				& \fobj(\phi(z))
			\\
			\mbox{subject to}
				& \fie(\phi(z)) \preceq 0
			\\
				& \feq(\phi(z)) =0
		\end{array}
	$$
	where $z\in\mathcalfont{Z}$ is optimization variable

\vitem
	if $\phi$ is injective and $\optdomain \subset \phi(\mathcalfont{Z})$,
	above optimization problem
	and
	optimization problem in \definitionname~\ref{definition:optimization problems}\
	are equivalent,
	\ie\
	\bit
	\item
		$X_\mathrm{opt}$ is optimal set of problem in \definitionname~\ref{definition:optimization problems}
		$\Rightarrow$
		$\phi^{-1}(X_\mathrm{opt})$ is optimal set of above problem
	\item
		$Z_\mathrm{opt}$ is optimal set of above problem
		$\Rightarrow$
		$\phi(Z_\mathrm{opt})$ is optimal set of problem in \definitionname~\ref{definition:optimization problems}
	\eit

\vitem
	two optimization problems said to be related by
	\define{%
	change of variable or substitution of variable $x=\phi(z)$%
	}
\eit


\myfoilhead{Convex optimization}

\begin{mydefinition}{convex optimization}
	optimization problem in \definitionname~\ref{definition:optimization problems}\
	where \xdomain\ is Banach space,
	\ie,
	complete linear normed vector space,
%	(refer to page~\pageref{page:Banach spaces}\ for Banach spaces),
	\fobj\ \& \fie\ are convex functions,
	and
	\feq\ is affine function,
	called \define{convex optimization problem}\
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
	when $\xdomain= \reals^n$, optimization problem can be formulated as
	$$
		\begin{array}{ll}
			\mbox{minimize}
				& \fobj(x)
			\\
			\mbox{subject to}
				& \fie(x) \preceq 0
			\\
				& Ax = b
		\end{array}
	$$
	for some $A\in\reals^{p\times n}$ and $b\in\reals^p$
	\eit
\end{mydefinition}

\bit
\item
	domain of convex optimization problem is \emph{convex}
	\bit
	\item
		since
		domains of \fobj, \fie, and \feq\ are convex (by definition of convex functions)\
%		domain of \feq\ is $\reals^n$, which is convex%,
		and intersection of convex sets is convex
	\eit
\vitem
	feasible set of convex optimization problem
	is \emph{convex}
	\bit
	\item
		since
		sublevel sets of convex functions are convex,
		feasible sets for affine function is either empty set, singleton, or affine sets, all of which are convex sets%,
%		and intersection of convex sets is convex
	\eit
\eit


\myfoilhead{Optimality conditions for convex optimization problems}

\begin{mytheorem}{local optimality implies global optimality}
	for convex optimization problem
	(in \definitionname~\ref{definition:convex optimization}),
	every local optimal point is global optimal point
\end{mytheorem}

\vfill
\begin{mytheorem}{optimality conditions for convex optimality problems}
	for convex optimization problem
	(in \definitionname~\ref{definition:convex optimization}),
	when \fobj\ is differentiable
	(\ie, $\dom \fobj$ is open and $\nabla \fobj$ exists everywhere in $\dom \fobj$)
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		$x\in\optdomain$ is optimal \iaoi\ $x\in\optfeasset$ and
		$$
			\left(
				\forall y \in \optfeasset
			\right)
			\left(
				\nabla \fobj(x)^T(y-x) \geq0
			\right)
		$$
	\iitem
		for unconstrained problems, $x\in\optdomain$ is optimal \iaoi\ $$\nabla \fobj(x)=0$$
	\eit
\end{mytheorem}
\vfill


\myfoilhead{Optimality conditions for some convex optimization problems}

\bit
\item
	unconstrained convex quadratic optimization
	$$
		\begin{array}{ll}
			\mbox{minimize}
				& \fobj(x) = (1/2)x^TPx + q^Tx
		\end{array}
	$$
	where $\xobj=\reals^n$ and $P\in\possemidefset{n}$
	\bit
	\item
		$x$ is optimal \iaoi\
		$$
			\nabla \fobj(x) = Px + q = 0
		$$
		exist three cases
		\bit
		\iitem
			if $P\in\posdefset{n}$, exists unique optimum $x^\ast = -P^{-1}q$
		\iitem
			if $q\in\range(P)$, $X_\mathrm{opt}=-P^\dagger q + \nullspace(P)$
		\iitem
			if $q\not\in\range(P)$, $p^\ast = -\infty$
		\eit
	\eit

\item
	analytic centering
	$$
		\begin{array}{ll}
			\mbox{minimize}
				& \fobj(x) = - \sum_{i=1}^m \log (b_i-a_i^Tx)
		\end{array}
	$$
	where $\xobj = \set{x\in\reals^n}{Ax \prec b}$
	\bit
	\item
		$x$ is optimal \iaoi\
		$$
			\nabla \fobj(x) = \sum_{i=1}^m \frac{1}{b_i-a_i^Tx}a_i = 0
		$$
		exist three cases
		\bit
		\iitem
			exists unique optimum, which happens \iaoi\ \set{x}{b_i-a_i^Tx}\ is nonempty and bounded
		\iitem
			exist infinitely many optima, in which case, $X_\mathrm{opt}$ is affine set
		\iitem
			exists no optimum, which happens \iaoi\ \fobj\ is unbounded below
		\eit
	\eit

\item
	convex optimization problem with equality constraints only
	$$
		\begin{array}{ll}
			\mbox{minimize}
				& \fobj(x)
			\\
			\mbox{subject to}
				& Ax =b
		\end{array}
	$$
	where $\xdomain=\reals^n$
	\bit
	\item
		$x$ is optimal \iaoi\
		$$
			\nabla \fobj(x) \perp \nullspace(A)
		$$
		or equivalently,
		exists $\nu\in\reals^p$
		such that
		$$
			\nabla \fobj(x) = A^T\nu
		$$
	\eit

\eit


\myfoilhead{Linear programming}

\begin{mydefinition}{linear programming}
	convex optimization problem in \definitionname~\ref{definition:convex optimization}
	with $\xdomain=\reals^n$ and linear \fobj\ \& \fie,
	called \define{linear program (LP)},
	which can be formulated as
	$$
		\begin{array}{ll}
			\mbox{minimize}
				& c^Tx
			\\
			\mbox{subject to}
				& Cx \preceq d
			\\
				& A x =b
		\end{array}
	$$
	where $c\in\reals^n$, $C\in\reals^{m\times n}$, $d\in\reals^m$, $A\in\reals^{p\times n}$, $b\in\reals^p$
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
	can transform above LP into \define{standard form LP}
	$$
		\begin{array}{ll}
			\mbox{minimize}
				& \tilde{c}^T\tilde{x}
			\\
			\mbox{subject to}
				& \tilde{A}\tilde{x} = \tilde{b}
			\\
				& \tilde{x} \succeq0
		\end{array}
	$$
	\eit
\end{mydefinition}


\myfoilhead{LP examples}

\bit
\item
	diet problem
	-
	find amount of $n$ different food to minimize purchase cost
	while satisfying nutrition requirements
	\bit
	\item
		assume exist $n$ food and $m$ nutritions,
		$c_i$ is cost of food $i$,
		$A_{ji}$ is amount of nutrition $j$ contained in unit quantity of food $i$,
		$b_j$ is amount requirement for nutrition $j$
	\item
		diet problem can be formulated as LP
		$$
			\begin{array}{ll}
				\mbox{minimize}
					& c^Tx
				\\
				\mbox{subject to}
					& Ax \succeq b
				\\
					& x\succeq0
			\end{array}
		$$
	\eit

\item
	Chebyshev center of polyhedron
	- find largest Euclidean ball contained in polyhedron
	\bit
	\item
		assume polyhedron is \set{x\in\reals^n}{a_i^Tx \leq b_i, i=1,\ldots, m}\
	\item
		problem of finding Chebyshev center of polyhedron can be formulated as LP
		$$
			\begin{array}{ll}
				\mbox{maximize}
					& r
				\\
				\mbox{subject to}
					& a_i^T x + r\|a_i\|_2 \leq b_i
			\end{array}
		$$
		where optimization variables are $x\in\reals^n$ and $r\in\reals$
	\eit

\item
	piecewise-linear minimization
	- minimize maximum of affine functions
	\bit
	\item
		assume $m$ affine functions $a_i^Tx + b_i$
	\item
		piecewise-linear minimization problem
		can be formulated as LP
		$$
			\begin{array}{ll}
				\mbox{minimize}
					& t
				\\
				\mbox{subject to}
					& a_i^Tx + b_ i \leq t,\quad i=1,\ldots,m
			\end{array}
		$$

	\eit

\item
	linear-fractional program
%	-
	$$
		\begin{array}{ll}
			\mbox{minimize} &
%				\frac
%				{
				(
				c^T x + d
				)
%				}
				/
%				{
				(
				e^T x + f
				)
%				}
			\\
			\mbox{subject to} &
				Gx \preceq h
			\\ &
				Ax = b
		\end{array}
	$$
	\bit
	\item
		if feasible set is nonempty,
		can be formulated as LP
		$$
			\begin{array}{ll}
				\mbox{minimize} &
					c^T y + dz
				\\
				\mbox{subject to} &
					Gy - hz \preceq0
				\\ &
					Ay-bz = 0
				\\ &
					e^Ty + fz = 1
				\\ &
					z\geq0
			\end{array}
		$$
	\eit
\eit


\myfoilhead{Quadratic programming}

\begin{mydefinition}{quadratic programming}
	convex optimization problem in \definitionname~\ref{definition:convex optimization}
	with
	$\xdomain=\reals^n$ and
	convex quadratic \fobj\ and linear \fie,
	called \define{quadratic program (QP)},
	which can be formulated as
	$$
		\begin{array}{ll}
			\mbox{minimize}
				& (1/2) x^TPx + q^Tx
			\\
			\mbox{subject to}
				& Gx \preceq h
			\\
				& A x =b
		\end{array}
	$$
	where
		$P\in\possemidefset{n}$, $q\in\reals^n$,
		$G\in\reals^{m\times n}$, $h\in\reals^m$,
		$A\in\reals^{p\times n}$, $b\in\reals^p$
\end{mydefinition}

\bit
\item
	when $P=0$, QP reduces to LP,
	hence \emph{LP is specialization of QP}
\eit


\myfoilhead{QP examples}

\bit
\item
	least-squares (LS) problems
	\bit
	\item
		LS can be formulated as QP
		$$
			\begin{array}{ll}
				\mbox{minimize} &
					\|Ax-b\|_2^2
			\end{array}
		$$
	\eit

\item distance between two polyhedra
	\bit
	\item
		assume two polyhedra
		\set{x\in\reals^n}{Ax\preceq b, Cx =d}\
		and
		\set{x\in\reals^n}{\tilde{A}x\preceq \tilde{b}, \tilde{C}x =\tilde{d}}\
	\item
		problem of finding distance between two polyhedra
		can be formulated as QP
		$$
			\begin{array}{ll}
				\mbox{minimize} &
					\|x-y\|_2^2
				\\
				\mbox{subject to} &
					Ax\preceq b, \quad Cx =d
				\\ &
					\tilde{A}y\preceq \tilde{b}, \quad \tilde{C}y =\tilde{d}
			\end{array}
		$$
	\eit
\eit


\myfoilhead{Quadratically constrained quadratic programming}

\begin{mydefinition}{quadratically constrained quadratic programming}
	convex optimization problem in \definitionname~\ref{definition:convex optimization}
	with $\xdomain=\reals^n$
	and
	convex quadratic \fobj\ \& \fie,
	called \define{quadratically constrained quadratic program (QCQP)},
	which can be formulated as
	$$
		\begin{array}{ll}
			\mbox{minimize}
				& (1/2) x^TP_0x + q_0^Tx
			\\
			\mbox{subject to}
				& (1/2) x^TP_ix + q_i^Tx + r_i \leq0,\quad i=1,\ldots,m
			\\
				& A x =b
		\end{array}
	$$
	where
		$P_i\in\possemidefset{n}$, $q_i\in\reals^n$, $r_i\in\reals$,
		$A\in\reals^{p\times n}$, $b\in\reals^p$
\end{mydefinition}

\bit
\item
	when $P_i=0$ for $i=1,\ldots,m$, QCQP reduces to QP,
	hence \emph{QP is specialization of QCQP}
\eit


\myfoilhead{Second-order cone programming}

\begin{mydefinition}{second-order cone programming}
	convex optimization problem in \definitionname~\ref{definition:convex optimization}\
	with $\xdomain=\reals^n$
	and
	linear \fobj\ and convex \fie\
	of form
	$$
		\begin{array}{ll}
			\mbox{minimize}
				& f^T x
			\\
			\mbox{subject to}
				& \|A_ix + b_i\|_2 \leq c_i^T x + d_i,\quad i=1,\ldots,m
			\\
				& F x =g
		\end{array}
	$$
	where
		$f\in\reals^n$,
		$A_i\in\reals^{n_i\times n}$, $b_i\in\reals^{n_i}$,
		$c_i\in\reals^{n}$, $d_i\in\reals$,
		$F\in\reals^{p\times n}$, $g\in\reals^p$
	called \define{second-order cone program (SOCP)}
\end{mydefinition}

\bit
\item
	when $b_i=0$, SOCP reduces to QCQP,
	hence \emph{QCQP is specialization of SOCP}
\eit


\myfoilhead{SOCP examples}

\bit
\item
	robust linear program
	-
	minimize $c^T x$
	while satisfying
	$\tilde{a}_i^T x \leq b_i$
	for every $\tilde{a}_i \in \set{a_i+P_iu}{\|u\|_2\leq1}$
	where $P_i\in\symset{n}$\
	\bit
	\item can be formulated as SOCP
		$$
			\begin{array}{ll}
				\mbox{minimize} &
					c^T x
				\\
				\mbox{subject to} &
					a_i^T x + \|P_i^T x\|_2 \leq b_i
			\end{array}
		$$
	\eit

\vitem
	linear program with random constraints
	-
	minimize $c^T x$
	while satisfying
	$\tilde{a}_i^T x \leq b_i$
	with probability no less than $\eta$
	where $\tilde{a} \sim \normal(a_i,\Sigma_i)$
	\bit
	\item
		can be formulated as SOCP
		$$
			\begin{array}{ll}
				\mbox{minimize} &
					c^T x
				\\
				\mbox{subject to} &
					a_i^T x + \Phi^{-1}(\eta)\|\Sigma_i^{1/2} x\|_2 \leq b_i
			\end{array}
		$$
	\eit
\eit
\vfill


\myfoilhead{Geometric programming}

\begin{mydefinition}{monomial functions}
	function $f:\pprealk{n}\to\reals$
	defined by
	$$
		f(x) = cx_1^{a_1} \cdots x_n^{a_n}
	$$
	where $c>0$ and $a_i\in\reals$,
	called \define{monomial function}
	or simply \define{monomial}
\end{mydefinition}

\begin{mydefinition}{posynomial functions}
	function $f:\pprealk{n}\to\reals$
	which is finite sum of monomial functions,
	called \define{posynomial function}
	or simply \define{posynomial}
\end{mydefinition}

\begin{mydefinition}{geometric programming}
	optimization problem
		$$
			\begin{array}{ll}
				\mbox{minimize} &
					\fobj(x)
				\\
				\mbox{subject to} &
					\fie(x) \preceq 1
				\\ &
					\feq(x) =1
			\end{array}
		$$
	for posynomials $\fobj:\pprealk{n} \to \reals$ \& $\fie: \pprealk{n} \to \reals^m$
	and monomials $\feq: \pprealk{n} \to \reals^p$,
	called \define{geometric program (GP)}
\end{mydefinition}


\myfoilhead{Geometric programming in convex form}

\bit
\item
	geometric program in
	\definitionname~\ref{definition:geometric programming}
	is not convex optimization problem (as it is)
\item
	however, can be transformed to equivalent convex optimization problem
	by change of variables and transformation of functions
\eit

\begin{myproposition}{geometric programming in convex form}
	geometric program (in \definitionname~\ref{definition:geometric programming})
	can be transformed to equivalent convex optimization problem
		$$
			\begin{array}{ll}
				\mbox{minimize} &
					\log\left(
					\sum_{k=1}^{K_0} \exp((a^{(0)}_k)^T y + b^{(0)}_k)
					\right)
				\\
				\mbox{subject to} &
					\log\left(
					\sum_{k=1}^{K_i} \exp((a^{(i)}_k)^T y + b^{(i)}_k)
					\right)
					\leq0
					\quad
					i=1,\ldots,m
				\\ &
					Gy = h
			\end{array}
		$$
	for some $a^{(i)}_k\in\reals^n$, $b^{(i)}_k\in\reals$, $G\in\reals^{p\times n}$, $h\in\reals^p$
	where optimization variable is $y=\log(x)\in\reals^n$
\end{myproposition}


\myfoilhead{Convex optimization with generalized inequalities}

\begin{mydefinition}{convex optimization with generalized inequality constraints}
	convex optimization problem in \definitionname~\ref{definition:convex optimization}
	with inequality constraints replaced by generalized inequality constraints,
	\ie
	$$
		\begin{array}{ll}
			\mbox{minimize}
				& \fobj(x)
			\\
			\mbox{subject to}
				& \fie_i(x) \preceq_{K_i} 0\quad i=1,\ldots,q
			\\
				& \feq(x) = 0
		\end{array}
	$$
	where
%	$\fobj$ is convex function,
	$K_i\subset R^{k_i}$ are proper cones
	and
	$\fie_i:\xie_i\to\reals^{k_i}$ are $K_i$-convex,
	called \define{convex optimization problem with generalized inequality constraints}
\end{mydefinition}

\shrinkspacewithintheoremslike
\bit
\item
	problem in \definitionname~\ref{definition:convex optimization with generalized inequality constraints}
	reduces to convex optimization problem in \definitionname~\ref{definition:convex optimization}
	when $q=1$ and $K_1=\prealk{m}$,
	hence \emph{convex optimization is specialization of convex optimization with generalized inequalities}
\item
	like convex optimization
	\bit
	\item
		feasible set is $\optfeasset = \set{x\in\optdomain}{\fie_i(x)\preceq_{K_i} 0, Ax=b}$ is convex
	\vitem
		local optimality implies global optimality
	\vitem
		optimality conditions in
		\theoremname~\ref{theorem:optimality conditions for convex optimality problems}\
		applies without modification
	\eit
\eit
\vfill


\myfoilhead{Conic programming}

\begin{mydefinition}{conic programming}
	convex optimization problem with generalized inequality constraints
	in \definitionname~\ref{definition:convex optimization with generalized inequality constraints}
	with linear \fobj\ and one affine \fie\
	$$
		\begin{array}{ll}
			\mbox{minimize}
				& \fobj(x)
			\\
			\mbox{subject to}
				& \fie(x) \preceq_{K} 0
			\\
				& \feq(x) = 0
		\end{array}
	$$
	called \define{conic program (CP)}
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		can transform above CP to \define{standard form CP}\
		$$
			\begin{array}{ll}
				\mbox{minimize}
					& \tildefobj(X)
				\\
				\mbox{subject to}
					& \tildefeq (X) = 0
				\\
					& X \succeq_{K} 0
			\end{array}
		$$
	\eit
\end{mydefinition}


\bit
\item
	cone program is one of simplest convex optimization problems
	with generalized inequalities
\eit


\myfoilhead{Semidefinite programming}

\begin{mydefinition}{semidefinite programming}
	conic program in \definitionname~\ref{definition:conic programming}
	with $\xdomain=\reals^n$ and $K=\possemidefset{n}$
%	\ie\
	$$
		\begin{array}{ll}
			\mbox{minimize} &
				c^Tx
			\\
			\mbox{subject to} &
				x_1F_1 + \cdots + x_nF_n + G \preceq 0
			\\ &
				Ax = b
		\end{array}
	$$
	where $F_1,\ldots,F_n,G\in\symset{k}$ and $A\in\reals^{p\times n}$,
	called \define{semidefinite program (SDP)}
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		above inequality, called \define{linear matrix inequality (LMI)}
	\iitem
		can transform SDP to standard form SDP\
		$$
			\begin{array}{ll}
				\mbox{minimize} &
					\Tr (CX)
				\\
				\mbox{subject to} &
					\Tr (A_iX) = b_i\quad i=1,\ldots,p
				\\ &
					X \succeq 0
			\end{array}
		$$
		where $\xdomain=\possemidefset{n}$ and $C,A_1,\ldots,A_p\in\symset{n}$ and $b_i\in\reals$
	\eit
\end{mydefinition}


\myfoilhead{SDP examples}

\bit
\item LP
	\bit
	\item
		if $k=m$, $F_i=\diag(C_{1,i}, \ldots, C_{m,i})$, $G=-\diag(d_1,\ldots, d_m)$
		in \definitionname~\ref{definition:semidefinite programming},
		SDP reduces to LP in \definitionname~\ref{definition:linear programming}\
	\item
		hence, LP is specialization of SDP
	\eit

\vitem SOCP
	\bit
	\item
		SOCP in \definitionname~\ref{definition:second-order cone programming}
		is equivalent to
		$$
			\begin{array}{ll}
				\mbox{minimize} &
					f^T x
				\\
				\mbox{subject to} &
					Fx = g
				\\ &
					\begin{my-matrix}{cc}
						c_i^Tx + d_i & x^TA_i^T + b_i^T
						\\
						A_ix + b_i & (c_i^Tx + d_i)I_{n_i}
					\end{my-matrix}
					\succeq 0
					\quad
					i=1,\ldots,m
			\end{array}
		$$
		which can be transformed to SDP in \definitionname~\ref{definition:semidefinite programming},
		thus, SDP reduces to SOCP
	\item
		hence, SOCP is specialization of SDP
	\eit
\eit


\myfoilhead{Determinant maximization problems}

\begin{mydefinition}{determinant maximization problems}
	convex optimization problem with generalized inequality constraints
	in \definitionname~\ref{definition:convex optimization with generalized inequality constraints}\
	with $\xdomain=\reals^n$
	of form
	$$
		\begin{array}{ll}
			\mbox{minimize} &
				-\log \det (x_1C_1 + \cdots + x_n C_n + D)
				+c^Tx
			\\
			\mbox{subject to} &
				x_1F_1 + \cdots + x_nF_n + G \preceq 0
			\\ &
				-x_1C_1 - \cdots - x_nC_n - D \prec 0
			\\ &
				Ax = b
		\end{array}
	$$
	where
	$c\in\reals^n$,
	$C_1,\ldots,C_n,D\in\symset{l}$,
	$F_1,\ldots,F_n,G\in\symset{k}$,
	and
	$A\in\reals^{p\times n}$,
	called \define{determinant maximization problem}
	or simply \define{max-det problem}
	(since it maximizes determinant of (positive definite) matrix with constraints)
\end{mydefinition}

\bit
\item
	if $l=1$, $C_1=\cdots=C_n=0$, $D=1$,
	max-det problem reduces to SDP,
	hence \emph{SDP is specialization of max-det problem}
\eit


\myfoilhead{Diagrams for containment of convex optimization problems}

\bit
\item
	\figref{diagrams for containment of convex optimization problems}
	shows
	containment relations among convex optimization problems
\item
	vertical lines ending with filled circles indicate existence of direct reductions,
	\ie, optimization problem transformations to special cases
\eit

\begin{figure}
\begin{center}
	\mypsfrag{LP}{LP}
	\mypsfrag{QP}{QP}
	\mypsfrag{QCQP}{QCQP}
	\mypsfrag{SOCP}{SOCP}
	\mypsfrag{SDP}{SDP}
	\mypsfrag{max-det}{max-det}
	\includegraphics[width=.7\textwidth]{figures/cvx_opt_diagrams}%
	\idxfig{diagrams for containment of convex optimization problems}
	\label{fig:diagrams for containment of convex optimization problems}
\end{center}
\end{figure}


\titlefoil{Duality}{Duality}

\myfoilhead{Lagrangian}

\begin{mydefinition}{Lagrangian}
	for optimization problem in \definitionname~\ref{definition:optimization problems}\
	with nonempty domain $\optdomain$,
	function $L:\optdomain \times \reals^m \times \reals^p \to \reals$
	defined by
	$$
		L(x,\lambda, \nu) = \fobj(x) + \lambda^T \fie(x) + \nu^T \feq(x)
	$$
	called \define{Lagrangian} associated with the optimization problem
	where
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		$\lambda$, called \define{Lagrange multiplier associated inequality constraints} $\fie(x)\preceq0$
	\iitem
		$\lambda_i$, called \define{Lagrange multiplier associated $i$-th inequality constraint} $\fie_i(x)\leq0$
	\iitem
		$\nu$, called \define{Lagrange multiplier associated equality constraints} $\feq(x)=0$
	\iitem
		$\nu_i$, called \define{Lagrange multiplier associated $i$-th equality constraint} $\feq_i(x)=0$
	\iitem
		$\lambda$ and $\nu$,
		called \define{dual variables} or \define{Lagrange multiplier vectors} associated with the optimization problem
	\eit
\end{mydefinition}
\index{Lagrangian}%
\index{Lagrange, Joseph-Louis!Lagrangian}


\myfoilhead{Lagrange dual functions}

\begin{mydefinition}{Lagrange dual functions}
	for optimization problem in \definitionname~\ref{definition:optimization problems}
	for which Lagrangian is defined,
	function $g:\reals^m \times \reals^p \to \reals\cup \{-\infty\}$
	defined by
	$$
		g(\lambda,\nu)
		=
		\inf_{x\in\optdomain} L(x,\lambda,\nu)
		=
		\inf_{x\in\optdomain} \left(\fobj(x) + \lambda^T \fie(x) + \nu^T \feq(x)\right)
	$$
	called
	\define{Lagrange dual function}
	or just
	\define{dual function}
	associated with the optimization problem
\end{mydefinition}
\index{Lagrange dual functions}%
\index{Lagrange, Joseph-Louis!Lagrange dual functions}

\bit
\item
	$g$ is \emph{(always) concave function} (even when optimization problem is not convex)
	\bit
	\iitem
		since is pointwise infimum of linear (hence concave) functions is concave
	\eit

\vitem
	$g(\lambda,\nu)$ provides
	lower bound for optimal value of associated optimization problem,
	\ie,
	$$
		g(\lambda,\nu) \leq p^\ast
	$$
	for every $\lambda\succeq0$ \proofref{Lagrange dual is lower bound for optimal value}

\vitem
	$(\lambda,\nu) \in \set{(\lambda,\nu)}{\lambda\succeq0, g(\lambda,\nu)>-\infty}$,
	said to be \define{dual feasible}
	\pagelabel{page:dual feasible}
\eit
\vfill


\myfoilhead{Dual function examples}

\bit
\item
	LS solution of linear equations
	$$
		\lssollineqs{primal}
	$$
	\bit
	\item
		Lagrangian - $L(x,\nu) = x^T x + \nu^T(Ax-b)$
	\item
		Lagrange dual function%
		\pagelabel{page:dual function of LS solution of lines equations}
		$$
			\lssollineqs{dual fcn}
		$$
	\eit

\item
	standard form LP
	$$
		\begin{array}{ll}
			\mbox{minimize} &
				c^Tx
			\\
			\mbox{subject to} &
				Ax = b
			\\ &
				x\succeq 0
		\end{array}
	$$
	\bit
	\item
		Lagrangian - $L(x,\lambda,\nu) = c^T x - \lambda^T x + \nu^T(Ax-b)$
	\item
		Lagrange dual function%
		\pagelabel{page:dual function of standard form LP}
		$$
			g(\lambda,\nu) = \left\{\begin{array}{ll}
				-b^T\nu & A^T\nu - \lambda + c = 0
				\\
				-\infty & \mbox{otherwise}
			\end{array}\right.
		$$
		\bit
		\iitem
			hence, set of dual feasible points is \set{(A^T\nu + c,\nu)}{A^T\nu +c \succeq0}\
		\eit
	\eit

\item
	\emph{maximum cut}, sometimes called \emph{max-cut}, problem, which is NP-hard%
	\pagelabel{page:max-cut problem}
	$$
		\begin{array}{ll}
			\mbox{minimize} &
				x^T W x
			\\
			\mbox{subject to} &
				x_i^2 = 1
		\end{array}
	$$
	where $W\in\symset{n}$
	\bit
	\item
		Lagrangian - $L(x,\nu) = x^T(W+\diag(\nu))x - \ones^Tx$
	\item
		Lagrange dual function
		$$
			g(\nu) = \left\{\begin{array}{ll}
				-\ones^T\nu
				& W + \diag(\nu) \succeq 0
				\\
				-\infty & \mbox{otherwise}
			\end{array}\right.
		$$
		\bit
		\iitem
			hence, set of dual feasible points is \set{\nu}{W+\diag(\nu)\succeq0}\
		\eit
	\eit

\item
	some trivial problem
	$$
		\begin{array}{ll}
			\mbox{minimize} &
				f(x)
			\\
			\mbox{subject to} &
				x=0
		\end{array}
	$$
	\bit
	\item
		Lagrangian - $L(x,\nu) =f(x)+\nu^Tx$
	\item
		Lagrange dual function
		$$
			g(\nu) = \inf_{x\in\reals^n} (f(x)+\nu^Tx)
			= -\sup_{x\in\reals^n} ((-\nu)^Tx-f(x))
			= - f^\ast(-\nu)
		$$
		\bit
		\iitem
			hence, set of dual feasible points is $-\dom f^\ast$,
			and
			for every $f:\reals^n\to\reals$ and $\nu\in\reals^n$
			$$
				-f^\ast(-\nu) \leq f(0)
			$$
		\eit
	\eit

\item
	minimization with linear inequality and equality constraints
	$$
		\begin{array}{ll}
			\mbox{minimize} &
				f(x)
			\\
			\mbox{subject to} &
				Ax\preceq b
			\\ &
				Cx= d
		\end{array}
	$$
	\bit
	\item
		Lagrangian - $L(x,\lambda, \nu) = f(x) + \lambda^T(Ax-b) + \nu^T(Cx-d)$
	\item
		Lagrange dual function
		$$
			g(\nu) = -b^T\lambda - d^T\nu - f^\ast(-A^T \lambda - C^T\nu)
		$$
		\bit
		\iitem
			hence, set of dual feasible points
			is \set{(\lambda,\nu)}{-A^T\lambda - C^T\nu \in \dom f^\ast, \lambda\succeq 0}
		\eit
	\eit

\item
	equality constrained norm minimization
	$$
		\begin{array}{ll}
			\mbox{minimize} &
				\|x\|
			\\
			\mbox{subject to} &
				Ax = b
		\end{array}
	$$
	\bit
	\item
		Lagrangian - $L(x,\nu) = \|x\| + \nu^T(Ax-b)$
	\item
		Lagrange dual function
		$$
			g(\nu) = -b^T\nu -\sup_{x\in\reals^n} ((-A^T\nu)^Tx - \|x\|)
			= \left\{\begin{array}{ll}
				-b^T \nu&\|A^T\nu\|_\ast\leq1
				\\
				- \infty & \mbox{otherwise}
			\end{array}\right.
		$$
		\bit
		\iitem
			hence, set of dual feasible points
			is \set{\nu}{\|A^T\nu\|_\ast \leq1}
		\eit
	\eit

\item
	entropy maximization
	$$
		\entmax{primal}
	$$
	where domain of objective function is \pprealk{n}\
	\bit
	\item
		Lagrangian - $L(x,\lambda,\nu) = \sum_{i=1}^n x_i\log x_i + \lambda^T(Ax-b) + \nu(\ones^Tx-1)$
	\item
		Lagrange dual function%
		\pagelabel{page:dual function of entropy maximization}

		$$
			g(\lambda,\nu) = \entmax{dual fcn}
		$$
		obtained using $f^\ast(y) = \sum_{i=1}^n \exp(y_i-1)$
		where $a_i$ is $i$-th column vector of $A$
	\eit

\item
	minimum volume covering ellipsoid
	$$
		\minvolcovering{primal}
	$$
	where domain of objective function is \posdefset{n}\
	\bit
	\item
		Lagrangian - $L(X,\lambda) = -\log \det X + \sum_{i=1}^m \lambda_i(a_i^T X a_i - 1)$
	\item
		Lagrange dual function%
			\pagelabel{page:dual function of minimum volume covering ellipsoid}
		$$
			g(\lambda)
				= \minvolcovering{dual fcn}
		$$
		obtained using $f^\ast(Y) = -\log\det(-Y) - n$
	\eit
\eit


\myfoilhead{Best lower bound}

\bit
\item
	for every $(\lambda,\nu)$ with $\lambda\succeq 0$,
	Lagrange dual function $g(\lambda,\nu)$ (in \definitionname~\ref{definition:Lagrange dual functions})
	provides lower bound for optimal value $p^\ast$
	for optimization problem in \definitionname~\ref{definition:optimization problems}\

\vitem
	natural question to ask is
	\bit
	\item
		how good is the lower bound?
	\item
		what is best lower bound we can achieve?
	\eit

\vitem
	these questions lead to definition of \emph{Lagrange dual problem}\
\eit
\vfill


\myfoilhead{Lagrange dual problems}

\begin{mydefinition}{Lagrange dual problems}
	for optimization problem in \definitionname~\ref{definition:optimization problems},
	optimization problem
	$$
		\begin{array}{ll}
			\mbox{maximize} &
				g(\lambda,\nu)
			\\
			\mbox{subject to} &
				\lambda \succeq 0
		\end{array}
	$$
	called \define{Lagrange dual problem}
	associated with problem in \definitionname~\ref{definition:optimization problems}
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		original problem in \definitionname~\ref{definition:optimization problems},
		(somestime) called \define{primal problem}\
	\iitem
		domain is $\reals^m\times \reals^p$
	\iitem
		\define{dual feasibility} defined in page~\pageref{page:dual feasible},
		\ie, $(\lambda,\nu)$ satisfying
		$
			\lambda \succeq 0 \quad g(\lambda,\nu) > -\infty
		$
		indeed means
		feasibility for Lagrange dual problem
	\iitem
		$d^\ast = \sup\set{g(\lambda,\nu)}{\lambda\in\reals^m,\:\nu\in\reals^p,\:\lambda\succeq 0}$,
		called \define{dual optimal value}
	\iitem
		$(\lambda^\ast,\nu^\ast) = \argsup\set{g(\lambda,\nu)}{\lambda\in\reals^m,\:\nu\in\reals^p,\:\lambda\succeq 0}$,
		said to be \define{dual optimal} or called \define{optimal Lagrange multipliers} (if exists)
	\eit
\end{mydefinition}
\index{Lagrange dual problems}%
\index{Lagrange, Joseph-Louis!Lagrange dual problems}

\bit
\item
	Lagrange dual problem in \definitionname~\ref{definition:Lagrange dual problems}\
	is convex optimization (even though original problem is not)
	since $g(\lambda,\nu)$ is always convex
\eit
\vfill


\myfoilhead{Making dual constraints explicit dual problems}

\bit
\item
	(out specific) way we define Lagrange dual function in \definitionname~\ref{definition:Lagrange dual functions}\
	as function $g$ of $\reals^m \times \reals^p$ into $\reals\cup\{-\infty\}$,
	\ie,
	$\dom g = \reals^n\times\reals^p$

\vitem
	however,
	in many cases,
	feasible set \set{(\lambda,\nu)}{\lambda \succeq 0 \quad g(\lambda,\nu) > -\infty}\
	is proper subset of $\reals^n\times\reals^p$

\vitem
	can make this implicit feasibility condition
	explicit by adding it as constraint
	(as shown in following examples)%
	\pagelabel{page:make implicit dual feasibility explicit}
\eit


\myfoilhead{Lagrange dual problems associated with LPs}

\bit
\item
	standard form LP
	\bit
	\item
		primal problem
		$$
			\begin{array}{ll}
				\mbox{minimize} &
					c^Tx
				\\
				\mbox{subject to} &
					Ax = b
				\\ &
					x\succeq 0
			\end{array}
		$$
	\item
		Lagrange dual problem
		$$
			\begin{array}{ll}
				\mbox{maximize} &
					g(\lambda,\nu) = \left\{\begin{array}{ll}
						-b^T\nu & A^T\nu - \lambda + c = 0
						\\
						-\infty & \mbox{otherwise}
					\end{array}\right.
				\\
				\mbox{subject to} &
					\lambda \succeq 0
			\end{array}
		$$
		(refer to page~\pageref{page:dual function of standard form LP}\
			for Lagrange dual function)
		\bit
		\iitem
			can make dual feasibility explicit by adding it to constraints
			as mentioned on page~\pageref{page:make implicit dual feasibility explicit}\
			$$
				\begin{array}{ll}
					\mbox{maximize} &
						-b^T\nu
					\\
					\mbox{subject to} &
						\lambda \succeq 0
					\\ &
						A^T\nu - \lambda + c = 0
				\end{array}
			$$
		\iitem
			can further simplify problem
			$$
				\begin{array}{ll}
					\mbox{maximize} &
						-b^T\nu
					\\
					\mbox{subject to} &
						A^T\nu + c \succeq 0
				\end{array}
			$$
		\eit
	\item
		last problem is \emph{inequality form LP}
	\item
		all three problems are equivalent,
		but \emph{not} same problems

	\item
		will, however, with abuse of terminology,
		refer to all three problems
		as Lagrange dual problem
	\eit

\item
	inequality form LP
	\bit
	\item
		primal problem
		$$
			\begin{array}{ll}
				\mbox{minimize} &
					c^Tx
				\\
				\mbox{subject to} &
					Ax \preceq b
			\end{array}
		$$
	\item
		Lagrangian
		$$
			L(x,\lambda) = c^Tx + \lambda^T(Ax-b)
		$$
	\item
		Lagrange dual function
		$$
			g(\lambda)
%			=
%				\inf_{x\in\reals^n} L(x,\lambda)
			=
				-b^T\lambda + \inf_{x\in\reals^n} (c+A^T\lambda)^T x
			=
				\left\{\begin{array}{ll}
				-b^T\lambda & A^T\lambda + c =0
				\\
				-\infty & \mbox{otherwise}
			\end{array}\right.
		$$

	\item
		Lagrange dual problem
		$$
			\begin{array}{ll}
				\mbox{maximize} &
					g(\lambda)
					= \left\{\begin{array}{ll}
						-b^T\lambda & A^T\lambda + c =0
						\\
						-\infty & \mbox{otherwise}
					\end{array}\right.
				\\
				\mbox{subject to} &
					\lambda \succeq 0
			\end{array}
		$$
		\bit
		\iitem
			can make dual feasibility explicit by adding it to constraints
			as mentioned on page~\pageref{page:make implicit dual feasibility explicit}\
			$$
				\begin{array}{ll}
					\mbox{maximize} &
						-b^T\nu
					\\
					\mbox{subject to} &
						A^T\lambda + c = 0
					\\ &
						\lambda \succeq 0
				\end{array}
			$$
		\eit
	\item
		dual problem is \emph{standard form LP}
	\eit

\vitem
	thus,
	dual of standard form LP is inequality form LP
	and vice versa

\vitem
	also, for both cases, dual of dual is same as primal problem
\eit


\myfoilhead{Lagrange dual problem of equality constrained optimization problem}

\bit
\item
	equality constrained optimization problem
	$$
		\begin{array}{ll}
			\mbox{minimize} &
				\fobj(x)
			\\
			\mbox{subject to} &
				Ax = b
		\end{array}
	$$

\vitem
	dual function
	\begin{eqnarray*}
			g(\nu)
			&
			=
			&
			\inf_{x\in\dom \fobj} (\fobj(x) + \nu^T(Ax-b))
			=
			-b^T\nu
			- \sup_{x\in\dom \fobj}(-\nu^TAx -\fobj(x))
		\\
		&
		=
		&
			-b^T\nu - \fobj^\ast(-A^T\nu)
	\end{eqnarray*}

\vitem
	dual problem
	$$
		\begin{array}{ll}
			\mbox{maximize} &
				-b^T\nu - \fobj^\ast(-A^T\nu)
		\end{array}
	$$
\eit
\vfill


\myfoilhead{Lagrange dual problem associated with equality constrained quadratic program}

\bit
\item
	strictly convex quadratic problem
	$$
		\begin{array}{ll}
			\mbox{minimize} &
				\fobj(x) = x^TPx + q^T x + r
			\\
			\mbox{subject to} &
				Ax=b
		\end{array}
	$$
	\bit
	\vitem
		conjugate function of objective function
		$$
			\fobj^\ast(x)
%			= \sup_{y} (x^Ty -y^TPy - q^Ty - r)
%			= -\inf_{y} ( y^TPy + (q-x)^Ty + r)
%			= -(-(q-x)^TP^{-1}(q-x)/4 + r)
%			= (q-x)^TP^{-1}(q-x)/4 - r
			= (x-q)^TP^{-1}(x-q)/4 - r
			=
				x^TP^{-1}x/4
				-q^TP^{-1}x/2
				+ q^TP^{-1}q/4
				-r
		$$
	\vitem
		dual problem
		$$
			\begin{array}{ll}
				\mbox{maximize} &
%					-b^T \nu 	-(\nu^T AP^{-1}A^T\nu /4 + q^TP^{-1}A^T\nu/2 + q^TP^{-1}q/4 -r)
%					\\
%					&
%					-\nu^T (AP^{-1}A^T)\nu /4 -b^T \nu - q^TP^{-1}A^T\nu/2- q^TP^{-1}q/4+r
%					\\
%					&
					-\nu^T (AP^{-1}A^T)\nu /4 -(b + A P^{-1} q/2)^T\nu - q^TP^{-1}q/4 +r
			\end{array}
		$$
	\eit


\eit
\vfill


\myfoilhead{Lagrange dual problems associated with nonconvex quadratic problems}

\bit
\item
	primal problem
	$$
		\noncvxquadprob{primal}
	$$
	where $A\in\symset{n}$, $A\not\in\possemidefset{n}$, and $b\in\reals^n$
	\bit
	\item
		since $A\not\succeq 0$, not convex optimization problem
	\item
		sometimes called \define{trust region problem}
		arising minimizing second-order approximation of function
		over bounded region
	\eit

\vitem
	Lagrange dual function
	$$
		g(\lambda)
		=
		\noncvxquadprob{dual fcn}
	$$
	where $(A+\lambda I)^\dagger$ is pseudo-inverse of $A+\lambda I$
\vitem
	Lagrange dual problem%
		\pagelabel{page:dual problem of trust region nonconvex quadratic problems}
	$$
		\noncvxquadprob{dual}
	$$
	where optimization variable is $\lambda \in\reals$
	\bit
	\item
		note we do not need constraint $\lambda \geq0$
		since it is implied by $A+\lambda I \succeq 0$
	\item
		though not obvious from what it appears to be,
		it is (of course) convex optimization problem
		(by definition of Lagrange dual function, \ie, \definitionname~\ref{definition:Lagrange dual functions})
	\item
		can be expressed ar
		$$
			\begin{array}{ll}
				\mbox{maximize} &
					-\sum_{i=1}^n (q_i^Tb)^2/(\lambda_i + \lambda) - \lambda
				\\
				\mbox{subject to} &
					\lambda \geq - \lambda_\mathrm{min}(A)
			\end{array}
		$$
		where $\lambda_i$ and $q_i$ are eigenvalues and corresponding orthogormal eigenvectors of $A$,
		when $\lambda_i + \lambda=0$ for some $i$,
		we interpret $(q_i^Tb)^2/0$ as 0 if $q_i^T0$ and $\infty$ otherwise
	\eit
\eit


\myfoilhead{Weak duality}

\bit
\item
	since $g(\lambda,\nu)\leq p^\ast$ for every $\lambda\succeq 0$,
	we have
	$$
		d^\ast = \sup\set{g(\lambda,\nu)}{\lambda\in\reals^m,\:\nu\in\reals^p,\:\lambda\succeq 0}
		\leq
		p^\ast
	$$
\eit
\begin{mydefinition}{weak duality}
	property that
	that
	optimal value of optimization problem (in \definitionname~\ref{definition:optimization problems})
	is always no less than
	optimal value of Lagrange daul problem (in \definitionname~\ref{definition:Lagrange dual problems})
	$$
		d^\ast \leq p^\ast
	$$
	called \define{weak duality}
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		$d^\ast$ is best lower bound for primal problem
		that can be obtained from Lagrange dual function (by definition)
	\iitem
		weak duality holds even when $d^\ast$ or/and $p^\ast$ are not finite, \eg\
		\bit
		\item
			\emph{if primal problem is unbounded below} so that $p^\ast=-\infty$,
			must have $d^\ast = -\infty$,
			\ie, \emph{dual problem is infeasible}\
		\item
			conversely,
			\emph{if dual problem is unbounded above} so that $d^\ast = \infty$,
			must have $p^\ast=\infty$,
			\ie, \emph{primal problem is infeasible}\
		\eit
	\eit
\end{mydefinition}


\myfoilhead{Optimal duality gap}

\begin{mydefinition}{optimal duality gap}
	difference between
	optimal value of optimization problem (in \definitionname~\ref{definition:optimization problems})
	and
	optimal value of Lagrange daul problem (in \definitionname~\ref{definition:Lagrange dual problems}),
	\ie\
	$$
		p^\ast - d^\ast
	$$
	called \define{optimal duality gap}
\end{mydefinition}

\bit
\item
	sometimes used for lower bound of optimal value of problem which is difficult to solve
	\bit
	\item
		for example,
		dual problem
		of max-cut problem (on page~\pageref{page:max-cut problem}),
		which is NP-hard,
		is
		$$
			\begin{array}{ll}
				\mbox{minimize} &
					-\ones^T \nu
				\\
				\mbox{subject to} &
					W + \diag(\nu) \succeq 0
			\end{array}
		$$
		where optimization variable is $\nu\in\reals^n$
		\bit
		\iitem
			the dual problem can be solved very efficiently using polynomial time algorithms
			while primal problme \emph{cannot} be solved unless $n$ is very small
		\eit
	\eit
\eit


\myfoilhead{Strong duality}

\begin{mydefinition}{strong duality}
	if
	optimal value of optimization problem (in \definitionname~\ref{definition:optimization problems})
	equals to
	optimal value of Lagrange daul problem (in \definitionname~\ref{definition:Lagrange dual problems}),
	\ie\
	$$
		p^\ast = d^\ast
	$$
	\define{strong duality} said to hold
\end{mydefinition}

\bit
\item
	strong duality does \emph{not} hold in general
	\bit
	\item
		if it held always, max-cut problem, which is NP-hard, can be solved in polynomial time,
		which would be one of biggest breakthrough in field of theoretical computer science
	\item
		may mean some of strongest cryptography methods, \eg, homeomorphic cryptography,
		can be broken
	\eit
\eit


\myfoilhead{Slater's theorem}

\bit
\item
	exist many conditions
	which guarantee strong duality,
	which are called \emph{constraint qualifications}
	- one of them is Slater's condition
\eit

\begin{mytheorem}{Slater's theorem}
	if
	optimization problem is convex (\definitionname~\ref{definition:convex optimization}),
	and exists feasible $x\in\optdomain$ contained in $\relint \optdomain$\
	such that
	$$
		\fie(x) \prec 0\quad \feq(x) = 0
	$$
	\emph{strong duality} holds
	(and \emph{dual optimum is attained when $d^\ast>-\infty$})
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		such condition, called \define{Slater's condition}
	\iitem
		such point, (sometimes) said to be \define{strictly feasible}
	\eit

	\shrinkspacewithintheoremslike
	when there are affine inequality constraints,
	can refine Slater's condition
	- if first $k$ inequality constraint functions $\fie_1$, \ldots, $\fie_k$ are affine,
	Slater's condition can be relaxed to
	$$
		\fie_i(x)\leq 0\;\;i=1,\ldots,k
		\quad
		\fie_i(x) < 0\;\;i=k+1,\ldots,m
		\quad
		\feq(x) = 0
%		Ax = b
	$$
\end{mytheorem}


\myfoilhead{Strong duality for LS solution of linear equations}

\bit
\item
	primal problem
	$$
		\lssollineqs{primal}
	$$

\vitem
	dual problem
	$$
		\lssollineqs{dual}
	$$

	(refer to page~\pageref{page:dual function of LS solution of lines equations} for Lagrange dual function)

\vitem
	``dual is always feasible''
	and
	``primal is feasible $\Rightarrow$ Slater's condition holds'',
	thus
	Slater's theorem (\theoremname~\ref{theorem:Slater's theorem})
	implies,
	exist only three cases
	\bit
	\item
		$(d^\ast = p^\ast \in \reals)$
		or
		$(d^\ast \in \reals\:\&\: p^\ast = \infty)$
		or
		$(d^\ast = p^\ast = \infty)$
	\eit

\vitem
	if primal is infeasible, though,
	$b\not\in\range(A)$,
	thus
	exists $z$, such that $A^Tz=0$ and $b^Tz \neq0$,
	then line \set{tz}{t\in\reals}\ makes dual problem unbounded above,
	hence $d^\ast=\infty$

\vitem
	hence, \emph{strong duality always holds},
	\ie, $(d^\ast= p^\ast \in \reals)$ or $(d^\ast = p^\ast = \infty)$
\eit


\myfoilhead{Strong duality for LP}

\bit
\item
	every LP either is infeasible or satisfies Slater's condition

\vvitem
	dual of LP is LP,
	hence, Slater's theorem (\theoremname~\ref{theorem:Slater's theorem})
	implies
	\bit
	\vitem
		if primal is feaisble,
		either $(d^\ast=p^\ast= -\infty)$ or $(d^\ast=p^\ast\in\reals)$
	\vitem
		if dual is feaisble,
		either $(d^\ast=p^\ast= \infty)$ or $(d^\ast=p^\ast\in\reals)$
	\vitem
		only other case left is $(d^\ast=-\infty\;\&\;p^\ast= \infty)$
		\bit
		\viitem
			indeed, this pathological case can happen
		\eit
	\eit
\eit


\myfoilhead{Strong duality for entropy maximization}

\bit
\item
	primal problem
	$$
		\entmax{primal}
	$$

\vitem
	dual problem
	(refer to page~\pageref{page:dual function of entropy maximization} for Lagrange dual function)
	$$
		\entmax{dual}
	$$

\vitem
	dual problem is feasible,
	hence, Slater's theorem (\theoremname~\ref{theorem:Slater's theorem})
	implies,
		if exists $x\succ 0$ with $Ax \preceq b$ and $\ones^T x =1$,
		strong duality holds,
		and indeed $d^\ast=p^\ast\in\reals$

\vitem
	by the way,
	can simplify dual problem by maximizing dual objective function over $\nu$
	$$
		\entmax{simplied dual}
	$$
	which is geometry program in convex form
	(\propositionname~\ref{proposition:geometric programming in convex form})
	with nonnegativity contraint
\eit
\vfill


\myfoilhead{Strong duality for minimum volume covering ellipsoid}

\bit
\item
	primal problem
	$$
		\minvolcovering{primal}
	$$
	where $\optdomain=\posdefset{n}$

\vitem
	dual problem
	$$
		\minvolcovering{dual}
	$$
	(refer to page~\pageref{page:dual function of minimum volume covering ellipsoid} for Lagrange dual function)

\vitem
	$X=\alpha I$ with large enough $\alpha>0$ satisfies primal's constraints,
	hence Slater's condition \emph{always} holds,
	thus,
	\emph{strong duality always holds},
	\ie, $(d^\ast = p^\ast \in \reals)$ or $(d^\ast = p^\ast = -\infty)$

\vitem in fact, $\range(a_1,\ldots,a_m) = \reals^n$ \iaoi\ $d^\ast=p^\ast\in\reals^n$
\eit


\myfoilhead{Strong duality for trust region nonconvex quadratic problems}

\bit
\item one of rare occasions
	in which
	\emph{strong duality obtains for nonconvex problems}
\item
	primal problem
	$$
		\noncvxquadprob{primal}
	$$
	where $A\in\symset{n}$, $A\not\in\possemidefset{n}$, and $b\in\reals^n$

\vitem
	Lagrange dual problem (page~\pageref{page:dual problem of trust region nonconvex quadratic problems})
	$$
		\noncvxquadprob{dual}
	$$

\vitem
	\emph{strong duality always holds}
	and $d^\ast=p^\ast\in\reals$
	(since dual problem is feasible - large enough $\lambda$ satisfies dual constraints)

\vitem
	in fact, exists stronger result
	- \emph{strong dual holds} for optimization problem with quadratic objective
	and \emph{one} quadratic inequality constraint,
	provided Slater's condition holds
\eit


\myfoilhead{Matrix games using mixed strategies}%
\pagelabel{page:Matrix games using mixed strategies}

\bit
\item
	matrix game - consider game with two players $A$ and $B$
	\bit
	\item
		player $A$ makes choice $1\leq a\leq n$,
		player $B$ makes choice $1\leq b\leq m$,
		then player $A$ makes payment of $P_{ab}$ to player $B$
	\item
		matrix $P\in\reals^{n\times m}$, called \define{payoff matrix}
	\item
		player $A$ tries to pay as little as possible
		\&
		player $B$ tries to received as much as possible
	\item
		players use \define{randomized or mixed strategies},
		\ie, each player makes choice randomly and independently of other player's choice
		according to probability distributions
		$$
			\Prob(a=i) = u_i\; i=1\leq i\leq n
			\quad
			\Prob(b=j) = v_j\; i=1\leq j\leq m
		$$

	\eit

\vitem
	expected payoff (from player $A$ to player $B$)
	$$
		\sum_i \sum_j u_iv_jP_{ij} = u^TPv
	$$

\vitem
	assume player $A$'s strategy is known to play $B$
	\bit
	\item
		player $B$ will choose $v$ to maximize $u^TPv$
		$$
			\sup\set{u^TPv}{v\succeq 0,\; \ones^Tv=1}
			= \max_{1\leq j\leq m} (P^Tu)_j
		$$
	\item
		player $A$ (assuming that player $B$ will employ above strategy to maximize payment)
		will choose $u$ to minimize payment
		$$
			\begin{array}{ll}
				\mbox{minimize} & \max_{1\leq j\leq m} (P^Tu)_j
				\\
				\mbox{subject to} &
					u\succeq 0\quad \ones^Tu=1
			\end{array}
		$$
	\eit

\vitem
	assume player $B$'s strategy is known to play $A$
	\bit
	\item
		then player $B$ will do same to maximize payment
		(assuming that player $A$ will employ such strategy to minimize payment)
		$$
			\begin{array}{ll}
				\mbox{maximize} & \min_{1\leq i\leq n} (Pv)_i
				\\
				\mbox{subject to} &
					v\succeq 0\quad \ones^Tv=1
			\end{array}
		$$
	\eit
\eit


\myfoilhead{Strong duality for matrix games using mixed strategies}

\bit
\item
	in matrix game,
	can guess
	in frist came,
	player $B$ has advantage over player $A$ because $A$'s strategy's exposed to $B$,
	and vice versa,
	hence
	optimal value of first problem is greater than that of second problem

\vitem
	surprising,
	no one has advantage over the other,
	\ie, optimal values of two problems are \emph{same}
	-
	will show this

\vitem
	first observe both problems are (convex) piecewise-linear optimization problems

\vitem
	formulate first problem as LP
	$$
		\begin{array}{ll}
			\mbox{minimize} &
				t
			\\
			\mbox{subject to} &
				u\succeq 0 \quad \ones^T u =1 \quad P^T u \preceq t\ones
		\end{array}
	$$
	\bit
	\item
		Lagrangian
		$$
			L(u,t,\lambda_1, \lambda_2,\nu) = \nu + (1-\ones^T\lambda_1)t + (P\lambda_1 - \nu \ones - \lambda_2)^Tu
		$$
	\item
		Lagrange dual function
		$$
			g(\lambda_1, \lambda_2,\nu) = \left\{\begin{array}{ll}
				\nu & \ones^T\lambda_1 = 1 \;\&\; P\lambda_1 - \nu \ones = \lambda_2
				\\
				-\infty & \mbox{otherwise}
			\end{array}\right.
		$$
	\eit

\vitem
	Lagrange dual problem
	$$
		\begin{array}{ll}
			\mbox{maximize} &
				\nu
			\\
			\mbox{subject to} &
				\ones^T\lambda_1 = 1 \quad P\lambda_1 - \nu \ones = \lambda_2
			\\ &
				\lambda_1 \succeq 0 \quad \lambda_2 \succeq 0
		\end{array}
	$$

\vitem
	eliminating $\lambda_2$ gives
	below Lagrange dual problem
	$$
		\begin{array}{ll}
			\mbox{maximize} &
				\nu
			\\
			\mbox{subject to} &
				\lambda_1 \succeq 0 \quad \ones^T\lambda_1 = 1 \quad P\lambda_1 \succeq \nu \ones
		\end{array}
	$$
	which is equivalent to second problem in matrix game


\vitem
	weak duality confirms ``player who knows other player's strategy has advantage or on par''
\vitem
	moreoever,
	primal problem satisfies Slater's condition, hence \emph{strong duality {always} holds},
	and dual is feasible,
	hence $d^\ast=p^\ast\in\reals$,
	\ie, regardless of who knows other player's strategy,
	no player has advantage

\eit


\myfoilhead{Geometric interpretation of duality}

\bit
\item
	assume (not necessarily convex) optimization problem
	in \definitionname~\ref{definition:optimization problems}
%	with $\xdomain = \reals^n$

\item
	define graph
	$$
		G = \set{(\fie(x), \feq(x), \fobj(x))}{x\in\optdomain}
		\subset \reals^m \times \reals^p \times \reals
	$$
%
%\item then
%	$$
%		p^\ast = \inf\set{t}{(u,v,t) \in G, u\preceq 0, v = 0}
%	$$

\item
	for every $\lambda\succeq 0$ and $\nu$
	\begin{eqnarray*}
		\lefteqn{
			p^\ast = \inf\set{t}{(u,v,t) \in G, u\preceq 0, v = 0}
		}
		\\
		&
		\geq
		&
			\inf\set{t+\lambda^Tu + \nu^T v}{(u,v,t) \in G, u\preceq 0, v = 0}
		\\
		&
		\geq
		&
			\inf\set{t+\lambda^Tu + \nu^T v}{(u,v,t) \in G}
		=
			g(\lambda,\nu)
	\end{eqnarray*}
	where second inequality comes from
	$
		\set{(u,v,t)}{(u,v,t) \in G, u\preceq 0, v = 0} \subset G
	$

\item
	above establishes \emph{weak duality}
	\emph{using graph}

\item
	last equality implies that
	$$(\lambda, \nu, 1)^T (u,v,t) \geq g(\lambda,\nu)$$
	hence if $g(\lambda,\nu) > -\infty$,
	$(\lambda, \nu, 1)$ and $g(\lambda,\nu)$ define
	\emph{nonvertical}
	\emph{supporting hyperplane} for $G$
	- nonvertical because third component is nonzero
\eit

\def\dualitygraphunitsize{1em}

\bit
\item
	\figref{geometric interpretation of duality - 1}\
	shows $G$ as area inside closed curve
	contained in $\reals^m\times\reals^p\times\reals$ where $m=1$ and $p=0$
	as primal optimal value $p^\ast$ and supporting hyperplane
	$\lambda u + t = g(\lambda)$
\eit

\begin{figure}
\begin{center}
%\setlength{\fboxsep}{0pt}
%\fbox{
	\dualitygraphone{\dualitygraphunitsize}
%}
	\idxfig{geometric interpretation of duality - 1}
	\label{fig:geometric interpretation of duality - 1}
\end{center}
\end{figure}

\bit
\item
	\figref{geometric interpretation of duality - 2}\
	shows three hyperplanes determined by three values for $\lambda$,
	one of which $\lambda^\ast$ is optimal solution for dual problem
\eit

\begin{figure}
\begin{center}
%\setlength{\fboxsep}{0pt}
%\fbox{
	\dualitygraphtwo{\dualitygraphunitsize}
%}
	\idxfig{geometric interpretation of duality - 2}
	\label{fig:geometric interpretation of duality - 2}
\end{center}
\end{figure}


\myfoilhead{Epigraph interpretation of duality}

\bit
\item
	define extended graph over $G$ - sort of epigraph of $G$
	\begin{eqnarray*}
%	$$
	\lefteqn{
		H = G + \preals^m \times \{0\} \times \preals
	}
	\\
	&
	=
	&
		\set{(u, v, t)}{x\in\optdomain, \fie(x) \preceq u, \feq(x) = v, \fobj(x)\leq t }
%	$$
	\end{eqnarray*}

\item if $\lambda\succeq 0$, $g(\lambda,\nu) = \inf\set{(\lambda,\nu,1)^T(u,v,t)}{(u,v,t) \in H}$, thus
	$$
		(\lambda,\nu,1)^T (u,v,t) \geq g(\lambda,\nu)
	$$
	defines nonvertical supporting hyperplane for $H$

\item
	now $p^\ast = \inf\set{t}{(0,0,t)\in H}$, hence $(0,0,p^\ast) \in \boundary H$, hence
	$$
		p^\ast =(\lambda,\nu,1)^T (0,0,p^\ast) \geq g(\lambda,\nu)
	$$

\item
	once again establishes \emph{weak duality}

\item
	\figref{geometric interpretation of duality - 3}\
	shows epigraph interpretation
\eit

\begin{figure}
\begin{center}
%\setlength{\fboxsep}{0pt}
%\fbox{
	\dualitygraphthree{\dualitygraphunitsize}
%}
	\idxfig{geometric interpretation of duality - 3}
	\label{fig:geometric interpretation of duality - 3}
\end{center}
\end{figure}


\myfoilhead{Proof of strong duality under constraint qualification}

\bit
\item
	now we show proof of strong duality
	- this is one of rare cases where proof is shown in main slides
	instead of ``selected proofs'' section like Galois theory
	since - (I hope) it will give you some good intuition about
	why strong duality holds for (most) convex optimization problems

\item
	assume Slater's condition holds,
	\ie,
	\fobj\ and \fie\ are convex, \feq\ is affine,
	and
	exists $x\in\optdomain$
	such that $\fie(x) \prec 0$ and $\feq(x) = 0$

\item
	further assume \optdomain\ has interior (hence, $\relint \optdomain = \interior{\optdomain}$
	and $\rank A=p$

\item
	assume $p^\ast\in\reals$ - since exists feasible $x$, the other possibility is $p^\ast = -\infty$,
	but then, $d^\ast = -\infty$, hence strong duality holds

\item
	$H$ is convex \proofref{epigraph of convex optimization is convex}

\item
	now define
	$$
		B = \set{(0,0,s)\in\reals^m\times\reals^p\times\reals}{s<p^\ast}
	$$

\item
	then $B\cap H=\emptyset$, hence \theoremname~\ref{theorem:separating hyperplane theorem}\
	implies exists separable hyperplane with $(\tilde{\lambda}, \tilde{\nu}, \mu)\neq 0$ and $\alpha$
	such that
	\begin{eqnarray*}
		(u,v,t) \in H
		&\Rightarrow&
		\tilde{\lambda}^T u + \tilde{\nu}^T v + \mu t \geq \alpha
	\\
		(u,v,t) \in B
		&\Rightarrow&
		\tilde{\lambda}^T u + \tilde{\nu}^T v + \mu t \leq \alpha
	\end{eqnarray*}

\item
	then $\tilde{\lambda} \succeq 0$ \& $\mu\geq0$ - assume $\mu>0$
	\bit
	\item
		can prove when $\mu=0$, but kind of tedius, plus,
		whole purpose is provide good intuition,
		so will not do it here
	\eit

\item
	above second inequality implies $\mu p^\ast \leq \alpha$ and
	for some $x\in\optdomain$\
	$$
		\mu L(x,\tilde{\lambda}/\mu, \tilde{\nu}/\mu)
		=
		\tilde{\lambda}^T \fie(x) + \tilde{\nu}^T \feq(x) + \mu \fobj(x) \geq \alpha \geq \mu p^\ast
	$$
	thus,
	$$
		g(\tilde{\lambda}/\mu, \tilde{\nu}/\mu) \geq p^\ast
	$$

\item
	finally, weak duality implies
	$$
		g(\lambda,\nu) = p^\ast
	$$
	where $\lambda = \tilde{\lambda}/\mu$ \& $\nu = \tilde{\nu}/\mu$
\eit

\begin{figure}
\begin{center}
%\setlength{\fboxsep}{0pt}
%\fbox{
	\dualitygraphfour{\dualitygraphunitsize}
%}
	\idxfig{geometric interpretation of duality - 4}
	\label{fig:geometric interpretation of duality - 4}
\end{center}
\end{figure}


\myfoilhead{Max-min characterization of weak and strong dualities}

\bit
\item
	note
	\shrinkspacewithintheoremslike
	\begin{eqnarray*}
%		\lefteqn{
		\sup_{\lambda\geq 0, \nu} L(x,\lambda,\nu)
%		=
		&=&
		\sup_{\lambda\geq 0, \nu} \left(
			\fobj(x) + \lambda^T \fie(x) + \nu^T \feq(x)
		\right)
%		}
		\\
		&
		=
		&
		\left\{\begin{array}{ll}
			\fobj(x) & x\in\optfeasset
			\\
			\infty & \mbox{otherwise}
		\end{array}\right.
	\end{eqnarray*}

\vitem
	thus
	$
		p^\ast = \inf_{x\in\optdomain} \sup_{\lambda\succeq 0, \nu} L(x,\lambda,\nu)
	$
	whereas
	$
		d^\ast = \sup_{\lambda\succeq 0,\nu} \inf_{x\in\optdomain} L(x,\lambda,\nu)
	$

\vitem
	weak duality means
	$$
		\sup_{\lambda\succeq 0, \nu} \inf_{x\in\optdomain} L(x,\lambda,\nu)
		\leq
		\inf_{x\in\optdomain} \sup_{\lambda\succeq 0, \nu} L(x,\lambda,\nu)
	$$

\vitem
	strong duality means
	$$
		\sup_{\lambda\succeq 0, \nu} \inf_{x\in\optdomain} L(x,\lambda,\nu)
		=
		\inf_{x\in\optdomain} \sup_{\lambda\succeq 0, \nu} L(x,\lambda,\nu)
	$$
\eit


\myfoilhead{Max-min inequality}

\bit
\item
	indeed, inequality\
	$
		\sup_{\lambda\succeq 0} \inf_{x\in\optdomain} L(x,\lambda,\nu)
		\leq
		\inf_{x\in\optdomain} \sup_{\lambda\succeq 0} L(x,\lambda,\nu)
	$
	holds for general case
\eit

\begin{myinequality}{max-min inequality}
	for $f:{X} \times {Y} \to \reals$
	$$
		\sup_{y\in{Y}} \inf_{x\in{X}} f(x,y)
		\leq
		\inf_{x\in{X}} \sup_{y\in{Y}} f(x,y)
	$$
	\proofref{max-min inequality}
\end{myinequality}

\begin{mydefinition}{strong max-min property}
	if below equality holds, we say
	$f$ (and $X$ and $Y$) satisfies \define{strong max-min property}
	or \define{saddle point property}
	$$
		\sup_{y\in{Y}} \inf_{x\in{X}} f(x,y)
		=
		\inf_{x\in{X}} \sup_{y\in{Y}} f(x,y)
	$$
\end{mydefinition}

\bit
\item
	this happens,
	\eg,
	$X=\optdomain$,
	$Y=\prealk{m} \times \reals^p$,
	$f$ is Lagrangian of
	optimization problem
	(in \definitionname~\ref{definition:optimization problems})
	for which strong duality holds
\eit


\myfoilhead{Saddle-points}

\begin{mydefinition}{saddle-points}
	for $f:X\times Y\to\reals$,
	pair $x^\ast\in X$ and $y^\ast\in Y$
	such that
	$$
		\left(
			\forall x \in X, y\in Y
		\right)
		\left(
			f(x^\ast,y) \leq f(x^\ast,y^\ast) \leq f(x,y^\ast)
		\right)
	$$
	called \define{saddle-point for $f$ (and $X$ and $Y$)}\
\end{mydefinition}

\bit
\vitem
	if assumption in \definitionname~\ref{definition:saddle-points}\ holds,
	$x^\ast$ minimizes $f(x,y^\ast)$ over $X$
	and
	$y^\ast$ maximizes $f(x^\ast,y)$ over $Y$
	$$
		\sup_{y\in Y} f(x^\ast,y)
			=
		f(x^\ast,y^\ast)
			=
		\inf_{x\in X} f(x,y^\ast)
	$$

	\bit
	\item
		strong max-min property (in \definitionname~\ref{definition:strong max-min property})\
		holds with $f(x^\ast,y^\ast)$ as common value
	\eit
\eit
\vfill


\myfoilhead{Saddle-point interpretation of strong duality}

\bit
\item
	for primal optimum $x^\ast$ and dual optimum $(\lambda^\ast,\nu^\ast)$

	\vspace{-1em}

	$$
		g(\lambda^\ast,\nu^\ast) \leq L(x^\ast, \lambda^\ast, \nu^\ast) \leq \fobj(x^\ast)
	$$

\vitem
	if strong duality holds,
	for every $x\in\optdomain$, $\lambda\succeq 0$, and $\nu$

	\vspace{-1em}

	$$
		L(x^\ast,\lambda,\nu)
			\leq
		\fobj(x^\ast) = L(x^\ast,\lambda^\ast,\nu^\ast) = g(\lambda^\ast,\nu^\ast)
			\leq
		L(x,\lambda^\ast, \nu^\ast)
	$$
	\bit
	\vitem
		thus $x^\ast$ and $(\lambda^\ast,\nu^\ast)$ form saddle-point of Lagrangian
	\eit

\vitem
	conversely, if $\tilde{x}$ and $(\tilde{\lambda},\tilde{\nu})$ are saddle-point of Lagrangian,
	\ie,
	for every $x\in\optdomain$, $\lambda\succeq 0$, and $\nu$

	\vspace{-1em}

	$$
		L(\tilde{x}, {\lambda},{\nu})
		\leq
		L(\tilde{x}, \tilde{\lambda},\tilde{\nu})
		\leq
		L({x}, \tilde{\lambda},\tilde{\nu})
	$$

	\bit
	\vitem
		hence
		$
			g(\tilde{\lambda},\tilde{\nu})
			= \inf_{x\in\optdomain} L(x,\tilde{\lambda},\tilde{\nu})
			= L(\tilde{x}, \tilde{\lambda},\tilde{\nu})
			= \sup_{\lambda\succeq 0, \nu} L(\tilde{x},{\lambda},{\nu}) = \fobj(\tilde{x})
		$,
		thus
		$g(\lambda^\ast,\nu^\ast) \leq g(\tilde{\lambda}, \tilde{\nu})$
		\&
		$\fobj(\tilde{x}) \leq \fobj(x^\ast)$

	\vitem
		thus $\tilde{x}$ and $(\tilde{\lambda}, \tilde{\nu})$ are primal and dual optimal
	\eit
\eit


\myfoilhead{Game interpretation}
\pagelabel{page:Game interpretation}

\bit
\item
	assume
	two players play zero-sum game with payment function $f:X\times Y\to \reals$
	where
	player $A$ pays player $B$ amount equal to $f(x,y)$
	when player $A$ chooses $x$ and player $B$ chooses $y$

\vitem
%	(naturally)
	player $A$ will try to minimize $f(x,y)$
	and
	player $B$ will try to maximize $f(x,y)$

\vitem
	assume player $A$ chooses first
	then player $B$ chooses after learning opponent's choice
	\bit
	\item
		if player $A$ chooses $x$, player $B$ will choose $\argsup_{y\in Y} f(x,y)$
	\item
		knowing that, player $A$ will first choose $\arginf_{x\in X} \sup_{y\in Y} f(x,y)$
	\item
		hence payment will be $\inf_{x\in X} \sup_{y\in Y} f(x,y)$
	\eit

\vitem
	if player $B$ makes her choise first, opposite happens, \ie,
	payment will be $\sup_{y\in Y} \inf_{x\in X} f(x,y)$

\vitem
	max-min inequality of \inequalityname~\ref{inequality:max-min inequality} says
	$$
		\sup_{y\in Y} \inf_{x\in X} f(x,y)
			\leq
		\inf_{x\in X} \sup_{y\in Y} f(x,y)
	$$
	\ie, whowever chooses later has advantage,
	which is similar or rather same as
	matrix games using mixed strategies on page~\pageref{page:Matrix games using mixed strategies}\

\vitem
	saddle-point for $f$ (and $X$ and $Y$),
	$(x^\ast,y^\ast)$,
	called \define{solution of game}
	- $x^\ast$ is optimal choice for player $A$
		and
	$x^\ast$ is optimal choice for player $B$
\eit


\myfoilhead{Game interpretation for weak and strong dualities}

\bit
\item
	assume payment function in zero-sum game on page~\pageref{page:Game interpretation}\
	is Lagrangian of optimization problem
	in \definitionname~\ref{definition:optimization problems}

\vitem
	assume that $X=\xdomain$ and $Y=\prealk{n} \times \reals^p$

\vitem
	if player $A$ chooses first, knowing that player $B$ will choose $\argsup_{(\lambda,\nu)\in Y}L(x,\lambda,\nu)$,
	she will choose $x^\ast = \arginf_{x\in\xdomain} \sup_{(\lambda,\nu)\in Y}L(x,\lambda,\nu)$

\vitem
	likewise, player $B$ will choose
	$(\lambda^\ast,\nu^\ast) = \argsup_{(\lambda,\nu)\in Y} \inf_{x\in\xdomain} L(x,\lambda,\nu)$

\vitem
	optimal dualtiy gap $p^\ast - d^\ast$ equals to advantage player who goes second has

\vitem
	if strong dualtiy holds, $(x^\ast, \lambda^\ast, \nu^\ast)$ is solution of game,
	in which case no one has advantage
\eit
\vfill


\myfoilhead{Certificate of suboptimality}

\bit
\item
	dual feasible point $(\lambda,\nu)$
	degree of suboptimality of current solution

\vitem
	assume $x$ is feasible solution,
	then
	$$
		\fobj(x) - p^\ast \leq \fobj(x) - g(\lambda,\nu)
	$$
	guarantees that $\fobj(x)$ is no further than $\epsilon = \fobj(x) - g(\lambda,\nu)$
	from optimal point point $x^\ast$
	(even though we do not know optimal solution)

\vitem
	for this reason,
	$(\lambda,\nu)$, called \define{certificate of suboptimality}

\vitem
	$x$ is $\epsilon$-suboptimal for primal problem
	and
	$(\lambda,\nu)$ is $\epsilon$-suboptimal for dual problem

\vitem
	strong duality means we \emph{could}
	find arbitrarily small certificate of suboptimality
\eit
\vfill


\myfoilhead{Complementary slackness}

\bit
\item
	assume strong duality holds for optimization problem
	in \definitionname~\ref{definition:optimization problems}
	and assume $x^\ast$ is primal optimum and $(\lambda^\ast,\nu^\ast)$ is dual optimum,
	then
	$$
		\fobj(x^\ast)
		= L(x^\ast,\lambda^\ast,\nu^\ast)
		= \fobj(x^\ast) + {\lambda^\ast}^T \fie(x^\ast) + {\nu^\ast}^T \feq(x^\ast)
	$$

\vitem
	$\feq(x^\ast)=0$ implies ${\lambda^\ast}^T \fie(x^\ast)=0$

\vitem
	then $\lambda^\ast \succeq 0$ and $\fie(x^\ast) \preceq 0$ imply
	$$
		\lambda_i^\ast \fie_i(x^\ast) = 0
		\quad
		i=1,\ldots,m
	$$
\eit

\begin{myproposition}{complementary slackness}
	when strong duality holds,
	for primal and dual optimal points $x^\ast$ and $(\lambda^\ast, \nu)$
	$$
		\lambda_i^\ast \fie_i(x^\ast) = 0
		\quad
		i=1,\ldots,m
	$$
	this property, called \define{complementary slackness}
\end{myproposition}
\vfill


\myfoilhead{KKT optimality conditions}

\begin{mydefinition}{KKT optimality conditions}
	for optimization problem in \definitionname~\ref{definition:optimization problems}
	where \fobj, \fie, and \feq\ are all differentiable,
	below conditions
	for ${x}\in\optdomain$ and $({\lambda}, {\nu})\in\reals^m\times\reals^p$
	\begin{eqnarray*}
		\fie({x})
			&\preceq&
		0
		\quad
		\mbox{- primal feasibility}
		\\
		\feq(x)
			&=&
		0
		\quad
		\mbox{- primal feasibility}
		\\
		\lambda
			&\succeq&
		0
		\quad
		\mbox{- dual feasibility}
		\\
		{\lambda}^T \fie({x})
			&=&
		0
		\quad
		\mbox{- complementary slackness}
		\\
		\nabla_x L(x,\lambda,\nu)
			&=&
		0
		\quad
		\mbox{- vanishing gradient of Lagrangian}
	\end{eqnarray*}
%	where
%	$$
%		\nabla_x L(x,\lambda,\nu)
%		=
%		\nabla \fobj(x)
%		+
%		(D\fie(x))^T \lambda
%		+
%		(D\feq(x))^T \nu
%	$$
	called \define{Karush-Kuhn-Tucker (KKT) optimality conditions}
\end{mydefinition}


\myfoilhead{KKT necessary for optimality with strong duality}

\begin{mytheorem}{KKT necessary for optimality with strong duality}
	for optimization problem in \definitionname~\ref{definition:optimization problems}
	where \fobj, \fie, and \feq\ are all differentiable,
	if strong duality holds,
	primal and dual optimal solutions $x^\ast$ and $(\lambda^\ast, \nu)$
	satisfy KKT optimality conditions (in \definitionname~\ref{definition:KKT optimality conditions}),
	\ie,
	for every optimization problem
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		when strong duality holds,
		KKT optimality conditions are necessary for primal and dual optimality
	\item []
		or equivalently
	\iitem
		primal and dual optimality with strong duality imply KKT optimality conditions
	\eit
\end{mytheorem}


\myfoilhead{KKT and convexity sufficient for optimality with strong duality}

\bit
\item
	assume convex optimization problem where \fobj, \fie, and \feq\ are all differentiable
	and ${x}\in\optdomain$ and $({\lambda}, {\nu})\in\reals^m\times\reals^p$
	satisfying KKT conditions,
	\ie\
	$$
%	\begin{array}{l}
		\fie({x}) \preceq 0, \; \feq({x}) = 0
%		\\
%		\quad
		, \;
		{\lambda} \succeq 0
%		\\
%		\quad
		, \;
		{\lambda}^T \fie({x}) = 0
%		\; i=1,\ldots,m
%		\\
%		\quad
		, \;
		\nabla_x L({x}, {\lambda},{\nu}) = 0
%	\end{array}
	$$
\vitem
	since $L(x,\lambda,\nu)$ is convex for $\lambda\succeq 0$,
	\ie,
	each of $\fobj(x)$, $\lambda^T \fie(x)$, and $\nu^T \feq(x)$
	is convex,
	vanishing gradient implies $x$ achieves infimum for Lagrangian,
	hence
	$$
		g(\lambda,\nu) = L(x,\lambda,\nu)
		= \fobj(x) + \lambda^T \fie(x) + \nu^T \feq(x)
		= f(x)
	$$

\vitem
	thus, strong duality holds,
	\ie,
	$x$ and $(\lambda,\nu)$ are primal and dual optimal solutions
	with zero duality gap
\eit
\vfill

\myfoilhead{}

\begin{mytheorem}{KKT and convexity sufficient for optimality with strong duality}
	for convex optimization problem in \definitionname~\ref{definition:convex optimization}
	where \fobj, \fie, and \feq\ are all differentiable,
	if ${x}\in\optdomain$ and $({\lambda}, {\nu})\in\reals^m\times\reals^p$
	satisfy KKT optimality conditions (in \definitionname~\ref{definition:KKT optimality conditions}),
	they are primal and dual optimal solutions having zero duality gap
	\ie\
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		for convex optimization problem,
		KKT optimality conditions are sufficient for primal and dual optimality with strong duality
	\item []
		or equivalently
	\iitem
		KKT optimality conditions and convexity
		imply primal and dual optimality and strong duality
	\eit
\end{mytheorem}

\bit
\item
	\theoremname~\ref{theorem:KKT necessary for optimality with strong duality}\
	together with
	\theoremname~\ref{theorem:KKT and convexity sufficient for optimality with strong duality}\
	implies
	that
	for convex optimization problem
	\bit
	\item
		\emph{
		KKT optimality conditions are necessary and sufficient
		for primal and dual optimality with strong duality
		}
	\eit
\eit


\myfoilhead{Solving primal problems via dual problems}
\pagelabel{page:Solving primal problems via dual problems}

\bit
\item
	when strong duality holds,
	can retrieve primal optimum from dual optimum
	since
	primal optimal solution is minimize of
	$$
		L(x,\lambda^\ast,\nu^\ast)
	$$
	where $(\lambda^\ast, \nu^\ast)$ is dual optimum

\vitem
	example - entropy maximization
		($\optdomain = \pprealk{n}$)
	\bit
	\vitem
		primal problem - \onelineoptprob{min.}{%
			\fobj(x) = \sum_{i=1}^n x_i \log x_i%
		}{%
			Ax \preceq b,\; \sum x = 1%
		}
	\vitem
		dual problem - \onelineoptprob{max.}{%
			-b^T\lambda - \nu - \exp(-\nu-1) \sum \exp(A^T\lambda)%
		}{%
			\lambda \succeq 0
		}
	\vitem provided dual optimum $(\lambda^\ast,\nu^\ast)$,
		primal optimum is
		$$
			x^\ast
%			= \argmin_{x\in\optdomain} L(x,\lambda^\ast,\nu^\ast)
			= \argmin_{x\in\optdomain}
			\left(
				\sum x_i \log x_i + {\lambda^\ast}^T (Ax-b) + \nu^\ast(\ones^Tx -1)
			\right)
		$$
	\vitem
		$\nabla_x L(x,\lambda^\ast,\nu^\ast) = \log x + A^T \lambda^\ast + (1+\nu^\ast)\ones$,
		hence
		$$
			x^\ast = \exp(-(A^T \lambda^\ast + (1+\nu^\ast)\ones))
		$$
	\eit
\eit
\vfill


\myfoilhead{Perturbed optimization problems}
\pagelabel{page:Perturbed optimization problems}

\bit
\item
%	perturbe constraints of
	original problem in \definitionname~\ref{definition:optimization problems}\
	with perturbed constraints
	$$
		\begin{array}{ll}
			\mbox{minimize} &
				\fobj(x)
			\\
			\mbox{subject to} &
				\fie(x) \preceq u
			\\ &
				\feq(x) =v
		\end{array}
	$$
	where $u\in\reals^m$ and $v\in\reals^p$

\item
	define $p^\ast(u,v)$ as optimal value of above \emph{perturbed} problem,
	\ie\
	$$
		p^\ast(u,v) = \inf\set{\fobj(x)}{x\in\optdomain, \fie(x) \preceq u, \feq(x) = v}
	$$
	which is convex\
	when problem is convex optimization problem
	\proofref{epigraph of convex optimization is convex}\
%	\bit
%	\item
	- note
		$p^\ast(0,0)=p^\ast$
%	\eit

\item
	assume and dual optimum $(\lambda^\ast,\nu^\ast)$,
	if strong duality holds,
	for every feasible $x$ for perturbed problem
	$$
		p^\ast(0,0)=g(\lambda^\ast,\nu^\ast)
		\leq \fobj(x) + {\lambda^\ast}^T \fie(x) + {\nu^\ast}^T \feq(x)
		\leq \fobj(x) + {\lambda^\ast}^T u + {\nu^\ast}^T v
	$$
	thus
	$$
		p^\ast(0,0)\leq p^\ast(u,v) + {\lambda^\ast}^T u + {\nu^\ast}^T v
	$$
	hence
	$$
		p^\ast(u,v)\geq p^\ast(0,0) - {\lambda^\ast}^T u - {\nu^\ast}^T v
	$$

\item
	\figref{sensitivity analysis of optimal value}\
	shows this for optimization problem
	with one inequality constraint and no equality constraint
\eit

\begin{figure}
\begin{center}
%\setlength{\fboxsep}{0pt}
%\fbox{
	\dualitygraphfive{1em}
%}
	\idxfig{sensitivity analysis of optimal value}
	\label{fig:sensitivity analysis of optimal value}
\end{center}
\end{figure}
\vfill


\myfoilhead{Global sensitivity analysis via perturbed problems}

\bit
\item
	recall
	$$
		p^\ast(u,v)\geq p^\ast(0,0) - {\lambda^\ast}^T u - {\nu^\ast}^T v
	$$

\vitem
	interpretations
	\bit
	\item
		if $\lambda^\ast_i$ is large, when $i$-th inequality constraint is tightened,
		optimal value increases a lot

	\item
		if $\lambda^\ast_i$ is small, when $i$-th inequality constraint is relaxed,
		optimal value decreases not a lot

	\item
		if $|\nu^\ast_i|$ is large,
		reducing $v_i$ when $\nu^\ast_i>0$ or increasing $v_i$ when $\nu^\ast_i<0$
		increases optimval value a lot

	\item
		if $|\nu^\ast_i|$ is small,
		increasing $v_i$ when $\nu^\ast_i>0$ or decreasing $v_i$ when $\nu^\ast_i<0$
		decreases optimval value not a lot
	\eit

\vitem
	it only gives lower bounds - will explore local behavior
\eit
\vfill


\myfoilhead{Local sensitivity analysis via perturbed problems}

\bit
\item
	assume $p^\ast(u,v)$ is differentiable \wrt\ $u$ and $v$,
	\ie, $\nabla_{(u,v)} p^\ast(u,v)$ exist
	\bit
	\item
		then
		$$
			\frac{\partial}{\partial u_i} p^\ast (0,0)
			=
			\lim_{h\to 0^+} \frac{p^\ast(he_i,0) - p^\ast(0,0)}{h}
			\geq
			\lim_{h\to 0^+} \frac{-{\lambda^\ast}^T (he_i) }{h}
			=
			-\lambda_i
		$$
		and
		$$
			\frac{\partial}{\partial u_i} p^\ast (0,0)
			=
			\lim_{h\to 0^-} \frac{p^\ast(he_i,0) - p^\ast(0,0)}{h}
			\leq
			\lim_{h\to 0^-} \frac{-{\lambda^\ast}^T (he_i) }{h}
			=
			-\lambda_i
		$$
	\item
		obtain same result for $v_i$, hence
		$$
%			{\partial p^\ast (0,0)}/ {\partial u_i} = -\lambda_i
%			\frac{\partial}{\partial u_i} p^\ast (0,0) = -\lambda_i
			\nabla_u\; p^\ast (0,0) = -\lambda
			\quad
%			\frac{\partial}{\partial v_i} p^\ast (0,0) = -\nu_i
			\nabla_v\; p^\ast (0,0) = -\nu
		$$
	\eit

\vitem
	so larger $\lambda_i$ or $|\nu_i|$ means larger change in optimal value of perturbed problem
	when $u_i$ or $v_i$ change a bit and vice versa
	quantitatively, - $\lambda_i$ an $\nu_i$ provide exact ratio and direction
%	if you change $u_i$ or $v_i$ by $\Delta\simeq 0$, $p^\ast(u,v)$
%	will
%	changes by $\lambda_i \Delta$ or $\nu_i \Delta$ respectively
\eit


\myfoilhead{Different dual problems for equivalent optimization problems - 1}

\bit
\item
	introducing new variables and equality constraints
	for unconstrained problems
	\bit
	\item
		unconstrained optimization problem\ % of form
		$$
			\begin{array}{ll}
				\mbox{minimize} &
					f(Ax+b)
			\end{array}
		$$
		\bit
		\iitem
			dual Lagrange function is $g = p^\ast$,
			hence strong duality holds,
			which, however, does not provide useful information
		\eit

	\item
		reformulate as equivalent optimization problem
		$$
			\begin{array}{ll}
				\mbox{minimize} &
					f(y)
				\\
				\mbox{subject to} &
					Ax+b = y
			\end{array}
		$$
		\bit
		\iitem
			Lagrangian
			-
	%		$$
			$
				L(x,y,\nu) = f(y) + \nu^T(Ax+b-y)
			$
	%		$$
		\iitem
			Lagrange dual function
			-
	%		$$
			$
				g(\nu) = -I(A^T\nu = 0) + b^T\nu - f^\ast(\nu)
			$
	%		$$
		\iitem
			dual optimization problem
			$$
				\begin{array}{ll}
					\mbox{maximize} &
						b^T\nu - f^\ast(\nu)
					\\
					\mbox{subject to} &
						A^T \nu = 0
				\end{array}
			$$
		\eit
	\eit
\eit
\vfill

\myfoilhead{}

\bit
\item
	examples
	\bit
	\item
		unconstrained geometric problem
		$$
			\begin{array}{ll}
				\mbox{minimize} &
					\log\left(
						\sum_{i=1}^m \exp(a_i^Tx + b_i)
					\right)
			\end{array}
		$$
		\bit
		\iitem
			reformulation
			$$
				\begin{array}{ll}
					\mbox{minimize} &
						\log\left(
							\sum_{i=1}^m \exp(y_i)
						\right)
					\\
					\mbox{subject to} &
						Ax + b =y
				\end{array}
			$$
		\iitem
			dual optimization problem
			$$
				\begin{array}{ll}
					\mbox{maximize} &
						b^T \nu - \sum_{i=1}^m \nu_i \log \nu_i
					\\
					\mbox{subject to} &
						\ones^T \nu = 1
					\\ &
						A^T \nu = 0
					\\ &
						\nu \succeq 0
				\end{array}
			$$
			which is
			entropy maximization problem
		\eit

	\item
		norm minimization problem
		$$
			\begin{array}{ll}
				\mbox{minimize} &
					\|Ax-b\|
			\end{array}
		$$
		\bit
		\iitem
			reformulation
			$$
				\begin{array}{ll}
					\mbox{minimize} &
						\|y\|
					\\
					\mbox{subject to} &
						Ax - b = y
				\end{array}
			$$
		\iitem
			dual optimization problem
			$$
				\begin{array}{ll}
					\mbox{maximize} &
						b^T \nu
					\\
					\mbox{subject to} &
						\|\nu\|_\ast \leq 1
					\\ &
						A^T \nu =0
				\end{array}
			$$
		\eit
	\eit
\eit
\vfill


\myfoilhead{Different dual problems for equivalent optimization problems - 2}

\bit
\item
	introducing new variables and equality constraints
	for constrained problems

	\bit
	\item
		inequality constrained optimization problem
			$$
				\begin{array}{ll}
					\mbox{minimize} &
						f_0(A_0x+b_0)
					\\
					\mbox{subject to} &
						f_i(A_ix+b_i) \leq 0\quad i=1,\ldots,m
				\end{array}
			$$
	\vitem
		reformulation
			$$
				\begin{array}{ll}
					\mbox{minimize} &
						f_0(y_0)
					\\
					\mbox{subject to} &
						f_i(y_i) \leq 0\quad i=1,\ldots,m
					\\ &
						A_i x + b_i = y_i\quad i=0,\ldots,m
				\end{array}
			$$
	\vitem
		dual optimization problem
			$$
				\begin{array}{ll}
					\mbox{maximize} &
						\sum_{i=0}^m \nu_i^T b_i - f_0^\ast(\nu_0)
						- \sum_{i=1}^m \lambda_i f_i^\ast(\nu_i/\lambda_i)
					\\
					\mbox{subject to} &
						\sum_{i=0}^m A_i^T \nu_i = 0
					\\ &
						\lambda \succeq 0
				\end{array}
			$$
	\eit
\eit
\vfill

\myfoilhead{}

\bit
\item
	examples
	\bit
	\item
		inequality constrained geometric program
		$$
			\begin{array}{ll}
				\mbox{minimize} &
					\log\left(\sum \exp(A_0x + b_0)\right)
				\\
				\mbox{subject to} &
					\log\left(\sum \exp(A_ix + b_i)\right)\leq 0\quad i=1,\ldots,m
			\end{array}
		$$
		where $A_i\in\reals^{K_i\times n}$
		and $\exp(z) := (\exp(z_1),\ldots,\exp(z_k)))\in\reals^n$
		and $\sum z := \sum_{i=1}^k z_i\in\reals$
		for $z\in\reals^k$
		\bit
		\vfill
		\iitem
			reformulation

			$$
				\begin{array}{ll}
					\mbox{minimize} &
						\log\left(\sum \exp(y_0)\right)
					\\
					\mbox{subject to} &
						\log\left(\sum \exp(y_i)\right)\leq 0\quad i=1,\ldots,m
					\\ &
						A_i x + b_i = y_i \quad i=0,\ldots,m
				\end{array}
			$$

		\vfill
		\iitem
			dual optimization problem

			$$
				\begin{array}{ll}
					\mbox{maximize} &
						\sum_{i=0}^m b_i^T \nu_i
						- \nu_0^T\log(\nu_0)
						- \sum_{i=1}^m \nu_i^T\log(\nu_i/\lambda_i)
					\\
					\mbox{subject to} &
						\nu_i \succeq 0\quad i=0,\ldots,m
					\\ &
						\ones^T \nu_0 = 1,\; \ones^T\nu_i=\lambda_i\quad i=1,\ldots,m
					\\ &
						\lambda_i\geq 0 \quad i=1,\ldots,m
					\\ &
						\sum_{i=0}^m A_i^T\nu_i = 0
				\end{array}
			$$

			where
			and $\log(z) := (\log(z_1),\ldots,\log(z_k)))\in\reals^n$
			for $z\in\pprealk{k}$

		\vfill
		\iitem
			simplified dual optimization problem

			$$
				\begin{array}{ll}
					\mbox{maximize} &
						\sum_{i=0}^m b_i^T \nu_i
						- \nu_0^T\log(\nu_0)
						- \sum_{i=1}^m \nu_i^T\log(\nu_i/\ones^T\nu_i)
					\\
					\mbox{subject to} &
						\nu_i \succeq 0\quad i=0,\ldots,m
					\\ &
						\ones^T \nu_0 = 1
					\\ &
						\sum_{i=0}^m A_i^T\nu_i = 0
				\end{array}
			$$
		\eit
	\eit
\eit
\vfill


\myfoilhead{Different dual problems for equivalent optimization problems - 3}

\bit
\item
	transforming objectives
	\bit
	\vitem
		norm minimization problem
		$$
			\begin{array}{ll}
				\mbox{minimize} &
					\|Ax - b\|
			\end{array}
		$$
	\vitem
		reformulation
		$$
			\begin{array}{ll}
				\mbox{minimize} &
					(1/2)\|y\|^2
				\\
				\mbox{subject to} &
					Ax - b = y
			\end{array}
		$$
	\vitem
		dual optimization problem
		$$
			\begin{array}{ll}
				\mbox{maximize} &
					-(1/2)\|\nu\|_\ast^2 + b^T\nu
				\\
				\mbox{subject to} &
					A^T\nu = 0
			\end{array}
		$$
	\eit
\eit
\vfill


\myfoilhead{Different dual problems for equivalent optimization problems - 4}

\bit
\item
	making contraints implicit
	\bit
	\vitem
		LP with box constraints
		$$
			\begin{array}{ll}
				\mbox{minimize} &
					c^T x
				\\
				\mbox{subject to} &
					Ax = b,\;
%				\\ &
					l \preceq x \preceq u
			\end{array}
		$$

	\vitem
		dual optimization problem
		$$
			\begin{array}{ll}
				\mbox{maximize} &
					-b^T\nu - \lambda_1^Tu + \lambda_2^Tl
				\\
				\mbox{subject to} &
					A^T\nu + \lambda_1 - \lambda_2 + c = 0,\;
%				\\ &
					\lambda_1 \succeq 0,\; \lambda_2 \succeq 0
			\end{array}
		$$

	\vitem
		reformulation
		$$
			\begin{array}{ll}
				\mbox{minimize} &
%					\left\{
%					\begin{array}{ll}
						c^T x
						+ I ( l\preceq x \preceq u)
%						& l\preceq x \preceq u
%						\\
%						\infty & \mbox{otherwise}
%					\end{array}
%					\right.
				\\
				\mbox{subject to} &
					Ax = b
			\end{array}
		$$

	\vitem
		dual optimization problem for reformulated primal problem
		$$
			\begin{array}{ll}
				\mbox{maximize} &
					-b^T \nu
					- u^T(A^T\nu + c)^-
					+ l^T(A^T\nu + c)^+
			\end{array}
		$$
	\eit
\eit
\vfill


\titlefoil{Theorems of Alternatives}{Theorems of Alternatives}

\myfoilhead{Weak alternatives}

\begin{mytheorem}{weak alternatives of two systems}
	for $\fie: \xie\to\reals^m$ \& $\feq: \xeq\to\reals^p$
	where \xie\ and \xeq\ are subsets of common set $\xdomain$,
	which is subset of Banach space,
	assuming $\optdomain = \xie \cap \xeq \neq \emptyset$,
	and
	$\lambda\in\reals^m$ \& $\nu\in\reals^p$,
	below two systems of inequalities and equalities are weak alternatives,
	\ie, at most one of them is feasible
		$$
				\fie(x) \preceq 0%
				\quad
				\feq(x) =0
		$$
		and
		$$
				\lambda \succeq 0%
				\quad
				\inf_{x\in\optdomain}
				\left(
					\lambda^T \fie(x) + \nu^T \feq(x)
				\right)
				>0
		$$
\end{mytheorem}

\bit
\vitem
	can prove \theoremname~\ref{theorem:weak alternatives of two systems}\
	using duality of optimization problems

\vitem
	consider primal and dual problems
	\bit
	\item primal problem
		$$
			\begin{array}{ll}
				\mbox{minimize} &
					0
				\\
				\mbox{subject to} &
					\fie(x) \preceq 0
				\\ &
					\feq(x) =0
			\end{array}
		$$
	\item dual problem
		$$
			\begin{array}{ll}
				\mbox{maximize} &
					g(\lambda,\nu)
				\\
				\mbox{subject to} &
					\lambda \succeq 0
			\end{array}
		$$
		where
		$$
			g(\lambda,\nu)
			=
				\inf_{x\in\optdomain}
				\left(
					\lambda^T \fie(x) + \nu^T \feq(x)
				\right)
		$$
	\eit

\vitem
	then
	$p^\ast,\; d^\ast \in \{0,\infty\}$

\vitem
	now assume \emph{first system of \theoremname~\ref{theorem:weak alternatives of two systems}\
	is feasible,} then $p^\ast = 0$, hence
	weak duality applies $d^\ast=0$,
	thus there exist no $\lambda$ and $\nu$ such that $\lambda\succeq 0$
	and $g(\lambda,\nu) > 0$\
	\ie, \emph{second system is infeasible,}
	since otherwise there exist $\lambda$ and $\nu$
	making $g(\lambda,\nu)$ arbitrarily large;
	if $\tilde{\lambda}\succeq 0$ and $\tilde{\nu}$
	satisfy $g({\lambda},{\nu})>0$,
	$g(\alpha\tilde{\lambda}, \alpha\tilde{\nu}) = \alpha g(\tilde{\lambda}, \tilde{\nu})$
	goes to $\infty$ when $\alpha\to\infty$

\vitem
	assume \emph{second system is feasible,}
	then
	$g(\lambda,\nu)$ can be arbitrarily large
	for above reasons,
	thus $d^\ast = \infty$,
	hence weak duality implies $p^\ast = \infty$,
	which implies
	\emph{first system is infeasible}\

\vitem
	therefore two systems are weak alternatives;
	at most one of them is feasible

\vitem []
	(actually, not hard to prove it without using weak duality)
\eit


\myfoilhead{Weak alternatives with strict inequalities}

\begin{mytheorem}{weak alternatives of two systems with strict inequalities}
	for $\fie: \xie\to\reals^m$ \& $\feq: \xeq\to\reals^p$
	where \xie\ and \xeq\ are subsets of common set $\xdomain$,
	which is subset of Banach space,
	assuming $\optdomain = \xie \cap \xeq \neq \emptyset$,
	and
	$\lambda\in\reals^m$ \& $\nu\in\reals^p$,
%
%	for $\fie: \xie\to\reals^m$ and $\feq: \xeq\to\reals^p$
%	where \xie\ and \xeq\ are subsets of common set $\xdomain$\
%	assuming $\optdomain = \xie \cap \xeq \neq \emptyset$,
	below two systems of inequalities and equalities are weak alternatives,
	\ie, at most one of them is feasible
		$$
				\fie(x) \prec 0%
				\quad
				\feq(x) =0
		$$
		and
		$$
				\lambda \succeq 0%
				\quad
				\lambda\neq 0
				\quad
				\inf_{x\in\optdomain}
				\left(
					\lambda^T \fie(x) + \nu^T \feq(x)
				\right)
				\geq
				0
		$$
\end{mytheorem}


\myfoilhead{Strong alternatives}

\begin{mytheorem}{strong alternatives of two systems}
	for convex $\fie: \xie\to\reals^m$ \& affine $\feq:\xeq\to\reals^p$
	where \xie\ and \xeq\ are subsets $\reals^n$
	assuming $\optdomain = \xie \cap \xeq \neq \emptyset$
	and
	$\lambda\in\reals^m$ \& $\nu\in\reals^p$,
	if exists $x \in \relint \optdomain$ with $\feq(x)=0$,
	below two systems of inequalities and equalities are strong alternatives,
	\ie, exactly one of them is feasible
		$$
				\fie(x) \preceq 0%
				\quad
				\feq(x) =0
		$$
		and
		$$
				\lambda \succeq 0%
				\quad
				\inf_{x\in\optdomain}
				\left(
					\lambda^T \fie(x) + \nu^T \feq(x)
				\right)
				>0
		$$
\end{mytheorem}


\myfoilhead{Strong alternatives with strict inequalities}

\begin{mytheorem}{strong alternatives of two systems with strict inequalities}
	for convex $\fie: \xie\to\reals^m$ \& affine $\feq:\xeq\to\reals^p$
	where \xie\ and \xeq\ are subsets $\reals^n$
	assuming $\optdomain = \xie \cap \xeq \neq \emptyset$
	and
	$\lambda\in\reals^m$ \& $\nu\in\reals^p$,
	if exists $x \in \relint \optdomain$ with $\feq(x)=0$,
	below two systems of inequalities and equalities are strong alternatives,
	\ie, exactly one of them is feasible
		$$
				\fie(x) \prec 0%
				\quad
				\feq(x) =0
		$$
		and
		$$
				\lambda \succeq 0
				\quad
				\lambda \neq 0
				\quad
				\inf_{x\in\optdomain}
				\left(
					\lambda^T \fie(x) + \nu^T \feq(x)
				\right)
				\geq
				0
		$$
\end{mytheorem}

\bit
\vitem
	proof -
	consider convex optimization problem and its dual
	\bit
	\vitem
		primal problem
		$$
			\begin{array}{ll}
				\mbox{minimize} &
					s
				\\
				\mbox{subject to} &
					\fie(x) - s \ones \preceq 0
				\\ &
					\feq(x) =0
			\end{array}
		$$
	\vitem
		dual problem
		$$
			\begin{array}{ll}
				\mbox{maximize} &
					g(\lambda,\nu)
				\\
				\mbox{subject to} &
					\lambda \succeq 0
					\quad
					\ones^T \lambda = 1
			\end{array}
		$$
		where
		$
				g(\lambda,\nu)
				=
				\inf_{x\in\optdomain}
				\left(
					\lambda^T \fie(x) + \nu^T \feq(x)
				\right)
		$
	\eit

\vitem
	first observe Slater's condition
	holds for primal problem
	since by hypothesis of \theoremname~\ref{theorem:strong alternatives of two systems with strict inequalities},
	exists $y\in\relint \optdomain$ with $\feq(y)=0$,
	hence $(y,\fie(y))\in\xie\times \reals$
	is primal feasible satisifying Slater's condition

\vitem
	hence Slater's theorem (\theoremname~\ref{theorem:Slater's theorem})
	implies
	$d^\ast=p^\ast$

\vitem
	assume first system
%	of \theoremname~\ref{theorem:strong alternatives of two systems with strict inequalities}\
	is feasible,
	then primal problem is strictly feasible and $d^\ast = p^\ast<0$,
	hence second system infeasible
	since otherwise
	feasible point for second system
	is feasible point of dual problem,
	hence $d^\ast\geq0$

\vitem
	assume first system
%	of \theoremname~\ref{theorem:strong alternatives of two systems with strict inequalities}\
	is infeasible,
	then $d^\ast = p^\ast\geq0$,
	hence
	Slater's theorem (\theoremname~\ref{theorem:Slater's theorem})
	implies exists dual optimal $(\lambda^\ast,\nu^\ast)$ (whether or not $d^\ast=\infty$),
	hence $(\lambda^\ast,\nu^\ast)$ is feasible point for second system
	of \theoremname~\ref{theorem:strong alternatives of two systems with strict inequalities}\

\vitem
	therefore
	two systems are strong alternatives;
	each is feasible \iaoi\ the other is infeasible
\eit


\myfoilhead{Strong alternatives for linear inequalities}

\bit
\item
	dual function of feasibility problem for $Ax\preceq b$
	is
	$$
		g(\lambda) = \inf_{x\in\reals^n} \lambda^T(Ax-b)
		=
		\left\{
		\begin{array}{ll}
			-b^T \lambda & A^T\lambda = 0
			\\
			-\infty & \mbox{otherwise}
		\end{array}
		\right.
	$$

\vitem
	hence
	alternative system is $\lambda\succeq0,\;b^T\lambda <0,\; A^T\lambda=0$

\vitem
	thus \theoremname~\ref{theorem:strong alternatives of two systems}\
	implies below systems are strong alternatives
	$$
		Ax \preceq b
		\quad\&\quad
		\lambda\succeq0 \quad b^T\lambda <0 \quad A^T\lambda=0
	$$

\vitem
	similarly
	alternative system is $\lambda\succeq0,\;b^T\lambda <0,\; A^T\lambda=0$
	and \theoremname~\ref{theorem:strong alternatives of two systems}\
	implies below systems are strong alternatives
	$$
		Ax \prec b
		\quad\&\quad
		\lambda\succeq0 \quad \lambda \neq 0 \quad b^T\lambda \leq 0 \quad A^T\lambda=0
	$$
\eit
\vfill


\myfoilhead{Farkas' lemma}

\begin{mytheorem}{Farkas' lemma}
	below systems of inequalities and equalities are strong alternatives
	$$
		Ax\preceq 0 \quad c^T x < 0
		\quad \& \quad
		A^T y + c = 0
		\quad
		y \succeq 0
	$$
\end{mytheorem}
\index{Farkas, Julius!Farkas' lemma}

\bit
\item
	will prove \theoremname~\ref{theorem:Farkas' lemma}\
	using LP and its dual

\vitem
%	-
	consider LP
	$\left(\mbox{minimize}\; c^T x \quad \mbox{subject to}\; Ax \preceq 0\right)$
%	$$
%		\begin{array}{ll}
%			\mbox{minimize} &
%				c^T x
%			\\
%			\mbox{subject to} &
%				Ax \preceq 0
%		\end{array}
%	$$

\vitem
	dual function is
	$
		g(y)
			=
		\inf_{x\in\reals^n} \left(c^Tx + y^TAx \right)
			=
		\left\{
		\begin{array}{ll}
			0 & A^Ty + c= 0
			\\
			-\infty & \mbox{otherwise}
		\end{array}
		\right.
	$

\vitem
	hence dual problem is
%	$$
	$
	\left(
%		\begin{array}{ll}
			\mbox{maximize}
			\;
%			&
				0
%			\\
			\quad
			\mbox{subject to}
			\;
%			&
				A^T y + c = 0%
			,
%			\\ &
			\;
%			\quad
				y \succeq 0
%		\end{array}
	\right)
	$
%	$$

\vitem
	assume first system is feasible,
	then homogeneity of primal problem implies $p^\ast = -\infty$,
	thus $d^\ast$, \ie, dual is infeasible,
	hence second system is infeasible

\vitem
	assume first system is infeasible,
	since primal is always feasible,
	$p^\ast=0$,
	hence strong duality implies $d^\ast =0$,
	thus second system is feasible
\eit
\vfill


\titlefoil{Convex Optimization with Generalized Inequalities}{Convex Optimization with Generalized Inequalities}

\newcommand{\bigpropercone}{\ensuremath{\mathcalfont{K}}}

\myfoilhead{Optimization problems with generalized inequalities}

\begin{mydefinition}{optimization problems with generalized inequalities}
	for $\fobj:\xobj \to \reals$, $\fie: \xie\to \bigtimes_{i=1}^m \reals^{k_i} $, $\feq: \xeq \to \reals^p$
	where \xobj, \xie, and \xeq\ are subsets of common set $\xdomain$
	$$
		\begin{array}{ll}
			\mbox{minimize}
				& \fobj(x)
			\\
			\mbox{subject to}
				& \fie(x) \preceq_\bigpropercone 0
			\\
				& \feq(x) =0
		\end{array}
	$$
	called \define{optimization problem with generalized inequalities}
	where
	$\bigpropercone = \bigtimes K_i$ is proper cone
	with
	$m$ proper cones
	$K_1\subset \reals^{k_1},\ldots, K_n\subset \reals^{k_m}$\
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		every terminology and associated notation is same
		as of optimization problem in \definitionname~\ref{definition:optimization problems}
		such as
		objective \& inequality \& equality contraint functions,
		{domain} of optimization problem \optdomain,
		feasible set \optfeasset,
		optimal value $p^\ast$
	\iitem
		note that
		when $K_i=\preals$ (hence $\bigpropercone=\prealk{m}$),
		above optimization problem coincides with that in \definitionname~\ref{definition:optimization problems},
		\ie,
		{optimization problems with generalized inequalities}
		{subsume}
		{(normal) optimization problems}
	\eit
\end{mydefinition}


\myfoilhead{Lagrangian for generalized inequalities}

\begin{mydefinition}{Lagrangian for generalized inequalities}
	for optimization problem in\
	\definitionname~\ref{definition:optimization problems with generalized inequalities}\
	with nonempty domain $\optdomain$,
	function $L:\optdomain \times \bigtimes_{i=1}^m \reals^{k_i} \times \reals^p \to \reals$
	defined by
	$$
		L(x,\lambda, \nu) = \fobj(x) + \lambda^T \fie(x) + \nu^T \feq(x)
%		= \fobj(x) + \sum \lambda_i^T \fie_i(x) + \nu^T \feq(x)
	$$

	called \define{Lagrangian} associated with the optimization problem
	where
	\shrinkspacewithintheoremslike\
	\ibit
%	\iitem interpret $\lambda^T \fie(x)$ as
%		$
%			\sum \lambda_i^T \fie_i(x)
%		$
%
	\iitem
		every terminology and associated notation is same
		as of optimization problem in \definitionname~\ref{definition:Lagrangian}
		such as
		dual variables or Lagrange multipliers $\lambda$ and $\nu$.
%
%	\iitem
%		note close resemblance between Lagrange for generalized inequalities
%		and Lagrangian defined in \definitionname~\ref{definition:Lagrangian}\

	\iitem
%		note
		Lagrangian for generalized inequalities
		{subsumes}
		(normal) Lagrangian (\definitionname~\ref{definition:Lagrangian})
%	(note resemblance with Lagrange dual functions in \ref{definition:Lagrangian})
	\eit
\end{mydefinition}
\index{Lagrangian for generalized inequalities}%
\index{Lagrangian!Lagrangian for generalized inequalities}%
\index{Lagrange, Joseph-Louis!Lagrangian!Lagrangian for generalized inequalities}


\myfoilhead{Lagrange dual functions for generalized inequalities}

\begin{mydefinition}{Lagrange dual functions for generalized inequalities}
	for optimization problem in \definitionname~\ref{definition:optimization problems with generalized inequalities}
	for which Lagrangian is defined,
	function $g:\bigtimes \reals^{k_i} \times \reals^p \to \reals\cup \{-\infty\}$
	defined by
	$$
		g(\lambda,\nu)
		=
		\inf_{x\in\optdomain} L(x,\lambda,\nu)
		=
		\inf_{x\in\optdomain} \left(\fobj(x) + \lambda^T \fie(x) + \nu^T \feq(x)\right)
	$$
	called
	\define{Lagrange dual function}
	or just
	\define{dual function}
	associated with optimization problem
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
%		note that
		Lagrange dual functions for generalized inequalities\
		{subsume}
		(normal) Lagrange dual functions (\definitionname~\ref{definition:Lagrange dual functions})
%	(note resemblance with Lagrange dual functions in \ref{definition:Lagrange dual functions})
	\eit
\end{mydefinition}
\index{Lagrange dual functions for generalized inequalities}%
\index{Lagrange, Joseph-Louis!Lagrange dual functions!Lagrange dual functions for generalized inequalities}

\shrinkspacewithintheoremslike

\bit
\item
	$g$ is \emph{concave function}

\vitem
	$g(\lambda,\nu)$
	is
	lower bound for optimal value of associated optimization problem
	\ie,
	$$
		g(\lambda,\nu) \leq p^\ast
	$$
	for every $\lambda\succeq_\bigpropercone^\ast0$
	where $\bigpropercone^\ast$ denotes dual cone of \bigpropercone,
	\ie,
	$\bigpropercone^\ast = \bigtimes K_i^\ast$
	where $K_i^\ast\subset\reals^{k_i}$ is dual cone of $K_i\subset\reals^{k_i}$\

\vitem
	$(\lambda,\nu)$
	with $\lambda\succeq_\bigpropercone 0$ and $g(\lambda,\nu)>-\infty$
	said to be \define{dual feasible}
\eit


\myfoilhead{Lagrange dual problems for generalized inequalities}

\begin{mydefinition}{Lagrange dual problems for generalized inequalities}
	for optimization problem in \definitionname~\ref{definition:optimization problems with generalized inequalities},
	optimization problem
	$$
		\begin{array}{ll}
			\mbox{maximize} &
				g(\lambda,\nu)
			\\
			\mbox{subject to} &
				\lambda \succeq_{\bigpropercone^\ast} 0
		\end{array}
	$$
	where $\bigpropercone^\ast$ denotes dual cone of \bigpropercone,
	\ie,
	$\bigpropercone^\ast = \bigtimes K_i^\ast$
	where $K_i^\ast\subset\reals^{k_i}$ is dual cone of $K_i\subset\reals^{k_i}$,
	called \define{Lagrange dual problem}
	associated with problem in \definitionname~\ref{definition:optimization problems with generalized inequalities}
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		every terminology and related notation
		is same as that in \definitionname~\ref{definition:Lagrange dual problems}\
		such as
		dual feasibility,
		dual optimal value $d^\ast$,
		optimal Lagrange multipliers $(\lambda^\ast,\nu^\ast)$
	\iitem
		Lagrange dual problems for generalized inequalities
		{subsume}
		(normal) Lagrange dual problems (\definitionname~\ref{definition:Lagrange dual problems})
	\eit
\end{mydefinition}
\index{Lagrange dual problems for generalized inequalities}%
\index{Lagrange, Joseph-Louis!Lagrange dual problems for generalized inequalities}

\bit
\vitem
	Lagrange dual problem in \definitionname~\ref{definition:Lagrange dual problems for generalized inequalities}\
	is convex optimization
	since $g(\lambda,\nu)$ is convex
\eit
\vfill


\myfoilhead{Slater's theorem for generalized inequalities}

\begin{mytheorem}{Slater's theorem for generalized inequalities}
	if optimization problem
	in \definitionname~\ref{definition:optimization problems with generalized inequalities}\
	is convex,
	\ie,
	\fobj\ is convex,
	\fie\ is \bigpropercone-convex\
	(\ie, every $\fie_i$ is $K_i$-convex)
	(\definitionname~\ref{definition:$K$-convex functions}),
	and exists feasible $x\in\optdomain$ contained in $\relint \optdomain$\
	such that
	$$
		\fie(x) \prec_\bigpropercone\ 0\quad \feq(x) = 0
	$$
	\emph{strong duality} holds
	(and \emph{dual optimal value is attained when $d^\ast>-\infty$})
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		such condition, called \define{Slater's condition}
	\iitem
		such point, (sometimes) said to be \define{strictly feasible}
	\iitem
		note resemblance with Slater's theorem in \theoremname~\ref{theorem:Slater's theorem}\
	\eit
\end{mytheorem}


\myfoilhead{Duality for SDP}

\bit
\item
	(inequality form) SDP%
		\pagelabel{page:(inequality form) SDP}
	$$
		\begin{array}{ll}
			\mbox{minimize} &
				c^Tx
			\\
			\mbox{subject to} &
				x_1F_1 + \cdots + x_nF_n + G \preceq 0
		\end{array}
	$$
	where $F_1,\ldots,F_n,G\in\symset{k}$
	and $\bigpropercone = \possemidefset{k}$

\vitem
	Lagrangian
	$$
		L(x,Z)
		= c^Tx + (x_1F_1 + \cdots + x_nF_n + G) \bullet Z
		= \sum x_i(F_i\bullet Z + c_i) + G \bullet Z
	$$
	where $X\bullet Y = \Tr XY$ for $X,Y\in\symset{k}$

\vitem
	Lagrange dual function
	$$
		g(Z) = \inf_{x\in\reals^n} L(x,Z)
		=
		\left\{
		\begin{array}{ll}
			G \bullet Z & F_i\bullet Z + c_i= 0\quad i=1,\ldots,n
			\\
			-\infty & \mbox{otherwise}
		\end{array}
		\right.
	$$

\vitem
	Lagrange dual problem%
		\pagelabel{page:dual of SDP}
	$$
		\begin{array}{ll}
			\mbox{maximize} &
				G\bullet Z
			\\
			\mbox{subject to} &
				F_i \bullet Z + c_i = 0\quad i=1,\ldots,n
			\\ &
				Z \succeq 0
		\end{array}
	$$
	where fact that \possemidefset{k}\ is self-dual,
	\ie,
	$\bigpropercone^\ast = \bigpropercone$

\vitem
	Slater's theorem (\theoremname~\ref{theorem:Slater's theorem for generalized inequalities})
	implies if primal problem is strictly feasible,
	\ie,
	exists $x\in\reals^n$ such that $\sum x_iF_i + G\prec 0$,
	strong duality holds
\eit


\myfoilhead{KKT optimality conditions for generalized inequalities}

\begin{mydefinition}{KKT optimality conditions for generalized inequalities}
	for optimization problem in \definitionname~\ref{definition:optimization problems with generalized inequalities}
	where \fobj, \fie, and \feq\ are all differentiable,
	below conditions
	for ${x}\in\optdomain$ and $({\lambda}, {\nu})\in\bigtimes \reals^{k_i} \times\reals^p$
	\begin{eqnarray*}
		\fie({x})
			&\preceq_\bigpropercone&
		0
		\quad
		\mbox{- primal feasibility}
		\\
		\feq(x)
			&=&
		0
		\quad
		\mbox{- primal feasibility}
		\\
		\lambda
			&\succeq_{\bigpropercone^\ast}&
		0
		\quad
		\mbox{- dual feasibility}
		\\
		{\lambda}^T \fie({x})
			&=&
		0
		\quad
		\mbox{- complementary slackness}
		\\
		\nabla_x L(x,\lambda,\nu)
			&=&
		0
		\quad
		\mbox{- vanishing gradient of Lagrangian}
	\end{eqnarray*}
%	where
%	$$
%		\nabla_x L(x,\lambda,\nu)
%		=
%		\nabla \fobj(x)
%		+
%		(D\fie(x))^T \lambda
%		+
%		(D\feq(x))^T \nu
%	$$
	called \define{Karush-Kuhn-Tucker (KKT) optimality conditions}
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		note KKT optimality conditions for generalized inequalities
		subsume
		(normal) KKT optimality conditions
		(\definitionname~\ref{definition:KKT optimality conditions})
	\eit
\end{mydefinition}


\myfoilhead{KKT conditions and optimalities for generalized inequalities}

\bit
\item
	for every optimization problem with generalized inequalities
	(\definitionname~\ref{definition:optimization problems with generalized inequalities}),
	every statement for normal optimization problem
	(\definitionname~\ref{definition:optimization problems}),
	regarding relations among
	KKT conditions,
	optimality,
	primal and dual optimality,
	and
	strong duality,
	is \emph{exactly the same}

	\bit
	\vitem
		for every optimization problem with generalized inequalities
		(\definitionname~\ref{definition:optimization problems with generalized inequalities})

		\bit
		\viitem
			if strong duality holds,
			primal and dual optimal points satisfy KKT optimality conditions
			in \definitionname~\ref{definition:KKT optimality conditions for generalized inequalities}
			(same as \theoremname~\ref{theorem:KKT necessary for optimality with strong duality})

		\viitem
			if optimization problem is convex and
			primal and dual solutions satisfy KKT optimality conditions
			in \definitionname~\ref{definition:KKT optimality conditions for generalized inequalities},
			the solutions are optimal with strong duality
			(same as \theoremname~\ref{theorem:KKT and convexity sufficient for optimality with strong duality})

		\viitem
			therefore,
			for convex optimization problem,
			\emph{KKT optimality conditions are necessary and sufficient
			for primal and dual optimality with strong duality}
		\eit
	\eit
\eit
\vfill


\myfoilhead{Perturbation and sensitivity analysis for generalized inequalities}

\bit
\item
%	perturbe constraints of
	original problem in \definitionname~\ref{definition:optimization problems with generalized inequalities}\
	with perturbed constraints
	$$
		\begin{array}{ll}
			\mbox{minimize} &
				\fobj(x)
			\\
			\mbox{subject to} &
				\fie(x) \preceq_\bigpropercone u
			\\ &
				\feq(x) =v
		\end{array}
	$$
	where $u\in\reals^m$ and $v\in\reals^p$

\vitem
	define $p^\ast(u,v) = p^\ast(u,v) = \inf\set{\fobj(x)}{x\in\optdomain, \fie(x) \preceq u, \feq(x) = v}$,
	which is convex when problem is convex optimization problem\
	- note
		$p^\ast(0,0)=p^\ast$

\vitem
	as for normal optimization problem case (page~\pageref{page:Perturbed optimization problems}),
	if and dual optimum $(\lambda^\ast,\nu^\ast)$,
	if strong duality holds,
	$$
		p^\ast(u,v)\geq p^\ast(0,0) - {\lambda^\ast}^T u - {\nu^\ast}^T v
	$$
	and
	$$
		\nabla_u\; p^\ast (0,0) = -\lambda
		\quad
		\nabla_v\; p^\ast (0,0) = -\nu
	$$
\eit


\myfoilhead{Sensitivity analysis for SDP}

\bit
\item
	assume inequality form SDP and its dual problem
	on page~\pageref{page:(inequality form) SDP} and page~\pageref{page:dual of SDP}\

\vitem
	consider perturbed SDP
	$$
		\begin{array}{ll}
			\mbox{minimize} &
				c^Tx
			\\
			\mbox{subject to} &
				x_1F_1 + \cdots + x_nF_n + G \preceq U
		\end{array}
	$$
	for some $U\in\symset{k}$
	\bit
	\vitem
		define $p^\ast:\symset{k} \to \reals$
		such that
		$p^\ast(U)$ is optimal value of above problem
	\eit

\vitem
	assume $x^\ast\in\reals^n$ and $Z^\ast\in\possemidefset{k}$
	are primal and dual optimum with zero dualty gap

\vitem
	then
	$$
		p^\ast(U) \geq p^\ast - Z^\ast \bullet U
	$$

\vitem
	if $\nabla_U p^\ast$ exists at $U=0$
	$$
		\nabla_U p^\ast(0) = - Z^\ast
	$$
\eit
\vfill


\myfoilhead{Weak alternatives for generalized inequalities}

\begin{mytheorem}{weak alternatives for generalized inequalities}
	for $\fie:\xie \to \bigtimes \reals^{k_i}$ \& $\feq:\xeq \to \reals^p$
	where \xie\ and \xeq\ are subsets of common Banach space
	assuming $\optdomain = \xie \cap \xeq \neq \emptyset$,
	and $\lambda \in \bigtimes \reals^{k_i}$ \& $\nu \in \reals^p$,
	below pairs of systems are strong alternatives
		\begin{eqnarray*}
%		$$
			\fie(x) \preceq_\bigpropercone 0
			\quad
			\feq(x) = 0
			\quad
			&
			\&
			&
			\quad
			\lambda\succeq_{\bigpropercone^\ast} 0
			\quad
			g(\lambda,\nu) > 0
%		$$
			\\
%		$$
			\fie(x) \prec_\bigpropercone 0
			\quad
			\feq(x) = 0
			\quad
			&
			\&
			&
			\quad
			\lambda\succeq_{\bigpropercone^\ast} 0
			\quad
			\lambda \neq 0
			\quad
			g(\lambda,\nu) \geq 0
%		$$
		\end{eqnarray*}
	where $\bigpropercone = \bigtimes K_i$ with proper cones $K_i\subset\reals^{k_i}$
	and function $g:\bigtimes \reals^{k_i} \times \reals^p \to \reals$ defined by
		$$
			g(\lambda,\nu) = \inf_{x\in\optdomain} \left( \lambda^T \fie(x) + \nu^T \feq(x) \right)
		$$

%	\shrinkspacewithintheoremslike\
%	\ibit
%	\iitem
		note this theorem subsumes
		\theoremname~\ref{theorem:weak alternatives of two systems}\
		and
		\theoremname~\ref{theorem:weak alternatives of two systems with strict inequalities}\
%	\eit
\end{mytheorem}


\myfoilhead{Strong alternatives for generalized inequalities}

\begin{mytheorem}{strong alternatives for generalized inequalities}
	for \bigpropercone-convex $\fie:\xie \to \bigtimes \reals^{k_i}$ \& affine $\feq:\xeq \to \reals^p$
	where \xie\ and \xeq\ are subsets of $\reals^n$
	assuming $\optdomain = \xie \cap \xeq \neq \emptyset$,
	and $\lambda \in \bigtimes \reals^{k_i}$ \& $\nu \in \reals^p$,
	if exists $x\in\relint \optdomain$ with $\feq(x)=0$,
	below pairs of systems are strong alternatives
		\begin{eqnarray*}
%		$$
			\fie(x) \preceq_\bigpropercone 0
			\quad
			\feq(x) = 0
			\quad
			&
			\&
			&
			\quad
			\lambda\succeq_{\bigpropercone^\ast} 0
			\quad
			g(\lambda,\nu) > 0
%		$$
			\\
%		$$
			\fie(x) \prec_\bigpropercone 0
			\quad
			\feq(x) = 0
			\quad
			&
			\&
			&
			\quad
			\lambda\succeq_{\bigpropercone^\ast} 0
			\quad
			\lambda \neq 0
			\quad
			g(\lambda,\nu) \geq 0
%		$$
		\end{eqnarray*}
	where $\bigpropercone = \bigtimes K_i$ with proper cones $K_i\subset\reals^{k_i}$
	and function $g:\bigtimes \reals^{k_i} \times \reals^p \to \reals$ defined by
		$$
			g(\lambda,\nu) = \inf_{x\in\optdomain} \left( \lambda^T \fie(x) + \nu^T \feq(x) \right)
		$$

	note this theorem subsumes
	\theoremname~\ref{theorem:strong alternatives of two systems}\
	and
	\theoremname~\ref{theorem:strong alternatives of two systems with strict inequalities}\
\end{mytheorem}


\myfoilhead{Strong alternatives for SDP}

\bit
\item
	for $F_1,\ldots,F_n,G\in\symset{k}$, $x\in\reals^n$, and $Z\in\symset{k}$
	\bit
	\item
		below systems are strong alternatives
		$$
			x_1F_1 + \cdots + x_nF_n + G \prec 0
		$$
		and
		$$
			Z \succeq 0 \quad Z\neq 0 \quad G\bullet Z \geq 0
			\quad F_i \bullet Z = 0\;i=1,\ldots,n
		$$

	\vitem
		if $\sum v_i F_i \succeq 0 \Rightarrow \sum v_i F_i = 0$,
		below systems are strong alternatives
		$$
			x_1F_1 + \cdots + x_nF_n + G \preceq 0
		$$
		and
		$$
			Z \succeq 0 \quad G\bullet Z > 0
			\quad F_i \bullet Z = 0\;i=1,\ldots,n
		$$
	\eit
\eit
\vfill


\titlefoil{Unconstrained Minimization}{Unconstrained Minimization}

\myfoilhead{Unconstrained minimization}

\bit
\item
	consider
	unconstrained convex optimization problem,
	\ie, $m=p=0$ in \definitionname~\ref{definition:convex optimization}
	$$
		\begin{array}{ll}
			\mbox{minimize} &
				\fobj(x)
		\end{array}
	$$
	where
%		$\fobj:\xobj \to \reals$ is convex
%	and
		domain of optimization problem is $\optdomain\ = \xobj \subset \reals^n$

\vitem
	assume
	\bit
	\vitem
		\fobj\ is twice-differentiable (hence by definition \xobj\ is open)
	\vitem
		optimal solution $x^\ast$ exists, \ie, $p^\ast = \inf_{x\in\optdomain} \fobj(x) = \fobj(x^\ast)$
	\eit

\vitem
	\theoremname~\ref{theorem:first-order condition for convexity}
%	(and \theoremname~\ref{theorem:KKT and convexity sufficient for optimality with strong duality})
	implies
	$x^\ast$ is optimal solution\
	\iaoi\
	$$
		\nabla \fobj(x^\ast) = 0
	$$

\vitem
	can solve above equation directly for few cases,
	but usually
	depend on iterative method,
	\ie,
	find sequence of points $\xseqk{0}, \xseqk{1}, \ldots \in \xobj$\
	such that
	$
		\lim_{k\to\infty} \fobj(\xseqk{k}) = p^\ast
	$
\eit
\vfill


\myfoilhead{Requirements for iterative methods}

\bit
\item
	requirements for iterative methods
	\bit
	\vitem
		initial point \xseqk{0}\ should be in domain of optimization problem,
		\ie\
		$$
			\xseqk{0} \in \xobj\
		$$

	\vitem
		sublevel set of $\fobj(\xseqk{0})$
		$$
			S = \bigset{x\in\xobj}{\fobj(x) \leq \fobj(\xseqk{0})}
		$$
		should be closed
	\eit

\vitem
	\eg\
	\bit
	\vitem
		sublevel set of $\fobj(\xseqk{0})$
		is closed for all $\xseqk{0}\in\xobj$
		if \fobj\ is closed, \ie, all its sublevel sets are closed
	\vitem
		\fobj\ is closed
		if $\xobj = \reals^n$ and \fobj\ is continuous\

	\vitem
		\fobj\ is closed
		if \fobj\ is continuous,
		\xobj\ is open,
		and $\fobj(x) \to \infty$ as $x \to \boundary \xobj$

	\eit
\eit
\vfill


\myfoilhead{Unconstrained minimization examples}

\bit
\item
	convex quadratic problem
	$$
		\begin{array}{ll}
			\mbox{minimize} &
				\fobj(x) =
				(1/2) x^TP x +q^Tx
		\end{array}
	$$
	where $P\in\possemidefset{n}$ and $q\in\reals^n$
	\bit
	\vitem
		solution obtained by solving
		$$
			\nabla \fobj(x^\ast) = P x^\ast + q = 0
		$$
		\bit
		\viitem
			if solution exists, $x^\ast = - P^\dagger q$ (thus $p^\ast>-\infty$)
		\viitem
			otherwise, problem is unbounded below, \ie, $p^\ast = -\infty$
		\eit

	\vitem
		\eemph{%
		ability to analytically solve quadratic minimization problem
		is basis for Newton's method,
		power method for unconstrained minimization%
		}

	\vitem
		least-squares (LS) is special case of convex quadratic problem
		$$
			\begin{array}{ll}
				\mbox{minimize} &
					(1/2) \|Ax-b\|_2^2
					= (1/2) x^T (A^TA) x - b^TAx + (1/2)\|b\|_2^2
			\end{array}
		$$
		\bit
		\viitem
			optimal always exists, can be obtained via normal equations
			$$
				A^T Ax^\ast = b
			$$
		\eit
	\eit

\vitem
	unconstrained GP
	$$
		\begin{array}{ll}
			\mbox{minimize} &
				\fobj(x) =
				\log\left(
					\sum \exp (Ax+b)
				\right)
		\end{array}
	$$
	for $A\in\reals^{m\times n}$ and $b\in\reals^m$
	\bit
	\vitem
		solution obtained by solving
		$$
			\nabla \fobj(x^\ast) = \frac{\sum A^T \exp(Ax^\ast+b)}{\sum \exp(Ax^\ast+b)} = 0
	 	$$

	\vitem
		need to resort to iterative method -
		since $\xobj = \reals^n$ and \fobj\ is continuous,
		\fobj\ is closed,
		hence
		every point in $\reals^n$ can be initial point
	\eit

\vitem
	analytic center of linear inequalities
	$$
		\begin{array}{ll}
			\mbox{minimize} &
				\fobj(x) = - \sum\log(b-Ax)
		\end{array}
	$$
	where $\xobj = \set{x\in\reals^n}{b-Ax \succ 0}$
	\bit
	\vitem
		need to resort to iterative method -
		since \xobj\ is open, \fobj\ is continuous,
		and $\fobj(x) \to \infty$ as $x\to\boundary \xobj$,
		\fobj\ is closed,
		hence
		every point in \xobj\ can be initial point
	\vitem
		\fobj, called \define{logarithmic barrier} for inequalities $Ax\preceq b$
	\eit

\vitem
	analytic center of LMI
	$$
		\begin{array}{ll}
			\mbox{minimize} &
				\fobj(x) = - \log\det F(x) = \log\det F(x)^{-1}
		\end{array}
	$$
	where $F:\reals^n\to \symset{k}$ is defined by
	$$
		F(x) = x_1F_1 + \cdots + x_nF_n
	$$
	where $F_i\in \symset{k}$
	and $\xobj = \set{x\in\reals^n}{F(x)\succ 0}$
	\bit
	\vitem
		need to resort to iterative method -
		since \xobj\ is open, \fobj\ is continuous,
		and $\fobj(x) \to \infty$ as $x\to\boundary \xobj$,
		\fobj\ is closed,
		hence
		every point in \xobj\ can be initial point
	\vitem
		\fobj, called \define{logarithmic barrier} for LMI
	\eit
\eit
\vfill


\myfoilhead{Strong convexity and implications}

\bit
\item
	function $\fobj$ is strongly convex on $S$\
	$$
		\left(
			\exists m >0
		\right)
		\left(
			\forall x \in S
		\right)
		\left(
			\nabla^2 \fobj(x) \succeq mI
		\right)
	$$

\vitem
	strong convexity implies for every $x,y\in S$
	$$
		\fobj(y) \geq \fobj(x) + \nabla \fobj(x)^T (y-x) + ({m}/{2}) \|y-x\|_2^2
	$$
	\bit
	\item
		which implies
		gradient provides optimality certificate
		and tells us how far current point is from optimum,
		\ie\
		$$
			\fobj(x) - p^\ast \leq ({1}/{2m}) \|\nabla \fobj(x)\|_2^2
			\quad
			\|x-x^\ast\|_2 \leq ({2}/{m}) \|\nabla \fobj(x)\|_2
		$$
%		\begin{eqnarray*}
%			\fobj(x) - p^\ast \leq ({1}/{2m}) \|\nabla \fobj(x)\|_2^2
%		&-&
%			\mbox{optimality certificate}
%		\\
%			\|x-x^\ast\|_2 \leq ({2}/{m}) \|\nabla \fobj(x)\|_2
%		&-&
%			\mbox{upper limit on distance from optimum}
%		\end{eqnarray*}
	\eit

\vitem
	first equation implies sublevel sets contained in $S$ is bounded,
	hence continuous function $\nabla^2 \fobj(x)$ is also bounded,
	\ie,
	$\left( \exists M >0 \right) \left( \nabla^2 \fobj(x) \preceq M I \right)$,
	then
	$$
%		p^\ast \leq \fobj(x) - \frac{1}{2M} \|\nabla \fobj(x)\|_2^2
		\fobj(x) - p^\ast \geq \frac{1}{2M} \|\nabla \fobj(x)\|_2^2
	$$
\eit
\vfill


\myfoilhead{Iterative methods}

\begin{mydefinition}{iterative meethods}
	numerical method generating sequence of points $\xseqk{0}, \xseqk{1}, \ldots \in S\subset \reals^n$
	to make $\fobj(\xseqk{k})$ approaches to some desired value from some $f:S\to\reals$,
	called \define{iterative method}
\end{mydefinition}

\begin{mydefinition}{iterative meethods with search directions}
	iterative method generating
	search direction $\sdirk{k}\in\reals^n$ and
	step length $\slenk{k}>0$ at each step $k$
	such that
	$$
		\xseqk{k+1} = \xseqk{k} + \slenk{k} \sdirk{k}
	$$
	called \define{iterative method with search direction}
	where \sdirk{k}, called \define{search direction},
	\slenk{k}, called \define{step length} (which actually is not length)

\end{mydefinition}

\begin{mydefinition}{descent methods}
	for function $f:S\to\reals$,
	iterative method reducing function value,
	\ie\
	$$
		\fobj(\xseqk{k+1}) \leq \fobj(\xseqk{k})
	$$
	for $k=0,1,\ldots$,
	called \define{descent method}
\end{mydefinition}
\vfill


\myfoilhead{Line search methods}

\begin{mydefinition}{line search method}
	for iterating method with search directions,
	determining
	search direction \sdirk{k}\
	and
	step length \slenk{k}\
	for each step,
	called \define{line search method}
\end{mydefinition}

\begin{myalgorithm}{exact line search}
	for descent iterating method with search directions,
	determine \slen\ by
	$$
		\slen = \argmin_{s>0} \fobj(x +s\sdir)
	$$
\end{myalgorithm}

\begin{myalgorithm}{backtracking line search}
	for descent iterating method with search directions,
	determine \slen\ by
	\begin{algorithmic}
	\Require $\fobj$, \sdirk{k}, $\alpha\in(0,0.5)$, $\beta\in(0,1)$
	\State $\slen:=1$
	\While{$\fobj(\xseqk{k} + \slen \sdirk{k}) > \fobj(\xseqk{k}) + \alpha \slen \nabla \fobj(\xseqk{k})^T \sdirk{k}$}
	\State{$\slen := \beta \slen$}
	\EndWhile
	\end{algorithmic}
\end{myalgorithm}
\vfill


\myfoilhead{Gradient descent method}

\begin{myalgorithm}{gradient descent method}\ %
\begin{algorithmic}
\Require $\fobj$, initial point $x\in \dom \fobj$
\Repeat
	\State{search direction - $\sdir := - \nabla \fobj(x)$}
	\State{do line search to choose $\slen>0$}
	\State{update - $x := x + \slen \sdir$}
\Until{stopping criterion satisfied}
\end{algorithmic}
\end{myalgorithm}


\myfoilhead{Summary of gradient descent method}

\bit
\item
	gradient method often exhibits approximately linear convergence,
	\ie,
	error $\fobj(\xseqk{k})-p^\ast$ converges to zero approximately as geometric series

\vitem
	choice of backtracking parameters $\alpha$ and $\beta$
	has noticeable but not dramatic effect on convergence

\vitem
	exact line search sometimes improves convergence of gradient method,
	but not by large,
	hence mostly not worth implementation

\vitem
	converge rate depends greatly on condition number of Hessian
	or sublevel sets
	- when condition number if large, gradient method can be useless
\eit
\vfill


\myfoilhead{Newton's method - motivation}

\bit
\item
	second-order Taylor expansion of $\fobj$
	-
	$
		\hat{\fobj}(\sdir) =
%		\fobj(x + \slen \sdir) = \fobj(x) + \slen \nabla \fobj(x)^T \sdir + \frac{1}{2} \slen^2 \sdir^T \nabla^2 \fobj(x) \sdir
		\fobj(x + \sdir) = \fobj(x) + \nabla \fobj(x)^T \sdir + \frac{1}{2} \sdir^T \nabla^2 \fobj(x) \sdir
	$

\vitem
	minimum of Taylor expansion achieved when
	$
		\nabla \hat{\fobj}(\sdir) = \nabla \fobj(x) + \nabla^2 \fobj(x) v = 0
	$

\vitem
	solution called \define{Newton step}
	$$
		\sdir_\mathrm{nt}(x) = - \nabla^2 \fobj(x)^{-1} \nabla \fobj(x)
	$$
	assuming $\nabla^2\fobj(x)\succ0$

\vitem
	thus Newton step minimizes local quadratic approximation of function

\vitem
	difference of current and quadratic approximation minimum
	$$
		\fobj(x) - \hat{\fobj}(\sdir_\mathrm{tn}(x))
%		=
%		- \nabla \fobj(x)^T \sdir_\mathrm{nt} - \frac{1}{2} \sdir_\mathrm{nt}^T \nabla^2 \fobj(x) \sdir_\mathrm{nt}
		=
		\frac{1}{2} \sdir_\mathrm{nt}^T \nabla^2 \fobj(x) \sdir_\mathrm{nt}
		=
		\frac{1}{2} \lambda(x)^2
	$$

\vitem
	\define{Newton decrement}%
		\index{Newton decrement}%
		\index{Newton, Isaac!Newton decrement}%
		\pagelabel{page:Newton decrement in quadratic approximation}
	$$
		\lambda(x)
			=
		\sqrt{\sdir_\mathrm{nt}(x)^T \nabla^2 \fobj(x) \sdir_\mathrm{nt}(x)}
			=
		\sqrt{\nabla \fobj(x)^T \nabla^2 \fobj(x)^{-1} \nabla \fobj(x)}
	$$
\eit


\myfoilhead{Newton's method}

\begin{myalgorithm}{Newton's method}
	damped descent method using Newton step
\begin{algorithmic}
\Require \fobj, initial point $x\in \dom \fobj$, tolerance $\epsilon>0$
\Loop
	\State{computer Newton step and descrement
		$$
			\sdir_\mathrm{nt}(x) := -\nabla^2 \fobj(x)^{-1} \nabla \fobj(x)
			\quad
			\lambda(x)^2 := \nabla \fobj(x)^T \nabla^2 \fobj(x)^{-1} \nabla \fobj(x)
		$$
	}
	\State{stopping criterion - quit if $\lambda(x)^2/2 < \epsilon$}
	\State{do line search to choose $t>0$}
	\State{update - $x := x + \slen \sdir_\mathrm{nt}$}
\EndLoop
\end{algorithmic}
\end{myalgorithm}
\index{Newton's method}%
\index{Newton, Isaac!Newton's method}

\bit
\item
	Newton step is descent direction since
	$$
		\left.
		\left(
		\frac{d}{dx}\fobj(x+t\sdir_\mathrm{nt})
		\right)
		\right|_{t=0}
		=
		\nabla \fobj(x) ^T \sdir_\mathrm{nt}
%		=
%		- \nabla \fobj(x)^T \nabla^2 \fobj(x)^{-1} \nabla \fobj(x)
		=
		- \lambda(x)^2
		<0
	$$
%	implied by $\nabla^2 \fobj(x) \succ 0$
\eit
\vfill


\myfoilhead{Assumptions for convergence analysis of Newton's method}

\bit
\item
	assumptions
	\bit
	\vitem
	%	assume strong convexity and boundedness of Hessian\
		strong convexity and boundedness of Hessian on sublevel set\
		$$
			\left(
				\exists\; m, M > 0
			\right)
			\left(
				\forall x \in S
			\right)
			\left(
				mI \preceq \nabla^2 \fobj(x) \preceq MI
			\right)
		$$

	\vitem
	%	further assume Lipschitz continuity on Hessian\
		Lipschitz continuity of Hessian on sublevel set\
		$$
			\left(
				\exists L > 0
			\right)
			\left(
				\forall x,y\in S
			\right)
			\left(
				\|\nabla^2 \fobj(x)- \nabla^2\fobj(y)\|_2 \leq L \|x-y\|_2
			\right)
		$$
	\eit

\vvitem
	Lipschitz continuity constant $L$ plays critical role
	in performance of Newton's method
	\bit
	\vitem
		intuition says
		Newton's method
		works well for functions
		whose quadratic approximations
		do not change fast,
		\ie,
		when $L$ is small
	\eit
\eit
\vfill


\myfoilhead{Convergence analysis of Newton's method}

\begin{mytheorem}{convergence analysis of Newton's method}
	for function $\fobj$ satisfying strong convexity, Hessian continuity \& Lipschitz continuity
	with $m, M, L>0$,
	exist $0<\eta<m^2/L$ and $\gamma > 0$
	such that
	for each step $k$\
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		damped Newton phase
		-
		if $\|\nabla \fobj(\xseqk{k})\|_2 \geq \eta$,
		$$
			\fobj(\xseqk{k+1}) - \fobj(\xseqk{k}) \leq - \gamma
		$$

	\iitem
		quadratic convergence phase
		-
		if $\|\nabla \fobj(\xseqk{k})\|_2 < \eta$,
		backtracking line search selects step length $\slenk{k}=1$
		$$
			\frac{L}{2m^2} \|\nabla \fobj(\xseqk{k+1})\|_2
				\leq
			\left(
				\frac{L}{2m^2} \|\nabla \fobj(\xseqk{k})\|_2
			\right)^2
		$$
	\eit
%	thus
	\# iterations of Newton's method required to satisfy stopping criterion
	$\fobj(\xseqk{k})-p^\ast\leq\epsilon$ is
	$$
		\frac{\fobj(\xseqk{0}) - p^\ast}{\gamma}
		+ \log_2 \log_2 (\epsilon_0 / \epsilon)
		\quad
		\mbox{where }
		\epsilon_0 = 2 m^3/L^2
	$$
\end{mytheorem}


\myfoilhead{Summary of Newton's method}

\bit
\item
	Newton's method is \emph{affine invariant},
	hence \emph{performance is independent of condition number unlike gradient method}

\vitem
	once entering quadratic convergence phase,
	Newton's method converges extremely fast

\vitem
	performance not much dependent on choice of algorithm parameters

\vitem
	big disadvantage is
	computational cost for evaluating search direction,
	\ie, solving linear system
\eit
\vfill


\myfoilhead{Self-concordance}

\begin{mydefinition}{self-concordance}
	convex function $f:X\to \reals$
	with $X\subset \reals^n$
	such that
	for all $x\in X, v\in\reals^n$,
	$g(t) = f(x+tv)$
	with $\dom g = \set{t\in\reals}{x+tv\in X}$
	satisfies
	$$
		\left(
			\forall t\in\dom g
		\right)
		\left(
			|g'''(t)| \leq 2 g''(t)^{3/2}
		\right)
	$$
\end{mydefinition}
\vfill

\begin{myproposition}{self-concordance for logarithms}
	if convex function $g:X\to\reals$ with $X\subset \ppreals$
	satisfies
	$$
		|g'''(x)| \leq 3 g''(x) / x
	$$
	function $f$ with $\dom f = \set{x\in\ppreals}{g(x)<0}$
	defined by
	$$
		f(x) = -\log(-g(x)) - \log x
	$$
	and
	function $h$ with $\dom h = \set{x\in\ppreals}{g(x)+ax^2+bx + c<0}$
	with $a\geq0$
	defined by
	$$
		h(x) = -\log(-g(x)-ax^2-bx-c) - \log x
	$$
	are self-concordant
\end{myproposition}
\vfill


\myfoilhead{Why self-concordance?}

\bit
\item
	convergence analysis of Newton's method depends on
	assumptions about function characteristics,
	\eg,
	$m,M, L > 0$
	for strong convexity, continuity of Hessian,
	\ie\
	$$
		m I \preceq \nabla^2 f(x) \preceq M I
			\quad
%		\|\nabla^2 f(x)- \nabla^2f(y)\|_2 \leq L \|x-y\|_2
		\|\nabla^2 f(x)- \nabla^2f(y)\| \leq L \|x-y\|
	$$

\vitem
	\define{self-concordance}
	discovered by Nesterov and Nemirovski
	(who gave name self-concordance)
	plays important role
	for reasons
	such as
	\bit
	\vitem
		convergence analysis does not depend any function characterizing paramters

	\vitem
		many barrier functions which are used for interior-point methods,
		which are important class of optimization algorithms
		are self-concordance

	\vitem
		property of self-concordance is affine invariant

	\eit
\eit


\myfoilhead{Self-concordance preserving operations}

\begin{myproposition}{self-concordance preserving operations}
	self-concordance is preserved by \emph{positive scaling, addition, and affine transformation,}
	\ie,
	if $f, g:X\to\reals$ are self-concordant functions with $X\subset\reals^n$,
	$h:H\to\reals^n$ with $H\subset \reals^m$ are affine functions,
	and $a>0$\
	$$
		af,
		\quad
		f+g,
		\quad
		f\circ h
	$$

	are self-concordant
	where
	$\dom f\circ h = \set{x\in H}{h(x) \in X}$

\end{myproposition}


\myfoilhead{Self-concordant function examples}

\bit
\item
	negative logarithm
	-
	$f:\ppreals \to \reals$ with
	$$
		f(x)=-\log x
	$$
	is self-concordant since
	$$
		|f'''(x)| / f''(x)^{3/2} = \left(2/x^3\right) / \left((1/x^2)^{3/2}\right) = 2
	$$

\vitem
	negative entropy plus negative logarithm
	-
	$f:\ppreals \to \reals$ with $$f(x)=x\log x-\log x$$
	is self-concordant since
	$$
		|f'''(x)| / f''(x)^{3/2} = (x+2)/{(x+1)^{3/2}}
		\leq 2
	$$
%	hence is self-concordant with $c=2$ in \definitionname~\ref{definition:self-concordance}

\vitem
	log barrier for linear inequalities
	-
	for $A\in\reals^{m\times n}$ and $b\in\reals^m$
	$$
		f(x) = - \sum \log(b-Ax)
	$$
	with $\dom f = \set{x\in\reals^n}{b-Ax \succ 0}$
	is self-concordant
	by
	\propositionname~\ref{proposition:self-concordance preserving operations},
	\ie, $f$ is affine transformation of sum of self-concordant functions

\vitem
	log-determinant
	-
	$f:\posdefset{n}\to\reals$
	with
	$$
		f(X) = \log\det X^{-1} = - \log\det X
	$$
	is self-concordant since
	for every $X\in \posdefset{n}$ and $V\in\symset{n}$
	function $g:\reals\to\reals$ defined by $g(t) = - \log\det(X+tV)$
	where $\dom f = \set{t\in\reals}{X+tV\succeq 0}$
	is self-concordant
	since
	\begin{eqnarray*}
		\lefteqn{
			g(t) = - \log \det (X^{1/2} (I + tX^{-1/2} V X^{-1/2})X^{1/2})
		}
		\\
		&=&
			-\log\det X - \log\det(I+tX^{-1/2}VX^{-1/2})
		\\
		&=&
			-\log\det X - \sum \log (1+t\lambda_i(X,V))
	\end{eqnarray*}
	where
	$\lambda_i(X,V)$ is $i$-th eigenvalue of $X^{-1/2}VX^{1/2}$
	is self-concordant
	by
	\propositionname~\ref{proposition:self-concordance preserving operations},
	\ie, $g$ is affine transformation of sum of self-concordant functions

\vitem
	log of concave quadratic
	-
	$f:X\to\reals$
	with
	$$
		f(x) = -\log(-x^TPx - q^Tx - r)
	$$
	where
		$P\in\possemidefset{n}$
	and
		$X=\set{x\in\reals^n}{x^TPx + q^Tx + r<0}$

\vitem
	function $f:X\to\reals$
	with
	$$
		f(x) = -\log(-g(x)) - \log x
	$$
	where $\dom f = \set{x\in\dom g \cap \ppreals}{g(x)<0}$
	and
	function $h:H\to\reals$
	$$
		h(x) = -\log(-g(x)-ax^2-bx-c) - \log x
	$$
	where $a\geq0$ and $\dom h = \set{x\in\dom g \cap \ppreals}{g(x)+ax^2+bx+c<0}$
	are self-concordant
	if $g$ is one of below
	\bit
	\vitem
		$g(x) = -x^p$ for $0<p\leq 1$
	\vitem
		$g(x) = -\log x$
	\vitem
		$g(x) = x \log x$
	\vitem
		$g(x) = x^p$ for $-1\leq p\leq 0$
	\vitem
		$g(x) = (ax+b)^2/x$ for $a,b\in\reals$
	\eit
	since above $g$ satisfy
	$|g'''(x)| \leq 3 g''(x)/x$
	for every $x\in\dom g$
	(\propositionname~\ref{proposition:self-concordance for logarithms})

\vitem
	function $f:X\to\reals$
	with $X = \set{(x,y)}{\|x\|_2 < y}\subset \reals^n \times \ppreals$
	defined by
	$$
		f(x,y) = -\log(y^2-x^Tx)
	$$
	is self-concordant - can be proved using \propositionname~\ref{proposition:self-concordance for logarithms}\

\vitem
	function $f:X\to\reals$
	with $X = \set{(x,y)}{|x|^p < y}\subset \reals \times \ppreals$
	defined by
	$$
		f(x,y) = -2\log y - \log(y^{2/p}- x^2)
	$$
	where $p\geq1$
	is self-concordant - can be proved using \propositionname~\ref{proposition:self-concordance for logarithms}\

\vitem
	function $f:X\to\reals$
	with $X = \set{(x,y)}{\exp(x) < y}\subset \reals \times \ppreals$
	defined by
	$$
		f(x,y) = -\log y - \log(\log y - x)
	$$
	is self-concordant - can be proved using \propositionname~\ref{proposition:self-concordance for logarithms}\
\eit


\myfoilhead{Properties of self-concordant functions}

\begin{mydefinition}{Newton decrement}
	for convex function $f:X\to\reals$ with $X\subset \reals^n$,
	function $\lambda:\tilde{X}\to\preals$
	with $\tilde{X} = \set{x\in X}{\nabla^2 \fobj(x) \succ 0}$
	defined by
	$$
		\lambda(x) = (\nabla \fobj(x)^T \nabla^2 \fobj(x)^{-1} \nabla \fobj(x))^{1/2}
	$$
	called \define{Newton decrement}
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		note
		$$
			\lambda(x)
			=
			\sup_{v\neq 0}
			\left(v^T \nabla \fobj(x) / \left( v^T \nabla^2 \fobj(x) v \right)^{1/2} \right)
		$$
	\eit
\end{mydefinition}
\index{Newton decrement}%
\index{Newton, Isaac!Newton decrement}%

\begin{mytheorem}{optimality certificate for self-concordant functions}
	for strictly convex self-concordant function $f:X\to\reals^n$ with $X\subset \reals^n$,
	Hessian is positive definition everywhere (hence Newton decrement is defined everywhere)
	and for every $x\in X$
	$$
		p^\ast \geq \fobj(x) - \lambda(x)^2
		\quad
		\Leftrightarrow
		\quad
		\fobj(x) - p^\ast \leq \lambda(x)^2
	$$
	if $\lambda(x) \leq 0.68$
\end{mytheorem}
\vfill


\myfoilhead{Stopping criteria for self-concordant objective functions}

\bit
\item
	recall $\lambda(x)^2$ provides \emph{approximate} optimality certificate,
	(page~\pageref{page:Newton decrement in quadratic approximation})
	\ie,
	assuming $\fobj$ is well approximated by quadratic function around $x$
	$$
		\fobj(x) - p^\ast \lessapprox \lambda(x)^2/2
	$$

\vitem
	however, strict convexity together with self-concordance
	provides proven bound
	(by \theoremname~\ref{theorem:optimality certificate for self-concordant functions})
	$$
		\fobj(x) - p^\ast \leq \lambda(x)^2
	$$
	for $\lambda(x) \leq 0.68$

\vitem
	hence can use following stopping criterion for guaranteed bound
	$$
		\lambda(x)^2 \leq \epsilon
		\quad
		\Rightarrow
		\quad
		\fobj(x) - p^\ast \leq \epsilon
	$$
	for $\epsilon \leq 0.68^2$
\eit
\vfill


\myfoilhead{Convergence analysis of Newton's method for self-concordant functions}

\begin{mytheorem}{convergence analysis of Newton's method for self-concordant functions}
	for strictly convex self-concordant function \fobj,
	exist $0<\eta\leq 1/4$ and $\gamma>0$ (which depend only on line search parameters)
	such that
	\shrinkspacewithintheoremslike\
	\ibit
	\iitem
		damped Newton phase
		-
		if $\lambda(\xseqk{k})>\eta$
		$$
			\fobj(\xseqk{k+1}) - \fobj(\xseqk{k}) \leq - \gamma
		$$

	\iitem
		quadratic convergence phase
		-
		if $\lambda(\xseqk{k})\leq\eta$
		backtracking line search selects step length $\slenk{k}=1$
		$$
			2\lambda(\xseqk{k+1})
			\leq
			\left(2\lambda(\xseqk{k})\right)^2
		$$
	\eit
	\# iterations required to satisfy stopping criterion
	$\fobj(\xseqk{k})-p^\ast\leq\epsilon$ is
	$$
%		\frac{\fobj(\xseqk{0}) - p^\ast}{\gamma}
		\left(\fobj(\xseqk{0}) - p^\ast\right)/{\gamma}
		+ \log_2 \log_2 (1 / \epsilon)
	$$
	where $\gamma = \alpha \beta (1-2\alpha)^2 / (20-8\alpha)$
\end{mytheorem}
\vfill


\titlefoil{Equality Constrained Minimization}{Equality Constrained Minimization}

\myfoilhead{Equality constrained minimization}

\bit
\item
	consider
	equality constrained convex optimization problem,
	\ie, $m=0$ in \definitionname~\ref{definition:convex optimization}
	$$
		\begin{array}{ll}
			\mbox{minimize} &
				\fobj(x)
			\\
			\mbox{subject to} &
				Ax = b
		\end{array}
	$$
	where
		$A\in\reals^{p\times n}$
	and
		domain of optimization problem is $\optdomain\ = \xobj \subset \reals^n$

\vvitem
	assume
	\bit
	\vitem
		$\rank A = p<n$,
		\ie, rows of $A$ are linearly independent
	\vitem


		\fobj\ is twice-differentiable (hence by definition \xobj\ is open)
	\vitem
		optimal solution $x^\ast$ exists, \ie, $p^\ast = \inf_{x\in\optfeasset} \fobj(x) = \fobj(x^\ast)$
		and $Ax^\ast = b$
	\eit
\eit
\vfill


\myfoilhead{Solving KKT for equality constrained minimization}

\bit
\item
	\theoremname~\ref{theorem:KKT and convexity sufficient for optimality with strong duality}
	implies
	$x^\ast\in\xobj$ is optimal solution \iaoi\
	exists $\nu^\ast\in\reals^p$
	satisfy KKT optimality conditions,%
		\pagelabel{page:KKT conditions for equality constrained minimization}
	\ie,
	\begin{eqnarray*}
		Ax^\ast = b
		&&\mbox{\define{primal feasibility equations}}
		\\
		\nabla \fobj(x^\ast) + A^T\nu^\ast = 0
		&&\mbox{\define{dual feasibility equations}}
	\end{eqnarray*}

\vitem
	solving equality constrained problem
	is equivalent to
	solving KKT equations
	\bit
	\item
		handful types of problems can be solved analytically
	\eit

\vitem
	using unconstrained minimization methods
	\bit
	\item
		can eliminate equality constraints and apply unconstrained minimization methods
	\item
		solving dual problem using unconstrained minimization methods
		and retrieve primal solution (refer to page~\pageref{page:Solving primal problems via dual problems})
	\eit

\vvitem
	will discuss Newton's method directly handling equality constraints
	\bit
	\item
		preserving problem structure such as sparsity
	\eit
\eit
\vfill


\myfoilhead{Equality constrained convex quadratic minimization}

\bit
\item
	equality constrained convex quadratic minimization problem
	$$
		\begin{array}{ll}
			\mbox{minimize} &
				\fobj(x) = (1/2)x^T P x + q^Tx
			\\
			\mbox{subject to} &
				Ax = b
		\end{array}
	$$
	where $P\in\possemidefset{n}$ and $A\in\reals^{p\times n}$

\vitem
	important since basis for extension of Newton's method to equality constrained problems

\vitem
	\define{KKT system}
	$$
		Ax^\ast = b \; \& \; Px^\ast + q + A^T\nu^\ast = 0
		\;
		\Leftrightarrow
		\;
		\underbrace{
		\mattwotwo{P}{A^T}{A}{0}
		}_{\mbox{\define{KKT matrix}}}
		\colvectwo{x^\ast}{\nu^\ast}
		=
		\colvectwo{-q}{b}
	$$

\vitem
	exist primal and dual optimum $(x^\ast,\nu^\ast)$ \iaoi\ KKT system has solution;
	otherwise, problem is unbounded below
\eit
\vfill


\myfoilhead{Eliminating equality constraints}

\bit
\item
	can solve equality constrained convex optimization
	by
	\bit
	\item
		eliminating equality constraints and
	\item
		using optimization method for solving unconstrained optimization
	\eit

\vitem
	note
	$$
		\optfeasset
		=
		\set{x}{Ax=b}
		=
		\set{Fz + x_0}{z\in\reals^{n-p}}
	$$
	for some $F\in\reals^{n\times(n-p)}$
	where $\range(F) = \nullspace(A)$

\vitem
	thus original problem equivalent to
	$$
		\begin{array}{ll}
			\mbox{minimize} &
				\fobj(Fz + x_0)
		\end{array}
	$$

\vitem
	if $z^\ast$ is optimal solution, $x^\ast = Fz^\ast + x_0$

\vitem
	optimal dual can be retrieved by
	$$
		\nu^\ast = - (AA^T)^{-1} A\nabla \fobj(x^\ast)
	$$
\eit


\myfoilhead{Solving dual problems}

\bit
\item
	Lagrange dual function of equality constrained problem
	\begin{eqnarray*}
			g(\nu)
		&
		=
		&
			\inf_{x\in\optdomain}
			\left(
				\fobj(x) + \nu^T(Ax-b)
			\right)
		=
			-b^T\nu - \sup_{x\in\optdomain} \left((-A^T\nu)^Tx -\fobj(x)\right)
		\\
		&
		=
		&
			-b^T \nu - {\fobj}^\ast(-A^T\nu)
	\end{eqnarray*}

\vitem
	dual problem
	$$
		\begin{array}{ll}
			\mbox{maximize} &
				-b^T \nu - {\fobj}^\ast(-A^T\nu)
		\end{array}
	$$

\vitem
	by assumption, strong duality holds, hence
	if $\nu^\ast$ is dual optimum
	$$
		g(\nu^\ast) = p^\ast
	$$

\vitem
	if dual objective is twice-differentiable,
	can solve dual problem using unconstrained minimization methods

\vitem
	primal optimum can be retrieved using method on page~\pageref{page:Solving primal problems via dual problems})

\eit
\vfill


\myfoilhead{Newton's method with equality constraints}

\bit
\item
	finally discuss Newton's method which directly handles equality constraints

	\bit
	\vitem
		similar to Newton's method for unconstrained minimization

	\vitem
		initial point, however, should be feasible, \ie, $\xseqk{0}\in\xobj$ and $A\xseqk{0} = b$

	\vitem
		Newton step tailored for equality constrained problem
	\eit
\eit
\vfill


\myfoilhead{Newton step via second-order approximation}

\bit
\item
	solve original problem approximately by solving
	$$
		\begin{array}{ll}
			\mbox{minimize} &
				\hat{\fobj}(x+\sdir) = \fobj(x) + \nabla \fobj(x)^T \sdir + (1/2) \sdir^T \nabla^2 \fobj(x) \sdir
			\\
			\mbox{subject to} &
				A(x+\sdir) = b
		\end{array}
	$$
	where $x\in\optfeasset$

\vitem
	\emph{Newton step for equality constrained minimization problem},
	defined by
	solution of
	KKT system
	for above convex quadratic minimization problem
	$$
		\mattwotwo{\nabla^2 \fobj(x)}{A^T}{A}{0}
		\colvectwo{\sdir_\mathrm{nt}}{w}
		=
		\colvectwo{-\nabla \fobj(x)}{0}
	$$
	\emph{only when KKT system is nonsingular}
\eit
\vfill


\myfoilhead{Newton step via solving linearized KKT optimality conditions}

\bit
\item
	recall KKT optimality conditions for equality constrained convex optimization problem
	$$
		Ax^\ast = b
		\quad
		\&
		\quad
		\nabla \fobj(x^\ast) + A^T\nu^\ast = 0
	$$

\vitem
	linearize KKT conditions
	\begin{eqnarray*}
%	$$
	&&
		A(x+\sdir) = b
		\quad
		\&
		\quad
		\nabla \fobj(x) + \nabla^2 \fobj(x) \sdir + A^Tw = 0
	\\
	&\Leftrightarrow&
		A\sdir = 0
		\quad
		\&
		\quad
		\nabla^2 \fobj(x) \sdir + A^Tw
		=
		- \nabla \fobj(x)
%	$$
	\end{eqnarray*}
	where $x\in\optfeasset$

\vitem
	Newton step defined by above equations
	is equivalent
	to
	that obtained by second-order approximation
\eit
\vfill


\myfoilhead{Newton decrement for equality constrained minimization}

\bit
\item
	\define{Newton descrement for equality constrained problem}
	is defined by
	$$
		\lambda(x)
		=
		\left(\sdir_\mathrm{nt} \nabla^2 \fobj(x) \sdir_\mathrm{nt}\right)^{1/2}
	$$
\index{Newton decrement!for equality constrained problem}%
\index{Newton, Isaac!Newton decrement!for equality constrained problem}

\vitem
	same expression as that for unconstrained minimization,
	but is \emph{different}
	since Newton step $\sdir_\mathrm{nt}$ is different from that for unconstrained minimization,
	\ie, $\sdir_\mathrm{nt} \neq -\nabla^2 \fobj(x)^{-1} \nabla \fobj(x)$
	(refer to \definitionname~\ref{definition:Newton decrement})

\vitem
	however, as before,
	$$
		\fobj(x) - \inf_{\sdir\in\reals^n}\set{\hat{\fobj}(x+\sdir)}{A(x+\sdir)=b}
		= \lambda(x)^2/2
	$$
	and
	$$
		\left.
		\left(
		\frac{d}{dt}\fobj(x+t\sdir_\mathrm{nt})
		\right)
		\right|_{t=0}
		=
		\nabla \fobj(x) ^T \sdir_\mathrm{nt}
%		=
%		- \nabla \fobj(x)^T \nabla^2 \fobj(x)^{-1} \nabla \fobj(x)
		=
		- \lambda(x)^2
		<0
	$$
\eit
\vfill


\myfoilhead{Feasible Newton's method for equality constrained minimization}

\begin{myalgorithm}{feasible Newton's method for equality constrained minimization}\;%
\begin{algorithmic}
\Require $\fobj$, initial point $x\in \dom \fobj$ with $Ax=b$, tolerance $\epsilon>0$
\Loop
	\State{%
		computer Newton step and descrement $\ntsdir(x)$ \& $\lambda(x)$
	}
	\State{stopping criterion - quit if $\lambda(x)^2/2 < \epsilon$}
	\State{do line search on \fobj\ to choose $t>0$}
	\State{update - $x := x + \slen \ntsdir$}
\EndLoop
\end{algorithmic}
\end{myalgorithm}

\bit
\vitem
	\algorithmname~\ref{algorithm:feasible Newton's method for equality constrained minimization}
	\bit
	\item
		assumes
		KKT matrix is nonsingular for every step
	\item
		is \emph{feasible descent method}
		since all iterates are feasible with $\fobj(\xseqk{k+1}) <\fobj(\xseqk{k})$
	\eit
\eit
\vfill


\myfoilhead{Assumptions for convergence analysis of feasible Newton's method for equality constrained minimization}
\pagelabel{page:conv-analysis-assumptions-feasible-equality-Newton-method}

\bit
\item
	feasibility of initial point - $\xseqk{0}\in\dom \fobj \;\&\; A\xseqk{0}=b$

\vitem
	sublevel set $S = \set{x\in \dom \fobj}{\fobj(x) \leq \fobj(\xseqk{0}),\; Ax=b}$
	is closed

\vitem
	boundedness of Hessian on $S$
	$$
		\left(
			\exists M > 0
		\right)
		\left(
			\forall x\in S
		\right)
		\left(
			\nabla^2 \fobj(x) \preceq M I
		\right)
	$$

\vitem
	boundedness of KKT matrix on $S$
	- corresponds to strong convexity assumption in unconstrained minimization
	$$
		\left(
			\exists K >0
		\right)
		\left(
			\forall x \in S
		\right)
		\left(
			\left\|
			\mattwotwo{\nabla^2 \fobj(x)}{A^T}{A}{0}^{-1}
			\right\|_2
			\leq K
		\right)
	$$

\vitem
	Lipschitz continuity of Hessian on $S$
	$$
		\left(
			\exists L > 0
		\right)
		\left(
			\forall x,y\in S
		\right)
		\left(
			\left\|\nabla^2 \fobj(x) - \nabla^2 \fobj(y)\right\|_2
			\leq
			L
			\|x-y\|_2
		\right)
	$$
\eit
\vfill


\myfoilhead{Convergence analysis of feasible Newton's method for equality constrained minimization}%
\pagelabel{page:Convergence analysis of feasible Newton's method for equality constrained minimization}

\bit
\item
	convergence analysis of Newton's method for equality constrained minimization
	can be done by analyzing
	unconstrained minimization after eliminating equality constraints

\vitem
	thus, yield \emph{exactly same} results as
	for unconstrained minimization
	(\theoremname~\ref{theorem:convergence analysis of Newton's method})
	(with different parameter values),
	\ie,
	\bit
	\vitem
		consists of damped Newton phase and quadratic convergence phase
	\vitem
		\# iterations required to achieve $\fobj(\xseqk{k})-p^\ast \leq \epsilon$
		is
		$$
			\left(\fobj(\xseqk{0})-p^\ast\right)/\gamma + \log_2 \log_2 (\epsilon_0/\epsilon)
		$$
	\eit

\vitem
	for \# iterations required to achieve $\fobj(\xseqk{k})-p^\ast \leq \epsilon$
	for self-concordant functions
	is also same as
	for unconstrained minimization
	(\theoremname~\ref{theorem:convergence analysis of Newton's method for self-concordant functions})
	$$
		\left(\fobj(\xseqk{0}) - p^\ast\right)/{\gamma}
		+ \log_2 \log_2 (1 / \epsilon)
	$$
	where $\gamma = \alpha \beta (1-2\alpha)^2 / (20-8\alpha)$
\eit
\vfill


\myfoilhead{Newton step at infeasible points}

\bit
\item
	only assume that $x\in\dom \fobj$ (hence, can be infeasible)

\vitem
	(as before) linearize KKT conditions
	\begin{eqnarray*}
	&&
		A(x+\ntsdir) = b
		\quad
		\&
		\quad
		\nabla \fobj(x) + \nabla^2 \fobj(x) \ntsdir + A^Tw = 0
	\\
	&\Leftrightarrow&
		A\ntsdir = b - Ax
		\quad
		\&
		\quad
		\nabla^2 \fobj(x) \ntsdir + A^Tw
		=
		- \nabla \fobj(x)
	\\
	&\Leftrightarrow&
		\mattwotwo{\nabla^2 \fobj(x)}{A^T}{A}{0}
		\colvectwo{\ntsdir}{w}
		=
		-
		\colvectwo{\nabla \fobj(x)}{Ax-b}
	\end{eqnarray*}

\vitem
	same as feasible Newton step \emph{except second component on RHS of KKT system}
\eit
\vfill


\myfoilhead{Interpretation as primal-dual Newton step}

\bit
\item
	update both primal and dual variables $x$ and $\nu$\

\vitem
	define $r:\reals^n\to\reals^p\to\reals^n\times\reals^p$
	by
	$$
		r(x,\nu) = (r_\mathrm{dual}(x,\nu),r_\mathrm{pri}(x,\nu))
	$$
	where
	\begin{eqnarray*}
%	$$
%		\begin{array}{rcl}
		\mbox{\define{dual residual}}
			&
			-
			&
		r_\mathrm{dual}(x,\nu)
			= \nabla \fobj(x) + A^T\nu
			\\
		\mbox{\define{primal residual}}
			&
			-
			&
		r_\mathrm{pri}(x,\nu)
			= Ax-b
%		\end{array}
%	$$
	\end{eqnarray*}
\eit

\vfill


\myfoilhead{Equivalence of infeasible Newton step to primal-dual Newton step}

\bit
\item
	linearize $r$ to obtain primal-dual Newton step, \ie\
	\begin{eqnarray*}
%	$$
	&&
		r(x,\nu) + D_{x,\nu} r(x,\nu) \colvectwo{\pdsdir}{\pdsdirnu} = 0
%	$$
	\\
%	$$
	&\Leftrightarrow&
		\mattwotwo{\nabla^2f(x)}{A^T}{A}{0}
		\colvectwo{\pdsdir}{\pdsdirnu}
		=
		- \colvectwo{\nabla f(x) + A^T\nu}{Ax-b}
%	$$
	\end{eqnarray*}

\vitem
	letting $\nu^+= \nu + \pdsdirnu$ gives
	$$
		\mattwotwo{\nabla^2f(x)}{A^T}{A}{0}
		\colvectwo{\pdsdir}{\nu^+}
		=
		- \colvectwo{\nabla f(x)}{Ax-b}
	$$
	\bit
	\vitem
		equivalent to infeasible Newton step
	\vitem
		reveals that current value of dual variable not needed
	\eit
\eit
\vfill


\myfoilhead{Residual norm reduction property}

\bit
\item
	infeasible Newton step is \emph{not} descent direction (unlike feasible Newton step)
	since
	\begin{eqnarray*}
		\lefteqn{
		\left. \left(
			\frac{d}{dt}\fobj(x+t\pdsdir)
		\right) \right|_{t=0}
		=
		\nabla \fobj(x) ^T \pdsdir
		}
	\\
	&=&
		- \pdsdir^T \left(\nabla^2 \fobj(x) \pdsdir + A^Tw \right)
		=
		- \pdsdir^T \nabla^2 \fobj(x) \pdsdir + (Ax-b)^Tw
	\end{eqnarray*}
	which is not necessarily negative

\vitem
	however, norm of residual decreases in infeasible Newton direction
	\begin{eqnarray*}
		\left.
		\left(
			\frac{d}{dx}
			\|r(y+t\pdsdiry)\|_2^2
		\right)
		\right|_{t=0}
		&
		=
		&
		- 2 r(y)^T r(y) = - 2 \|r(y)\|_2^2
		\\
		\Leftrightarrow
		\quad
		\left.
		\left(
			\frac{d}{dx}
			\|r(y+t\pdsdiry)\|_2
		\right)
		\right|_{t=0}
		&
		=
		&
		\frac{-2\|r(y)\|_2^2}{2\|r(y)\|_2}
		= - \|r(y)\|_2
	\end{eqnarray*}
	where $y=(x,\nu)$ and $\pdsdiry = (\pdsdir, \pdsdirnu)$

\vitem
	can use $r(\xseqk{k},\nuseqk{k})$ to measure optimization progress for infeasible Newton's method
\eit
\vfill


\myfoilhead{Full and damped step feasibility property}

\bit
\item
	assume step length is $t$ at some iteration,
	then
	$$
		r_\mathrm{pri}(x^+,\nu^+) = Ax^+-b
		= A(x + t \pdsdir) - b
		= (1-t) r_\mathrm{pri}(x,\nu)
	$$

\vitem
	hence
	$l>k$
	$$
		\seqk{r}{l}
		=
		\left(
		\prod_{i=k}^{l-1}
		(1-\seqk{t}{i})
		\right)
		\seqk{r}{k}
	$$
	\bit
	\vitem
		primal residual reduced by $1-\seqk{t}{k}$ at step $k$

	\vitem
		Newton step becomes feasible step once full step length ($t=1$) taken
	\eit
\eit
\vfill


\myfoilhead{Infeasible Newton's method for equality constrained minimization}

\begin{myalgorithm}{infeasible Newton's method for equality constrained minimization}\;%
\begin{algorithmic}
\Require $\fobj$, initial point $x\in \dom \fobj$ \& $\nu$, tolerance $\epsilon_\mathrm{pri}>0$ \& $\epsilon_\mathrm{dual}>0$
\Repeat
	\State{%
		computer Newton step and descrement
		$\pdsdir(x)$
		\&
		$\pdsdirnu(x)$,
		\
%		(by solving KKT conditions on page~\pageref{page:KKT conditions for equality constrained minimization})
	}
	\State{do line search on $r(x,\nu)$ to choose $\slen>0$}
	\State{update
		-
		$x := x + \slen \pdsdir$
		\&
		$\nu := \nu + \slen \pdsdirnu$
	}
\Until{$\|r_\mathrm{dual}(x,\nu)\| \leq \epsilon_\mathrm{dual}$ \& $\|Ax-b\| \leq \epsilon_\mathrm{pri}$}
\end{algorithmic}
\end{myalgorithm}

\bit
\vitem
	note similarity and difference of
		\algorithmname~\ref{algorithm:infeasible Newton's method for equality constrained minimization}\
		\&
		\algorithmname~\ref{algorithm:feasible Newton's method for equality constrained minimization}\
	\bit
	\vitem
		line search done not on \fobj, but on primal-dual residuals $r(x,\nu)$
	\vitem
		stopping criteria depends on $r(x,\nu)$, not on Newton decrementa $\lambda(x)^2$
	\vitem
		primal and dual feasibility checked separately
		- here norm in $\|Ax-b\|$ can be any norm,
		\eg,
		$\|\cdot\|_0$,
		$\|\cdot\|_1$,
		$\|\cdot\|_2$,
		$\|\cdot\|_\infty$,
		depending on specific application
	\eit
\eit
\vfill


\myfoilhead{Line search methods for infeasible Newton's method}
\pagelabel{page:Line search methods for infeasible Newton's method}

\bit
\item
	line search methods for infeasible Newton's method,
	\ie,
		\algorithmname~\ref{algorithm:exact line search}\
		\&
		\algorithmname~\ref{algorithm:backtracking line search}\
	same with \fobj\ replaced by $\|r(x,\nu)\|_2$,

\vitem
	but they have special forms (of course)
	- refer to below special case descriptions
\eit


\begin{myalgorithm}{exact line search for infeasible Newton's method}\;%
	$$
		\slen = \argmin_{s>0} \|r(x +s\pdsdir, \nu + s\pdsdirnu)\|_2
	$$
\end{myalgorithm}

\begin{myalgorithm}{backtracking line search for infeasible Newton's method}\;%
	\begin{algorithmic}
	\Require \sdir, \sdirnu, $\alpha\in(0,0.5)$, $\beta\in(0,1)$
	\State $\slen:=1$
	\While{
		$\|r(x +\slen\pdsdir, \nu + \slen\pdsdirnu)\|_2 > (1-\alpha \slen)\|r(x,\nu)\|_2$
	}
	\State{$\slen := \beta \slen$}
	\EndWhile
	\end{algorithmic}
\end{myalgorithm}
\vfill


\myfoilhead{Pros and cons of infeasible Newton's method}


\bit
\item pros
	\bit
	\vitem
		do not need to find feasible point separately,
		\eg\
		\ibit
		\iitem
			``\onelineoptprob{minimize}{-\log(Ax) + b^Tx}{}''
		\eit
		can be solved by converting to
		\ibit
		\iitem
			``\onelineoptprob{minimize}{-\log(y) + b^Tx}{y=Ax}''
		\eit
		and solved by infeasible Newton's method

	\vitem
		if step length is one at any iteration,
		following steps coincides with feasible Newton's method
		- could switch to feasible Newton's method
	\eit

\vvitem cons
	\bit
	\vitem
		exists no clear way to detect feasibility - primal residual decreases slowly
		(phase I method in interior point method resolves this problem)

	\vitem
		convergence of infeasible Newton's method can be very slow
		(until feasibility is achieved0
	\eit
\eit
\vfill


\myfoilhead{Assumptions for convergence analysis of infeasible Newton's method for equality constrained minimization}
\pagelabel{page:conv-analysis-assumptions-infeasible-equality-Newton-method}

\bit
\item
	sublevel set $S = \bigset{(x,\nu)\in \dom \fobj\times \reals^m}{
		\|r(x,\nu)\|_2
			\leq
		\|r(\xseqk{0},\nuseqk{0})\|_2
	}$
	is closed,
	which always holds because $\|r\|_2$ is closed

\vitem
	boundedness of KKT matrix on $S$
	$$
		\left(
			\exists K >0
		\right)
		\left(
			\forall x \in S
		\right)
		\left(
			\left\|
				Dr(x,\nu)^{-1}
			\right\|_2
			=
			\left\|
			\mattwotwo{\nabla^2 \fobj(x)}{A^T}{A}{0}^{-1}
			\right\|_2
			\leq K
		\right)
	$$

\vitem
	Lipschitz continuity of Hessian on $S$
	$$
		\left(
			\exists L > 0
		\right)
		\left(
			\forall (x,\nu), (y,\mu)\in S
		\right)
		\left(
			\left\|Dr(x,\nu) - Dr(y,\mu)\right\|_2
			\leq
			L
			\|(x,\nu) - (y,\mu)\|_2
		\right)
	$$

\vitem
	above assumptions imply $\set{x\in\dom \fobj}{Ax=b}\neq\emptyset$
	and exist optimal point $(x^\ast,\nu^\ast)$
\eit
\vfill


\myfoilhead{Convergence analysis of infeasible Newton's method for equality constrained minimization}

\bit
\item
	very simliar to that for Newton's method for unconstrained minimization

\item
	consist of two phases - like unconstrained minimization or infeasible Newton's method (refer to
	\theoremname~\ref{theorem:convergence analysis of Newton's method}\
	or page~\pageref{page:Convergence analysis of feasible Newton's method for equality constrained minimization})
	\bit
	\item
		damped Newton phase
		-
		if $\|r(\xseqk{k},\nuseqk{k})\|_2> 1/(K^2L)$
		$$
			\|r(\xseqk{k+1},\nuseqk{k+1})\|_2
			\leq
			\|r(\xseqk{k},\nuseqk{k})\|_2
			- \alpha \beta / K^2L
		$$
	\item
		quadratic convergence
		damped Newton phase
		-
		if $\|r(\xseqk{k},\nuseqk{k})\|_2 \leq 1/(K^2L)$
		$$
			\left( K^2L \|r(\xseqk{k},\nuseqk{k})\|_2 / 2 \right)
			\leq
			\left( K^2L \|r(\xseqk{k-1},\nuseqk{k-1})\|_2 / 2 \right)^2
			\leq
			\cdots
			\leq (1/2)^{2^k}
		$$
	\eit
\item
	\# iterations of infeasible Newton's method required to satisfy $\|r(\xseqk{k},\nuseqk{k})\|_2\leq\epsilon$
	$$
		\|r(\xseqk{0},\nuseqk{0})\| /(\alpha \beta / K^2L)
		+ \log_2 \log_2 (\epsilon_0/\epsilon) \quad \mbox{where}\; \epsilon_0 = 2/(K^2L)
	$$

\item
	$(\xseqk{k},\nuseqk{k})$ converges to $(x^\ast,\nu^\ast)$
\eit
\vfill


\titlefoil{Barrier Interior-point Methods}{Barrier Interior-point Methods}

\myfoilhead{Interior-point methods}

\bit
\item
	want to solve inequality constrained minimization problem

\vitem
	interior-point methods solve convex optimization problem (\definitionname~\ref{definition:convex optimization})
	or KKT optimality conditions (\definitionname~\ref{definition:KKT optimality conditions})
	by
	\bit
	\vitem
		applying Newton's method to sequence of
		\bit
		\viitem
			equality constrained problems or
		\iitem
			modified versions of KKT optimality conditions
		\eit
	\eit

\vitem
	discuss interior-point \define{barrier method} \& interior-point \define{primal-dual method}

\vvitem
	hierarchy of convex optimization algorithms
	\bit
	\vitem
		simplest - linear equality constrained quadratic program - can solve analytically
	\vitem
		Newton's method - solve linear equality constrained convex optimization problem
		by solving sequence of linear equality constrained quadratic programs
	\vitem
		interior-point methods
		- solve linear equality \& convex inequality constrained problem
		by solving sequence of lienar equality constrained convex optimization problem
	\eit
\eit
\vfill


\myfoilhead{Indicator function barriers}

\bit
\item
	approxmiate general convex inequality constrained problem as linear equality constrained problem

\vitem
	make inequality constraints implicit in objective function
	$$
		\begin{array}{ll}
			\mbox{minimize} &
				\fobj(x) + \sum I_-(\fie(x))
			\\
			\mbox{subject to} &
				Ax=b
		\end{array}
	$$
	where $I_-:\reals\to \reals$ is indicator function for nonpositive real numbers, \ie\
	$$
		I_{-}(u) = \left\{\begin{array}{ll}
			0	 & u\leq 0
			\\
			\infty	 & u> 0
		\end{array}\right.
	$$
\eit
\vfill


\myfoilhead{Logarithmic barriers}

\bit
\item
	approximate indicator function by logarithmic function
	$$
		\hat{I}_- = -(1/t) \log(-u)
		\quad
		\dom \hat{I}_- = -\ppreals
	$$
	for $t>0$ to obtain
	$$
		\begin{array}{ll}
			\mbox{minimize} &
				\fobj(x) + \sum -(1/t) \log(-\fie(x))
			\\
			\mbox{subject to} &
				Ax=b
		\end{array}
	$$

\vitem
	objective function is convex due to composition rule for convexity preservation
	(page~\pageref{page:convexity preserving operation - composition}),
	and differentiable

\vitem
	hence, can use Newton's method to solve it

\vvitem
	function $\phi$ defined by
	$$
		\phi(x) = - \sum \log(-\fie(x))
	$$
	with $\dom \phi \set{x\in\xdomain}{\fie(x) \prec 0}$
	called \define{logarithmic barrier} or \define{log barrier}

\vitem
	solve sequence of log barrier problems as we increase $t$
\eit
\vfill


\myfoilhead{Central path}
\pagelabel{page:Central path}

\bit
\item
	optimization problem

	$$
		\begin{array}{ll}
			\mbox{minimize} &
				t \fobj(x) + \phi(x)
			\\
			\mbox{subject to} &
				Ax = b
		\end{array}
	$$

	with $t>0$
	where

	$$
		\phi(x) = - \sum \log(-\fie(x))
	$$

\vitem
	solution of above problem, called \define{central point}, denoted by \define{$x^\ast(t)$},
	set of central points, called \define{central path}

\vitem
	intuition says $x^\ast(t)$ will converge to $x^\ast$
	as $t\to\infty$

\vvitem KKT conditions imply

	$$
		Ax^\ast(t) = b \quad \fie(x^\ast(t)) \prec 0
%		\mbox{ - primal feasibility}
	$$

	and exists $\nu^\ast(t)$ such that

	\begin{eqnarray*}
		0
		&=&
		t \nabla \fobj(x^\ast(t)) + \nabla \phi(x^\ast(t)) + t A^T \nu^\ast(t)
		\\
		 &=&
		 t\nabla \fobj(x^\ast(t))
		 - \sum \frac{1}{\fie_i(x^\ast(t))} \nabla\fie_i(x^\ast(t))
		 + t A^T \nu^\ast(t)
	\end{eqnarray*}

\vitem
	thus if we let $\lambda^\ast(t) = -1/t\fie_i(x^\ast(t))$,
	$x^\ast(t)$ minimizes

	$$
		L(x,\lambda^\ast(t),\nu^\ast(t))
		= \fobj(x) + {\lambda^\ast(t)}^T \fie(x) + {\nu^\ast(t)}^T (Ax-b)
	$$

	where $L$ is Lagrangian of original problem in \definitionname~\ref{definition:convex optimization}

\vitem
	hence, dual function $g(\lambda^\ast(t),\nu^\ast(t))$ is finite
	and
%	$$
%		g(\lambda^\ast(t), \nu^\ast(t))
%		= \fobj(x^\ast(t)) + {\lambda^\ast(t)}^T \fie(x^\ast(t)) + {\nu^\ast(t)}^T (Ax^\ast(t)-b)
%		= \fobj(x^\ast(t)) - m/t
%	$$

	\begin{eqnarray*}
	\lefteqn{
		g(\lambda^\ast(t), \nu^\ast(t))
		=
		\inf_{x\in\xdomain} L(x,\lambda^\ast(t),\nu^\ast(t))
		=
		L(x^\ast(t),\lambda^\ast(t),\nu^\ast(t))
	}
	\\
	&
	=
	&
		\fobj(x^\ast(t)) + {\lambda^\ast(t)}^T \fie(x^\ast(t)) + {\nu^\ast(t)}^T (Ax^\ast(t)-b)
		= \fobj(x^\ast(t)) - m/t
	\end{eqnarray*}

	and

	$$
		\fobj(x^\ast(t)) - p^\ast \leq \fobj(x^\ast(t)) - g(\lambda^\ast(t), \nu^\ast(t))
		= m/t
	$$

\vitem []
	that is,

	\begin{quote}
	\begin{center}
%	$$
		\eemph{
		$x^\ast(t)$ is no more than $m/t$-suboptimal
		}
%	$$
	\end{center}
	\end{quote}

\vitem []
	which
	confirms out intuition that $x^\ast(t)\to x^\ast$ as $t\to\infty$
\eit
\vfill


\myfoilhead{Central path interpretation via KKT conditions}

\bit
\item
	previous arguments imply that $x$ is central point,
	\ie, $x=x^\ast(t)$ for some $t>0$
	\iaoi\
	exist $\lambda$ and $\nu$ such that
	\begin{eqnarray*}
		Ax=b
		\quad
		\fie({x})
			&\preceq&
		0
		\quad
		\mbox{- primal feasibility}
		\\
		\lambda
			&\succeq&
		0
		\quad
		\mbox{- dual feasibility}
		\\
		- {\lambda_i}^T \fie_i({x})
			&=&
		1/t
		\quad
		\mbox{- complementary $1/t$-slackness}
		\\
		\nabla_x L(x,\lambda,\nu)
			&=&
		0
		\quad
		\mbox{- vanishing gradient of Lagrangian}
	\end{eqnarray*}
	called \define{centrality conditions}

\vitem
	only difference between centrality conditions and KKT conditions in \definitionname~\ref{definition:KKT optimality conditions}
	is \emph{complementary $1/t$-slackness}
	\bit
	\vitem
		note that I've just made up term ``complementary $1/t$-slackness''
		- you won't be able to find terminology in any literature
	\eit

\vitem
	for large $t$, $\lambda^\ast(t)$ \& $\nu^\ast(t)$ \emph{very closely} satisfy (true) complementary slackness
\eit
\vfill


\myfoilhead{Central path interpretation via force field}

\bit
\item
	assume exist no equality constraints

\vitem
	interpret $\phi$ as potential energy by some force field, \eg, electrical field
	and $t\fobj$ as potential energy by some other force field, \eg, gravity

\vitem
	then
	\bit
	\vitem
		force by first force field (in $n$-dimensional space), which we call \emph{barrier force}, is
		$$
			- \nabla \phi(x) = \sum \frac{1}{\fie_i(x)} \nabla \fie_i(x)
		$$
	\vitem
		force by second force field, which we call \emph{objective force}, is
		$$
			- \nabla (t\fobj(x)) = -t \nabla \fobj(x)
		$$
	\eit

\vitem
	$x^\ast(t)$ is point where two forces exactly balance each other
	\bit
	\vitem
		as $x$ approach boundary, barrier force pushes $x$ harder from barriers,
	\vitem
		as $t$ increases, objective force pushes $x$ harder to point where objective potential energy is minimized
	\eit
\eit
\vfill


\myfoilhead{Equality constrained problem using log barrier}

\bit
\item
	central point $x^\ast(t)$ is $m/t$-suboptimal point
	guaranteed by optimality certificate $g(\lambda^\ast(t),\nu^\ast(t))$

\vitem
	hence solving below problem provides solution with $\epsilon$-suboptimality
	$$
		\begin{array}{ll}
			\mbox{minimize} &
				(m/\epsilon) \fobj(x) + \phi(x)
			\\
			\mbox{subject to} &
				Ax=b
		\end{array}
	$$

\vitem
	but works only for small problems
	since for large $m/\epsilon$,
	objective function ill behaves
\eit
\vfill


\myfoilhead{Barrier methods}

\begin{myalgorithm}{barrier method}\;%
\begin{algorithmic}
\Require strictly feasible $x$, $t>0$, $\mu>1$, tolerance $\epsilon>0$
\Repeat
	\State{%
		centering step -
		find $x^\ast(t)$
		by minimizing $t\fobj + \phi$ subject to $Ax=b$
		starting at $x$
	}
	\State{%
		(optionally)
		compute $\lambda^\ast(t)$ \& $\nu^\ast(t)$
	}
	\State{%
		stopping criterion - quit if $m/t<\epsilon$
	}
	\State{%
		increase $t$ - $t := \mu t$
	}
	\State{%
		update $x$ - $x := x^\ast(t)$
	}
\Until
\end{algorithmic}
\end{myalgorithm}
\bit
\item
	\define{barrier method}, also called \define{path-following method},
	solves sequence of equality constrained optimization problem with log barrier
	\bit
	\item
		when first proposed by Fiacco and McCormick in 1960s,
		it was called \define{sequential unconstrained minimization technique (SUMT)}
	\eit
\item
	\define{centering step} also called \define{outer iteration}
\item
	each iteration of algorithm used for equality constrained problem,
	called \define{inner iteration}
\eit


\myfoilhead{Accuracy in centering in barrier method}

\bit
\item
	accuracy of centering
	\bit
	\vitem
		only goal of centering is getting close to $x^\ast$,
		hence exact calculation of $x^\ast(t)$ not critical
		as long as approximates of $x^\ast(t)$ go to $x^\ast$

	\vitem
		while cannot calculate $g(\lambda,\nu)$ for this case,
		below provides dual feasible point
		when
		Newton step \ntsdir\ for optimization problem on page~\pageref{page:Central path}\ is small,
		\ie, for nearly centered
		$$
			\tilde{\lambda}_i
			= -\frac{1}{t\fie_i(x)}
				\left(
					1 - \frac{\nabla \fie_i(x)^T \ntsdir}{\fie_i(x)}
				\right)
		$$
	\eit
\eit


\myfoilhead{Choices of parameters of barrier method}

\bit
\vitem
	choice of $\mu$
	\bit
	\vitem
		$\mu$ determines aggressiveness of $t$-update
		\bit
		\viitem
			larger $\mu$, less outer iterations, but more inner iterations
		\viitem
			smaller $\mu$, less outer iterations, but more inner iterations
		\eit
	\vitem
		values from $10$ to $20$ for $\mu$
		seem to work well
	\eit

\vitem
	candidates for choice of initial $t$\
	- choose \seqk{t}{0}\ such that
	\bit
	\vitem []
		$$
			m / \seqk{t}{0} \approx \fobj(\xseqk{0}) - p^\ast
		$$
	\vitem [or]
		make central path condition on page~\pageref{page:Central path}\ maximally satisfied
		$$
			\seqk{t}{0}
			= \arginf_{t}
				\inf_{\tilde{\nu}}
				\left\|
					t \nabla \fobj(\xseqk{0}) + \nabla \phi(\xseqk{0}) + A^T \tilde{\nu}
				\right\|
		$$
	\eit
\eit
\vfill


\myfoilhead{Convergence analysis of barrier method}

\bit
\item
	assuming $t\fobj + \phi$
	can be minimized by Newton's method
	for
	\seqk{t}{0},
	$\mu\seqk{t}{0}$,
	$\mu^2\seqk{t}{0}$,
	\ldots

\vvitem
	at $k$'th step, duality gap achieved is $m/\mu^k\seqk{t}{0}$

\vvitem
	\# centering steps required to achieve accuracy of $\epsilon$ is
	$$
		\left\lceil
		\frac{\log \left(m/\epsilon \seqk{t}{0}\right)}{\log \mu}
		\right\rceil
	$$
	plus one (initial centering step)

\vvitem
	for convergence of centering
	\bit
	\vitem
		for feasible centering problem,
		$t\fobj + \phi$ should satisfy conditions on page~\pageref{page:conv-analysis-assumptions-feasible-equality-Newton-method},
		\ie,
		initial sublevel set is closed,
		associated inverse KKT matrix is bounded
		\& Hessian satisfies Lipschitz condition
	\vitem
		for infeasible centering problem,
		$t\fobj + \phi$ should satisfy conditions on page~\pageref{page:conv-analysis-assumptions-infeasible-equality-Newton-method}\
	\eit
\eit


\yesnoexec{\showincomplete}{%
\myfoilhead{Newton step for modified KKT equations}
}{}

\yesnoexec{\showincomplete}{%
\myfoilhead{Feasiblity and phase I methods}
}{}

\yesnoexec{\showincomplete}{%
\myfoilhead{Termination near phase II central path}
}{}

\yesnoexec{\showincomplete}{%
\myfoilhead{Phase I via infeasible start Newton method}
}{}

\yesnoexec{\showincomplete}{%
\myfoilhead{Complexity analysis via self-concordance}
}{}

\yesnoexec{\showincomplete}{%
\myfoilhead{Combined phase I/phase II complexity}
}{}

\yesnoexec{\showincomplete}{%
\myfoilhead{X}
}{}

\yesnoexec{\showincomplete}{%
\titlefoil{Barrier Method for Generalized Inequalities}{Barrier Method for Generalized Inequalities}
}{}

\titlefoil{Primal-dual Interior-point Methods}{Primal-dual Interior-point Methods}

\myfoilhead{Primal-dual \& barrier interior-point methods}

\bit
\item
	in primal-dual interior-point methods
	\bit
	\item
		both primal and dual variables are updated at each iteration

	\vitem
		search directions are obtained from Newton's method,
		applied to modified KKT equations,
		\ie,
		optimality conditions for logarithmic barrier centering problem

	\vitem
		primal-dual search directions are similar to, but not quite the same as,
		search directions arising in barrier methods

	\vitem
		primal and dual iterates are {not} necessarily feasible
	\eit

\vitem
	primal-dual interior-point methods
	\bit
	\item
		often more efficient than barrier methods
		especially when high accuracy is required
		- can exhibit better than linear convergence

	\vitem
		(customized versions) outperform barrier method
		for several basic problems, such as, LP, QP, SOCP, GP, SDP

	\vitem
		can work for feasible, but \emph{not} strictly feasible problems

	\vitem
		still active research topic, but show great promise
	\eit
\eit
\vfill


\myfoilhead{Modified KKT conditions and central points}

\bit
\item
	modified KKT conditions (for convex optimization in \definitionname~\ref{definition:convex optimization}) expressed as
	$$
		r_t(x,\lambda,\nu)
		=
		\colvecthree
			{\nabla \fobj(x) + D\fie(x)^T\lambda + A^T\nu}
			{-\diag(\lambda)\fobj(x) - (1/t) \ones}
			{Ax-b}
	$$
	where
	\begin{eqnarray*}
		\mbox{\define{dual residual}}
		&-&
			r_\mathrm{dual}(x,\lambda,\nu)
			= {\nabla \fobj(x) + D\fie(x)^T\lambda + A^T\nu}
		\\
		\mbox{\define{centrality residual}}
		&-&
			r_\mathrm{cent}(x,\lambda,\nu)
			= {-\diag(\lambda)\fobj(x) - (1/t) \ones}
		\\
		\mbox{\define{primal residual}}
		&-&
			r_\mathrm{pri}(x,\lambda,\nu)
			= {Ax-b}
	\end{eqnarray*}

\vitem
	if $x$, $\lambda$, $\nu$ satisfy $r_t(x,\lambda,\nu)=0$ (and $\fie(x) \prec 0$),
	then
	\bit
	\vitem
		$x=x^\ast(t)$, $\lambda=\lambda^\ast(t)$, $\nu=\nu^\ast(t)$
	\vitem
		$x$ is primal feasible and $\lambda$ \& $\nu$ are dual feasible
		with duality gap $m/t$
	\eit
\eit
\vfill


\myfoilhead{Primal-dual search direction}

\bit
\item
	assume current (primal-dual) point $y=(x,\lambda,\nu)$
	and Newton step $\sdiry =( \sdir, \sdirnu, \sdirlbd)$

\vitem
	as before, linearize equation to obtain Newton step,
	\ie,
	$$
		r_t(y+\sdiry) \approx r_t(y) + Dr_t(y) \sdiry = 0
		\quad
		\Leftrightarrow
		\quad
		\sdiry = -Dr_t(y)^{-1} r_t(y)
	$$
	hence
	$$
		\begin{my-matrix}{ccc}
			\nabla^2 f(x) + \sum \lambda_i \nabla^2 \fie_i(x)
			&
			D\fie(x)^T
			&
			A^T
		\\
			-\diag(\lambda) D\fobj(x)
			&
			-\diag(\fobj(x))
			&
			0
		\\
			A
			&
			0
			&
			0
		\end{my-matrix}
		\colvecthree{\sdir}{\sdirlbd}{\sdirnu}
		=
		- \colvecthree
%		{r_\mathrm{dual}(x,\lambda,\nu)}
%		{r_\mathrm{cent}(x,\lambda,\nu)}
%		{r_\mathrm{pri}(x,\lambda,\nu)}
		{r_\mathrm{dual}}
		{r_\mathrm{cent}}
		{r_\mathrm{pri}}
	$$

\vitem
	above equation determines
	\define{primal-dual search direction} $\pdsdiry = (\pdsdir, \pdsdirlbd, \pdsdirnu)$
\eit
\vfill


\myfoilhead{Surrogate duality gap}

\bit
\item
	iterates \xseqk{k}, \lbdseqk{k}, and \nuseqk{k}\ of primal-dual interior-point method
	are \emph{not} necessarily feasible

\vitem
	hence, cannot easily evaluate duality gap \seqk{\eta}{k}\
	as for barrier method

\vitem
	define \define{surrogate duality gap}
	for $\fie(x) \prec 0$ and $\lambda\succeq0$
	as
	$$
		\hat{\eta}(x,\lambda) = - \fie(x)^T \lambda
	$$

\vitem
	$\hat{\eta}$ would be duality gap if $x$ were primal feasible and $\lambda$ \& $\nu$ were dual feasible

\vitem
	value $t$ corresponding to surrogate duality gap $\hat{\eta}$ is $m/\hat{\eta}$
\eit
\vfill


\myfoilhead{Primal-dual interior-point method}

\begin{myalgorithm}{primal-dual interior-point method}\;%
\begin{algorithmic}
\Require initial point $x$ with $\fie(x)\prec0$, $\lambda \succ 0$, $\mu > 1$,
	$\epsilon_\mathrm{pri}>0$, $\epsilon_\mathrm{dual}>0$, $\epsilon>0$
\Repeat
	\State{%
		set $t := \mu m /\hat{\eta}$
	}
	\State{%
		computer primal-dual search direction $\pdsdiry = (\pdsdir, \pdsdirlbd, \pdsdirnu)$
	}
	\State{do line search to choose $s>0$}
	\State{update
		-
		$x := x + s \pdsdir$,
		$\lambda := \lambda + s \pdsdirnu$,
		$\nu := \nu + s \pdsdirnu$
	}
\Until{
	$\|r_\mathrm{pri}(x,\lambda,\nu)\|_2\leq \epsilon_\mathrm{pri}$,
	$\|r_\mathrm{dual}(x,\lambda,\nu)\|_2\leq \epsilon_\mathrm{dual}$,
	$\hat{\eta} \leq \epsilon$
	}
\end{algorithmic}
\end{myalgorithm}

\bit
\item
	common to choose small
	$\epsilon_\mathrm{pri}$, $\epsilon_\mathrm{dual}$, \& $\epsilon$
	since primal-dual method often shows faster than linear convergence
\eit
\vfill


\myfoilhead{Line search for primal-dual interior-point method}

\bit
\item
	liner search is standard backtracking line search on $r(x,\lambda,\nu)$
	similar to that in \algorithmname~\ref{algorithm:exact line search for infeasible Newton's method}
	except making sure that $\fie(x) \prec 0$ and $\lambda\succ0$

\vitem
	note initial $s$
	in \algorithmname~\ref{algorithm:backtracking line search for primal-dual interior-point method}
	is largest $s$ that makes $\lambda + s\pdsdirlbd$ positive
\eit

\begin{myalgorithm}{backtracking line search for primal-dual interior-point method}\;%
	\begin{algorithmic}
	\Require \pdsdir, \pdsdirlbd, \pdsdirnu, $\alpha\in(0.01,0.1)$, $\beta\in(0.3,0.8)$
	\State $s:= 0.99\sup\set{s\in[0,1]}{\lambda + s \sdirlbd \succeq 0} = 0.99\min\{1,\min\set{-\lambda_i/\sdirlbd_i}{\sdirlbd_i < 0}\}$
	\While{
		$\fie (x +s\pdsdir) \not \prec 0$
	}
	\State{$t := \beta t$}
	\EndWhile
	\While{
		$\|r(x +s\pdsdir, \lambda + s\pdsdirlbd, \nu + s\pdsdirnu)\|_2 > (1-\alpha s)\|r(x,\lambda,\nu)\|_2$
	}
	\State{$t := \beta t$}
	\EndWhile
	\end{algorithmic}
\end{myalgorithm}

%page~\pageref{page:Line search methods for infeasible Newton's method}
%\ref{algorithm:backtracking line search}
%\ref{algorithm:backtracking line search for infeasible Newton's method}

\vfill

\yesnoexec{\showincomplete}{%
\myfoilhead{X}
}{}



%$\lambda$

}{}


%\TITLEFOIL{Thank You}{thank-you}

\ifthenelse{\equal{\value{numsectionsforwhichproofexists}}{0}}{}{%
\yesnoexec{\sproof}{%

\TITLEFOIL{Selected Proofs}{Proofs}

\labelfoilhead{Selected proofs}

\bit
\yesnoexec{\aalgebra}{%
\begin{myproof}{relation among coset indices}
	Let $\{h_1,\ldots,h_n\}$ and $\{k_1,\ldots,k_m\}$
	be coset representations of $H$ in $G$ and $K$ in $H$
	respectively.
	Then $n=(G:H)$ and $m=(H:K)$.
	Note that $\bigcup_{i,j} h_i k_j K = \bigcup_i h_i H = G$,
	and if $h_ik_j K= h_k k_lK$ for some $1\leq i,k \leq n$ and $1\leq j,k\leq m$,
	$h_ik_j KH= h_k k_lKH \Leftrightarrow h_ik_j H= h_k k_lH \Leftrightarrow h_i H = h_j H \Leftrightarrow h_i=h_j$,
	thus $k_j K = k_l K$, hence $k_j=k_l$.
	Thus $\set{h_ik_j}{1\leq i\leq n, 1\leq j\leq m}$
	is cosets representations of $K$ in $G$,
	therefore $(G:K) = mn = (G:H)(H:K)$.
	\qed\
\end{myproof}

\begin{myproof}{normality and commutativity of commutator subgroups}
	\bit
	\item
		For $a, x, y\in G$,
		\begin{eqnarray*}
			\lefteqn{
			a xy x^{-1}y^{-1}
			=
			a x(a^{-1}x^{-1}x a) y x^{-1}y^{-1}(a^{-1} a)
			}
			\\
			&=&
			( axa^{-1}x^{-1})(x (a y) x^{-1}(ay)^{-1}) a
		\end{eqnarray*}
		and
		\begin{eqnarray*}
			\lefteqn{
			xy x^{-1}y^{-1} a
			=
			(a a^{-1}) xy x^{-1} (a y^{-1} y a^{-1}) y^{-1} a
			}
			\\
			&=&
			a ((a^{-1} x)y (a^{-1}x)^{-1} y^{-1}) (y a^{-1} y^{-1} a),
		\end{eqnarray*}
		hence commutator subgroup of $G$
		propagate every element of $G$ from fron to back and vice versa.
		Therefore for every $a\in G$, $aG^C=G^Ca$.

	\item
		For $x,y\in G$,
		$x G^C yG^C = xy G^C = G^C xy = (G^C x)(G^Cy)$, hence $G/G^C$ is commutative.

	\item
		For a homeomorphism of $G$, $f$, into a commutative group, and $x,y\in G$,
		$$
			f(xyx^{-1}y^{-1})
			=
			f(x)f(y)f(x^{-1})f(y^{-1})
			=
			f(x)f(x^{-1})f(y)f(y^{-1})
			=e
		$$
		thus $xyx^{-1}y^{-1}\in \Ker f$,
		hence $G^C \subset \Ker f$.
	\eit
	\qed\
\end{myproof}

\begin{myproof}{set of functions into ring is ring}
\bit
\item
	First, we show that the mapping addition defines a commutative additive group in $\Map(S,A)$.
	The addition is associative because $A$ is a ring,
	hence defines an additive (abelian) group,
	thus, monoids
	(\definitionname~\ref{definition:monoids} \& \definitionname~\ref{definition:group}),
	\ie,
	\begin{eqnarray*}
		\lefteqn{
		\left(
			\forall f,g,h \in \Map(S,A)
		\right)
		}
		\\
		&
		\left(
			\forall x \in S
		\right)
		(&
			((f+g)+h)(x)
			= (f(x)+g(x)) + h(x)
		\\
		&&
			= f(x)+ (g(x) + h(x))
			= (f+ (g + h))(x)
		)
		\\
		&\Rightarrow&
		(f+g)+h = f+(g+h).
	\end{eqnarray*}
	Thus, the mapping addition defines an additive monoid in $\Map(S,A)$
	with the zero mapping whose value is the additive unit element of $A$
	as the additive unit element of $\Map(S,A)$
	(\definitionname~\ref{definition:monoids}).
	Now for every $f\in R$,
	a mapping $g\in R$ defined by $x \mapsto -f(x)$
	satisfies $f+g = g+f=0$,
	hence is the inverse of $f$.
	Therefore the additive monoid is a group (\definitionname~\ref{definition:group}).
	We further note that
	the addition is commutative because the additive group of $A$ is abelian
	(\definitionname~\ref{definition:ring}),
	\ie,
	\begin{eqnarray*}
		\lefteqn{
		\left(
			\forall f,g \in S
		\right)
		}
		\\
		&
		\left(
			\forall x \in M
		\right)
		(&
			(g+f)(x) = g(x) + f(x) = f(x) + g(x) = (f+g)(x)
		)
		\\
		&\Rightarrow&
		f+g = g+f.
	\end{eqnarray*}
	Therefore, the mapping addition defines a commutative additive group in $\End(M)$.

\item
	The mapping multiplication is associative because
	$A$ is ring,
	hence
	defines a multiplicative monoid,
	\ie,
	\begin{eqnarray*}
		\lefteqn{
		\left(
			\forall f,g,h \in \Map(S,A)
		\right)
		}
		\\
		&
		\left(
			\forall x \in S
		\right)
		(&
			((fg)h)(x)
			= (fg)(x) h(x)
			= (f(x)g(x)) h(x)
		\\
		&&
			= f(x) (g(x) h(x))
			= f(x) (gh)(x)
			= (f (g h))(x)
		)
		\\
		&\Rightarrow&
		(fg)h = f(gh).
	\end{eqnarray*}
	Thus, the mapping multiplication defines a multiplicative monoid in $\Map(S,A)$
	with the mapping whose value is the multiplicative unit element of $A$
	as the multiplicative unit element
	(\definitionname~\ref{definition:monoids}).

\item
	Now we show that the multiplication is distributive over addition in $\Map(S,A)$.
	Similary this is due to that the multiplication is distributive over addition in $A$.
	Note that
	\begin{eqnarray*}
		\lefteqn{
		\left(
			\forall f,g,h \in \Map(S,A)
		\right)
		}
		\\
		&
		\left(
			\forall x \in S
		\right)
		(&
			(f (g+h))(x)
			= f(x)(g+h)(x)
			= f(x)(g(x) + h(x))
		\\&&
			= f(x) g(x) + f(x) h(x)
			= (fg)(x) + (fh)(x)
		)
		\\
		& \Rightarrow&
		f (g+h) = fg + fh.
	\end{eqnarray*}
	We can similarly show that
	$$
		\left(
			\forall f,g,h \in \Map(S,A)
		\right)
		\left(
		(f+g) h = f h + g h
		\right).
	$$
\eit
	Therefore
	$\Map(S,A)$
	is
	is ring
	(\definitionname~\ref{definition:ring}).
	\qed\
\end{myproof}

\begin{myproof}{set of group endomorphisms is ring}
\bit
\item
	First, we show that the addition defines a commutative additive group in $\End(M)$.
	The addition is associative because $M$ is group, hence, monoids
	(\definitionname~\ref{definition:monoids} \& \definitionname~\ref{definition:group}),
	\ie,
	\begin{eqnarray*}
		\lefteqn{
		\left(
			\forall f,g,h \in \End(M)
		\right)
		}
		\\
		&
		\left(
			\forall x \in M
		\right)
		(&
			((f+g)+h)(x)
			= (f(x)+g(x)) + h(x)
		\\
		&&
			= f(x)+ (g(x) + h(x))
			= (f+ (g + h))(x)
		)
		\\
		&\Rightarrow&
		(f+g)+h = f+(g+h).
	\end{eqnarray*}
	Thus, the addition defines an additive monoid in $\End(M)$
	with the zero mapping whose values is the unit element of $M$
	as the additive unit element
	(\definitionname~\ref{definition:monoids}).
	Now for every $f\in \End(M)$,
	a mapping $g\in\End(M)$ defined by $x \mapsto -f(x)$
	satisfies $f+g = g+f=0$,
	hence is the inverse of $f$.
	Therefore the addition defines the additive group in $\End(M)$ (\definitionname~\ref{definition:group}).
	We further note that
	the addition is commutative because $M$ is abelian,
	\ie,
	\begin{eqnarray*}
		\lefteqn{
		\left(
			\forall f,g \in \End(M)
		\right)
		\left(
			\forall x \in M
		\right)
		}
		\\
		&&
		\left(
			(g+f)(x) = g(x) + f(x) = f(x) + g(x) = (f+g)(x)
		\right).
	\end{eqnarray*}
	Therefore, the addition defines a commutative additive group in $\End(M)$.

\item
	The multiplication is associative because
	the mapping composition is an associative operation,
	\ie,
	$
		\left(
			\forall f,g,h \in \End(M)
		\right)
		\left(
			(f\circ g)\circ h
			= f\circ (g\circ h)
		\right),
	$
	hence, the mapping composition defines a multiplicative monoid in $\End(M)$
	with the identity mapping as the multiplicative unit element
	(\definitionname~\ref{definition:monoids}).

\item
	Now we show that the multiplication is distributive over addition.
	Note that
	\begin{eqnarray*}
		\lefteqn{
		\left(
			\forall f,g,h \in \End(M)
		\right)
		}
		\\
		&
		\left(
			\forall x \in M
		\right)
		(&
			(f\circ(g+h))(x) = f(g(x) + h(x))
		\\&&
			= (f\circ g)(x) + (f\circ h)(x)
		)
		\\
		& \Rightarrow&
		f\circ(g+h) = (f\circ g) + (f\circ h).
	\end{eqnarray*}
	We can similarly show that
	$$
		\left(
			\forall f,g,h \in \End(M)
		\right)
		\left(
		(f+g)\circ h = (f\circ h) + (g\circ h)
		\right).
	$$
\eit
	Therefore
	for abelian group $M$,
	\cemph{set $\End(M)$ of group homeomorphisms of $M$ into itself}
	is ring
	(\definitionname~\ref{definition:ring}).
	\qed\
\end{myproof}

\begin{myproof}{nonzero ideals of integers are principal}
	Suppose \ideal{a}\ is a nonzero ideal of \integers.
	Because if negative integer, $n$, is in \ideal{a},
	$-n$ is also in \ideal{a}
	because \ideal{a} is an additive group in the ring, \integers.
	Thus, \ideal{a}\ has at least one positive integer.
	By \principlename~\ref{principle:well ordering principle},
	there exists the smallest positive integer in \ideal{a}.
	Let $n$ be that integer.
	Let $m\in\ideal{a}$.
	By \theoremname~\ref{theorem:Euclidean algorithm},
	there exist $q, r \in \integers$
	such that $m=qn + r$ with $0\leq r < n$.
	Since by the definition of ideals of rings (\definitionname~\ref{definition:ideal})
	\ideal{a}\ is an additive group in \integers,
	hence $m-qn = r$ is also in \ideal{a},
	thus $r$ should be $0$
	because we assume $n$ is the smallest positive integer in \ideal{a}.
	Thus $\ideal{a} = \set{qn}{q\in\integers} = n\integers$.
	Therefore the ideal is either $\{0\}$ or $n\integers$ for some $n>0$.
	Both $\{0\}$ and $n\integers$ are ideal.
	\qed\
\end{myproof}

\begin{myproof}{ideal generated by elements of ring}
	For all $x\in (a_1,\ldots,a_n)$, and $y\in A$
	$yx = y\left(\sum x_i a_i\right) = \sum (yx_i)a_i$ for some $\seqscr{x_i}{i=1}{n} \subset A$,
	hence $yx\in A$,
	and $(a_1,\ldots, a_n)$ is additive group,
	thus is ideal of $A$,
	hence
	$$
		\bigcap_{\ideal{a}: \mathrm{ideal\ containing\ }a_1,\ldots,a_n} \ideal{a}
		\subset (a_1,\ldots,a_n)
	$$
	Conversely, if \ideal{a}\ contains $a_1,\ldots,a_n$,
	$A a_i \subset \ideal{a}$,
	hence for every sequence, $\seqscr{x_i}{i=1}{n}\subset A$,
	$\sum x_i a_i\subset \ideal{a}$ because \ideal{a}\ is additive subgroup of $A$,
	thus $(a_1,\ldots,a_n)$ is contained in every ideal containing $a_1$, \ldots, $a_n$,
	hence
	$$
		(a_1,\ldots,a_n)
		\subset
		\bigcap_{\ideal{a}: \mathrm{ideal\ containing\ }a_1,\ldots,a_n} \ideal{a}
	$$
	\qed\
\end{myproof}

\begin{myproof}{kernel of ring-homeomorphism is ideal}
	Let $\Ker{f}$ be the kernel of a ring homeomorphism $f:A\to B$.
	Then \definitionname~\ref{definition:ring-homeomorphism} implies
	\[
		\left(
			\forall a, b \in \Ker{f}
		\right)
		\left(
			f(a+b) = f(a) + f(b) = 0 + 0 = 0
			\Rightarrow
			a+b\in\Ker{f}
		\right)
	\]
	hence, $\Ker{f}$ is closed under addition.
	Also \definitionname~\ref{definition:ring-homeomorphism} implies
	\begin{eqnarray*}
		\lefteqn{
			\left(
				\forall a \in \Ker{f}
			\right)
		}
		\\
		&&
		\left(
			f(-a) = f((-1)a) = f(-1)f(a) = f(-1) 0 = 0
			\Rightarrow
			-a\in\Ker{f}
		\right)
	\end{eqnarray*}
	hence, every element of $\Ker{f}$ has its inverse.
	Also
	$0\in\Ker{f}$ because $f(0)=0$ by \definitionname~\ref{definition:ring-homeomorphism}.
	Thus, $\Ker{f}$ is a subgroup of $A$ as additive group.
	\definitionname~\ref{definition:ring-homeomorphism} also implies
	\begin{eqnarray*}
		\lefteqn{
		\left(
			\forall a \in A, x \in \Ker{f}
		\right)
		}
		\\
		&&
		\left(
			f(ax) = f(a) f(x) = f(a) 0 = 0
			\;\& \;
			f(xa) = f(x) f(a) = 0 f(a) = 0
		\right)
	\end{eqnarray*}
	hence,
	$\Ker{f}$ is a two-side ideal, \ie, an ideal.
	\qed
%\definitionname~\ref{definition:groups}
%\definitionname~\ref{definition:ideal}
\end{myproof}

\begin{myproof}{image of ring-homeomorphism is subring}
	Let $f:A\to B$ be a ring-homeomorphism for two rings $A$ and $B$.
	\bit
	\item
		Then for any $z,w \in f(A)$, there exist $x,y\in A$ such that $f(x) = z$ and $f(y) = w$,
		hence \definitionname~\ref{definition:ring-homeomorphism} implies
		$$
			z + w = f(x) + f(y) = f(x+y) \in f(A)
		$$
		because $x+y\in A$,
		hence $f(A)$ is closed under addition.
		Because $0\in A$, \definitionname~\ref{definition:ring-homeomorphism} implies $0=f(0)\in f(A)$,
		hence $f(A)$ contains the additive unit element.
		Also, for every $z\in f(A)$, there exist $x\in A$ such that $f(x)=z$,
		but there exists $-x \in A$ because a ring is a commutative group \wrt\ addition (\definitionname~\ref{definition:ring})
		thus, $f(-x) \in f(A)$,
		hence \definitionname~\ref{definition:ring-homeomorphism} implies
		$$
			f(-x) + z = f(-x) + f(x) = f(-x + x) = f(0) = 0
		$$
		and
		the additive inverse of $z$, which is $f(-x)$, is in $f(A)$.
		Therefore $f(A)$ is an additive group.
		Lastly for any $z,w \in f(A)$, there exist $x,y\in A$ such that
		$f(x) = z$ and $f(y) = w$,
		hence
		\definitionname~\ref{definition:ring} implies
		$$
			z + w = f(x) + f(y) = f(x+y) = f(y+x) = f(y) + f(x) = w + z,
		$$
		thus,
		\begin{equation}
		\label{eq:bhsl-1}
			\mbox{
			$f(A)\subset B$ is a commutative group \wrt\ addition.
			}
		\end{equation}
	\item
		Then for any $z,w \in f(A)$, there exist $x,y\in A$ such that $f(x) = z$ and $f(y) = w$,
		hence \definitionname~\ref{definition:ring-homeomorphism} implies
		$$
			z w = f(x) f(y) = f(xy) \in f(A)
		$$
		because $xy\in A$,
		hence $f(A)$ is closed under multiplication.
		Because $1\in A$, \definitionname~\ref{definition:ring-homeomorphism}
		implies $1 = f(1)\in f(A)$,
		hence $f(A)$ contains the multiplicative unit element,
		thus,
		\begin{equation}
		\label{eq:bhsl-2}
			\mbox{
				$f(A)\subset B$ is a monoid \wrt\ multiplication.
			}
		\end{equation}
	\eit
	Therefore $f(A)\subset B$ is a subring of $B$ by (\ref{eq:bhsl-1}) and (\ref{eq:bhsl-2}).
	\qed
\end{myproof}

\begin{myproof}{algebraicness of smallest subfields}
	\propositionname~\ref{proposition:algebraicness of finitely generated subfield by single element}
	implies that
	$k(\alpha_1) = k[\alpha_1]$ and $[k(\alpha_1):k] = \deg \Irr(\alpha_1, k, X)$.
	Because $\alpha_2$ is algebraic over $k$, hence algebraic over $k(\alpha_1)$ \emph{a fortiori},
	thus, the same proposition implies
	$$
		k(\alpha_1, \alpha_2)
		= (k(\alpha_1))[\alpha_2]
		= (k[\alpha_1])[\alpha_2]
		= k[\alpha_1, \alpha_2]
	$$
	and
	$$
		[k(\alpha_1,\alpha_2):k(\alpha_1)] = \deg \Irr(\alpha_2,k(\alpha_1),X)
	$$
	hence \propositionname~\ref{proposition:dimension of finite extension} implies
	\begin{eqnarray*}
		\lefteqn{
		[k(\alpha_1,\alpha_2):k]
		= [k(\alpha_1,\alpha_2):k(\alpha_1)] [k(\alpha_1):k]
		}
		\\
		&=&
			\deg \Irr(\alpha_1,k,X) \deg \Irr(\alpha_2,k(\alpha_1),X).
	\end{eqnarray*}
	Using the mathematical induction,
	it is straightforward to show
	that
	$$
		k(\alpha_1, \ldots, \alpha_n) = k[\alpha_1, \ldots, \alpha_n]
	$$
	and
	\begin{eqnarray*}
%		\lefteqn{
			[k(\alpha_1,\ldots,\alpha_n):k] &=& \deg \Irr(\alpha_1,k,X) \deg \Irr(\alpha_2,k(\alpha_1),X)
%		}
		\\
		&&
			\cdots \deg \Irr(\alpha_n, k(\alpha_1,\ldots,\alpha_{n-1}), X),
	\end{eqnarray*}
	thus \propositionname~\ref{proposition:algebraicness of finite field extensions}
	implies that $k(\alpha_1,\ldots,\alpha_n)$ is finitely algebraic over $k$.
	\qed\
\end{myproof}

\begin{myproof}{finite generation of compositum}
	First, it is obvious that $E = k(\alpha_1, \ldots, \alpha_n) \subset F(\alpha_1, \ldots, \alpha_n)$
	and $F\subset F(\alpha_1, \ldots, \alpha_n)$,
	hence $EF\subset F(\alpha_1, \ldots, \alpha_n)$ because $EF$
	is defined to be the smallest subfield that contains both $E$ and $F$.
	Now every subfield containing both $E$ and $F$
	contains all $f(\alpha_1,\ldots,\alpha_n)$ where $f\in F[X]$,
	hence all $f(\alpha_1,\ldots,\alpha_n) / g(\alpha_1,\ldots,\alpha_n)$
	where $f,g\in F[X]$ and $g(\alpha_1,\ldots,\alpha_n)\neq0$.
	Thus, $F(\alpha_1, \ldots, \alpha_n)\subset EF$ again by definition.
	Therefore $EF = F(\alpha_1, \ldots, \alpha_n)$.
	\qed\
\end{myproof}

\begin{myproof}{existence of algebraically closed algebraic extensions}
	\theoremname~\ref{theorem:existence of algebraically closed field extensions}\
	implies there exists an algebraically closed extension of $k$.
	Let $E$ be such one.
	Let $K$ be union of all algebraic extensions of $k$ contained in $E$,
	then $K$ is algebraic over $k$.
	Since $k$ is algebraic over itself, $K$ is not empty.
	Let $f\in K[X]$ with $\deg f\geq1$.
	If $\alpha$ is a root of $f$, $\alpha \in E$.
	Since $K(\alpha)$ is algebraic over $K$ and $K$ is algebraic over $k$,
	$K(\alpha)$ is algebraic over $k$ by \propositionname~\ref{proposition:algebraic and finite extensions are distinguished}.
	Therefore $K(\alpha)\subset K$ and $\alpha \in K$.
	Thus, $K$ is algebraically closed algebraic extension of $k$.
	\qed\
\end{myproof}

\begin{myproof}{theorem - Galois subgroups associated with intermediate fields}
	Suppsoe $\alpha\in K^G$
	and let $\sigma:k(\alpha) \to \algclosure{K}$
	be an embedding inducing the identity on $k$.
	If we let $\tau:K\to\algclosure{K}$ extend $\sigma$,
	$\tau$ is automorphism
	by normality of $K/k$ (\definitionname~\ref{definition:normal extensions}),
	hence $\tau\in G$, thus $\tau$ fixed $\alpha$,
	which means $\sigma$ is the identity, which is the only embedding extension
	of the identity embedding of $k$ onto itself to $k(\alpha)$,
	thus, by \definitionname~\ref{definition:separable degree of field extensions},
	$$
		[k(\alpha):k]_s = 1.
	$$
	Since $K$ is separable over $k$, $\alpha$ is separable over $k$ (by \theoremname~\ref{theorem:finite separable field extensions}),
	and $k(\alpha)$ is separable over $k$ (by \definitionname~\ref{definition:separable algebraic elements}),
	thus $[k(\alpha):k] = [k(\alpha):k]_s = 1$, hence $k(\alpha)=k$, thus $\alpha\in k$,
	hence
	$$
		K^G \subset k.
	$$
	Since by definition, $k\subset K^G$, we have $K^G = k$.

	Now since $K/k$ is a normal extension, $K/F$ is also a normal extension
	(by \theoremname~\ref{theorem:retention of normality of extensions}).
	Also, since $K/k$ is a separable extension,
	$K/F$ is also separable extension
	(by \theoremname~\ref{theorem:separable extensions are distinguished}\ and \definitionname~\ref{definition:distinguished class of field extensions}).
	Thus, $K/F$ is Galois (by \definitionname~\ref{definition:Galois extensions}).

	Now let $F$ and $F'$ be two intermediate fields. Since $K^{G(K/k)}= k$,
	we have $K^{G(K/F)}=F$ and $K^{G(K/F')}=F'$,
	thus if ${G(K/F)}={G(K/F')}$,
	$F=F'$,
	hence the map is injective.
	\qed
\end{myproof}

\begin{myproof}{Galois subgroups associated with intermediate fields - 1}
	First, $K/F_1$ and $K/F_2$ are Galois extensions
	by \theoremname~\ref{theorem:Galois subgroups associated with intermediate fields - 1},
	hence $G(K/F_1)$ and $G(K/F_2)$ can be defined.
	Also, \theoremname~\ref{theorem:retention of normality of extensions}\
	and \theoremname~\ref{theorem:separable extensions are distinguished}\
	imply that $K/F_1F_2$ is Galois extension,
	hence $G(K/F_1F_2)$ can be defined, too.

	Every automorphism of $G$ leaving both $F_1$ and $F_2$ leaves $F_1F_2$ fixed,
	hence $G(K/F_1)\cap G(K/F_2)\subset G(K/F_1F_2)$.
	Conversely,
	every automorphism of $G$ leaving $F_1F_2$ fxied
	leaves both $F_1$ and $F_2$ fixed,
	hence $G(K/F_1F_2)\subset G(K/F_1) \cap G(K/F_2)$.

	Now we can do the same thing using rather mathematically rigorous terms.
	Assume that $\sigma \in G(K/F_1) \cap G(K/F_2)$.
	Then
	$$
		\left(
			\forall x\in F_1, y\in F_2
		\right)
		\left(
			x^\sigma = x
			\ \& \ %
			y^\sigma = y
		\right),
	$$
	thus
	\begin{eqnarray*}
	\lefteqn{
		\left(
			\forall n,m\in \naturals
		\right)
	}
	\\
	&&
		\left(
			\forall
			x_1,\ldots, x_n,
			x'_1,\ldots, x'_m
				\in F_1,
			y_1, \ldots, y_n,
			y'_1, \ldots, y'_m
				\in F_2
		\right)
	\\
	&&
		\left(
			\left(
%			(
			\frac{
%			(
			x_1y_1 + \cdots + x_ny_n
%			)
			}
%			/
%			(
			{
			x'_1y'_1 + \cdots + x'_my'_m
%			)
			}
%			)
			\right)
			^\sigma
			=
			\frac{
%			(
				x_1y_1 + \cdots + x_ny_n
%			)
			}
%			/
%			(
			{
				x'_1y'_1 + \cdots + x'_my'_m
			}
%			)
		\right),
	\end{eqnarray*}
	hence $\sigma \in G(K/F_1F_2)$,
	thus $G(K/F_1) \cap G(K/F_2)\subset \in G(K/F_1F_2)$.
	Conversely if $\sigma \in G(K/F_1F_2)$,
	$$
		\left(
			\forall x\in F_1, y\in F_2
		\right)
		\left(
			x^\sigma = x
			\ \& \ %
			y^\sigma = y
		\right),
	$$
	hence $\sigma \in G(K/F_1) \cap G(K/F_2)$,
	thus
	$G(K/F_1) \cap G(K/F_2) \subset G(K/F_1F_2)$.
	\qed\
\end{myproof}

%\begin{myproof}{Galois subgroups associated with intermediate fields - 2}
%	First, $K/F_1$ and $K/F_2$ are Galois extensions
%	by \theoremname~\ref{theorem:association of intermediate fields to Galois subgroups},
%	hence $G(K/F_1)$ and $G(K/F_2)$ can be defined.
%	Also, \theoremname~\ref{theorem:retention of normality of extensions}\
%	and \theoremname~\ref{theorem:separable extensions are distinguished}\
%	imply that $K/(F_1\cap F_2)$ is Galois extension,
%	hence $G(K/(F_1\cap F_2))$ can be defined, too.
%
%	Now every automorphism in the smallest subgroup of $G$ containing $G(K/F_1)$ and $G(K/F_2)$
%	leaves
%	every element belonging to both $F_1$ and $F_2$ fixed,
%	hence is in $G(K/(F_1\cap F_2))$.
%	Conversely, every automorphism not in the smallest subgroup of $G$ containing $G(K/F_1)$ and $G(K/F_2)$,
%\end{myproof}

\begin{myproof}{Galois subgroups associated with intermediate fields - 3}
	First, $K/F_1$ and $K/F_2$ are Galois extensions
	by \theoremname~\ref{theorem:Galois subgroups associated with intermediate fields - 1},
	hence $G(K/F_1)$ and $G(K/F_2)$ can be defined.

	If $F_1\subset F_2$,
	every automorphism leaving $F_2$ fixed leaves $F_1$ fixed,
	hence it is in $G(K/F_1)$, thus $G(K/F_2)\subset G(K/F_1)$.
	Conversely, if $G(K/F_2)\subset G(K/F_1)$,
	every intermediate field $G(K/F_1)$ leaves fixed is left fixed by $G(K/F_2)$,
	hence $F_1\subset F_2$.

	Now we can do the same thing using rather mathematically rigorous terms.
	Assume $F_1\subset F_2$
	and that $\sigma\subset G(K/F_2)$.
	Since \theoremname~\ref{theorem:Galois subgroups associated with intermediate fields - 1} implies
	that
	$$
		F_1 \subset F_2 = \set{x\in K}{(\forall \sigma \in G(K/F_2))(x^\sigma=x)},
	$$
	hence
	$
		\left(
			\forall x \in F_1
		\right)
		\left(
			x^\sigma = x
		\right),
	$
	thus $\sigma \in G(K/F_1)$,
	hence
	$$
		G(K/F_2)
		\subset
		G(K/F_1).
	$$
	Conversely,
	assume that $G(K/F_2) \subset G(K/F_1)$.
	Then
	\begin{eqnarray*}
%	$$
		\lefteqn{
		F_1 = \set{x\in K}{(\forall \sigma\in G(K/F_1))(x^\sigma=x)}
		}
		\\
%		\subset
		&\subset&
		\set{x\in K}{(\forall \sigma\in G(K/F_2))(x^\sigma=x)} = F_2
%	$$
	\end{eqnarray*}
	\qed\
\end{myproof}
}{}

\yesnoexec{\ranalysis}{%
\begin{myproof}{Bolzano-Weierstrass-implies-seq-compact}
	if sequence, $\seq{x_n}$, has cluster point, $x$,
	every ball centered at $x$ contains at one least point in sequence, hence, can choose subsequence converging to $x$.
	conversely, if $\seq{x_n}$ has subsequence converging to $x$,
	$x$ is cluster point.\
	\qed\
\end{myproof}

\begin{myproof}{compact-in-metric-implies-seq-compact}
	for $\seq{x_n}$,
	$\seq{\closure{A_n}}$ with $A_m=\seq{b_n}_{n=m}^\infty$
	has \emph{finite intersection property}
	because any finite subcollection $\{A_{n_1},\ldots,A_{n_k}\}$
	contains $x_{n_k}$,
	hence
	\[
		\bigcap \closure{A_n} \neq \emptyset,
	\]

	thus, there exists $x\in X$ contained in every $A_n$.
	$x$ is cluster point because for every $\epsilon>0$ and $N\in\naturals$,
	then $x\in\closure{A_{N+1}}$,
	hence there exists $n> N$ such that $x_n$ contained in ball about $x$ with radius, $\epsilon$.
	hence it's sequentially compact.
	\qed\
\end{myproof}

\begin{myproof}{restriction-of-continuous-topology-continuous}
	because for every open set $O$, $g^{-1}(O) \in \tJ$,
	$A\cap g^{-1}(O)$ is open by definition of \emph{inherited topology}.
	\qed\
\end{myproof}

\begin{myproof}{l-infinity-not-have-natural-representation}
	$C[0,1]$ is \emph{closed} subspace of $L^\infty[0,1]$.
	define $f(x)$ for $x\in C[0,1]$
	such that $f(x) = x(0)\in\reals$.
	$f$ is linear functional because $f(\alpha x+\beta y) = \alpha x(0) + \beta y(0) = \alpha f(x) + \beta (y)$.
	because $|f(x)| = |x(0)| \leq \|x\|_\infty$, $\|f\|\leq 1$.
	for
	$x\in C[0,1]$ such that $x(t)=1$ for $0\leq t\leq 1$, $|f(x)| = 1 = \|x\|_\infty$, hence achieves supremum,
	thus $\|f\|=1$.

	if we define linear functional $p$ on $L^\infty[0,1]$ such that $p(x) = f(x)$, %for every $x\in L^\infty$,
	$p(x+y) = x(0)+y(0)=p(x)+p(y)\leq p(x)+p(y)$, $p(\alpha x)=\alpha x(0) = \alpha p(x)$, and $f(x)\leq p(x)$
	for all $x,y\in L^\infty[0,1]$ and $\alpha \geq0$,
	and $f(s)=p(s)\leq p(s)$ for all $s\in C[0,1]$. Hence, Hahn-Banach theorem implies,
	exists $F:L^\infty[0,1]\to\reals$ such that $F(x)=f(x)$ for every $x\in C[0,1]$
	and $F(x)\leq f(x)$ for every $x\in L^\infty[0,1]$.

	Now assume $y\in L^1[0,1]$ such that $F(x)=\int_{[0,1]} xy$ for $x\in C[0,1]$.
	If we define \seq{x_n}\ in $C[0,1]$
	with $x_n(0)=1$ vanishing outside $t=0$ as $n\to \infty$,
	then
	$\int_{[0,1]} x_n y \to 0$ as $n\to\infty$, but $F(x_n)=1$ for all $n$,
	hence, contradiction.
	Therefore there is not natural representation for $F$.
	\qed\
\end{myproof}

\begin{myproof}{orthonormal-system}
	Assume \seq{\varphi_n}\ is complete, but not maximal.
	Then there exists orthonormal system, $R$, such that $\seq{\varphi_n}\subset R$, but $\seq{\varphi_n}\neq R$.
	Then there exists another $z\in R$ such that $z\not\in\seq{\varphi_n}$.
	But definition $\innerp{z}{\varphi_n}=0$, hence $z=0$.
	But $\|z\|=0$, hence, cannot be member of orthonormal system. contraction,
	hence proved right arrow,
	\ie, sufficient condition (of the former for the latter).

	Now assume that it is maximal.
	Assume there exists $z\neq0\in H$ such that $\innerp{z}{\varphi_n}=0$.
	Then $\seq{\varphi_n}_{n=0}^\infty$ with $\varphi_0=z/\|z\|$
	is anoter orthogonal system containing \seq{\varphi_n},
	hence contradiction, thus
	proved left arrow,
	\ie, necessarily condition.
	\qed\
\end{myproof}
}{}

\yesnoexec{\measprob}{%
\begin{myproof}{central limit theorem}
	Let $Z_n(t) = t^T(X_n-c)$ for $t\in\reals^k$ and $Z(t) = t^TY$.
	Then \seq{Z_n(t)}\ are independent random variables
	having same distribution
	with $\Expect Z_n(t) = t^T(\Expect X_n -c ) = 0$
	and
	$$
		\Var Z_n(t) = \Expect Z_n(t)^2 = t^T \Expect (X_n-c)(X_n-c)^T t = t^T\Sigma t
	$$
	Then by \theoremname~\ref{theorem:Lindeberg-Levy theorem}\
	$\sum^n Z_i(t)/\sqrt{nt^T\Sigma t}$ converges in distribution to standard normal random variable.
	Because $\Expect Z(t) = 0$ and $\Var Z(t) = t^T \Expect Y Y^T z = t^T \Sigma t$,
	for $t\neq0$, $Z(t)/\sqrt{t^T \Sigma t}$ is standard normal random variable.
	Therefore $\sum^n Z_i(t)/\sqrt{nt^T\Sigma t}$ converges in distribution to $Z/\sqrt{t^T \Sigma t}$ for every $t\neq0$,
	thus, $\sum^n Z_i(t)/\sqrt{n} = t^T(\sum^n X_i - nc)/\sqrt{n}$ converges in distribution to $Z(t)=t^T Y$
	for every $t\in\reals$.
	Then \theoremname~\ref{theorem:convergence in distribution of random vector}\ implies
	$(S_n - nc)/\sqrt{n}$ converges in distribution to $Y$.
	\qed\
\end{myproof}
}{}

\yesnoexec{\cvxopt}{%
\begin{myproof}{intersection of convex sets is convex set}
	Suppose \coll\ is a collection of convex sets.
	Suppose $x,y\in \bigcap_{C\in\coll} C$ and $0< \theta<1$.
	Then for each $C\in \coll$ and $\theta x + (1-\theta)y\in C$,
	hence, $\theta x + (1-\theta)y\in \bigcap_{C\in\coll} C$,
	$\bigcap_{C\in\coll} C$ is a convex set.
	\qed\
\end{myproof}

\begin{myproof}{theorem of alternative for linear strict generalized inequalities}
	Suppose $A x\prec_{K} b$ is infeasible.
	Then $\set{b-Ax}{x\in\reals^n} \cap \interior{K} = \emptyset$.
	\theoremname~\ref{theorem:separating hyperplane theorem}
	implies
	there exist nonzero $\lambda\in\reals^n$ and $c\in\reals$
	such that
%	$$
	\begin{equation}
	\label{eq:djkfn-1}
		\left(
			\forall x\in\reals^n
		\right)
		\left(
			\lambda^T(b-Ax)\leq c
		\right)
	\end{equation}
%	$$
	and
%	$$
	\begin{equation}
	\label{eq:djkfn-2}
		\left(
			\forall y\in\interior{K}
		\right)
		\left(
			\lambda^Ty \geq c
		\right).
	\end{equation}
%	$$
	The former equation (\ref{eq:djkfn-1})
	implies $\lambda^TA = 0$ and $\lambda^Tb\leq c$.
	and the latter $a\succeq_{K^\ast}0$.
	If $c>0$, there exists $y\in \interior{K}$ such that $\lambda^T y \geq c >0$.
	Then $\lambda^T ((c/2\lambda^T y)y) = c/2 < c$,
	but $(c/2\lambda^T y)y\in \interior{K}$, hence contradiction.
	Thus, $c\leq0$.
	If $\lambda^T y <0$ for some $y\in \interior{K}$,
	then $\alpha y \in \interior{K}$ for any $\alpha>0$,
	thus there exists $z\in \interior{K}$ which makes $\lambda^T z$ arbitrarily large toward $-\infty$.
	Therefore $\lambda^T y$ is nonnegative for every $y\in \interior{K}$.
	Then the latter equation (\ref{eq:djkfn-2})
	implies
	$
		\left(
			\forall y\in\interior{K}
		\right)
		\left(
			\lambda^Ty \geq 0
		\right),
	$
	hence $\lambda\in K^\ast$ (by \definitionname~\ref{definition:dual cones}).
	Therefore we have
	$$
		\lambda \neq0,\ %
		\lambda \succeq_{K^\ast} 0,\ %
		A^T \lambda = 0,\ %
		\lambda^T b \leq0.
	$$
	Conversely, assume that all of above are satisfied.
	Then for every $x\in\reals^n$,
	there exists nonzero $\lambda \succeq_{K^\ast}0$
	such that
	$$
		\lambda^T (Ax) \geq \lambda^Tb,
	$$
	thus \propositionname~\ref{proposition:generalized inequalities and dual generalized inequalities}\
	implies $Ax \not\prec_K b$.
	\qed\
\end{myproof}

\begin{myproof}{convexity of infimum of convex function}
	Note
	\begin{eqnarray*}
		\lefteqn{
				\epi \inf_{y\in C} f(x,y)
				=\set{(x,t)}{(\forall \epsilon >0)(\exists y\in C)( f(x,y)\leq t + \epsilon)}
		}
		\\
		&=&
			\bigcap_{n\in\naturals} \bigset{(x,t)}{(\exists y\in C)( f(x,y, t + 1/n) \in \epi f)}
		\\
		&=&
			\bigcap_{n\in\naturals} \left(\bigset{(x,t)}{(\exists y\in C)( f(x,y, t) \in \epi f)} - (0,1/n)\right)
	\end{eqnarray*}
	where $\bigset{(x,t)}{(\exists y\in C)( f(x,y, t) \in \epi f)} - (0,1/n)$
	for each $n$ since $\epi f$ is convex and projection of a convex set is convex.
	Since the intersection of any collection of convex sets is convex,
	$\epi \inf_{y\in C} f(x,y)$ is convex,
	thus $\inf_{y\in C} f(x,y)$ is convex function.
	\qed\
\end{myproof}

\begin{myproof}{Lagrange dual is lower bound for optimal value}
	For every $\lambda\succeq 0$ and $y\in\optfeasset$
	$$
		g(\lambda, \nu)
		\leq
		\fobj(y) + \lambda^T \fie(y) + \nu^T \feq(y)
		\leq
		\fobj(y)
		\leq
		\inf_{x\in\optfeasset} \fobj(x) = p^\ast.
	$$
	\qed
\end{myproof}

\begin{myproof}{max-min inequality}
	For every $x\in X, y\in Y$
	$$
		f(x,y) \leq \sup_{x'\in X} f(x',y)
	$$
	hence
	for every $x\in X$
	$$
		\inf_{y''\in Y} f(x,y'') \leq \inf_{y'\in Y} \sup_{x'\in X} f(x',y')
	$$
	\ie,
	$\inf_{y'\in Y} \sup_{x'\in X} f(x',y')$ is upper bound of $\inf_{y''\in Y} f(x,y'')$,
	hence
	$$
		\sup_{x\in X}\inf_{y''\in Y} f(x,y'') \leq \inf_{y'\in Y} \sup_{x'\in X} f(x',y')
	$$
	\qed\
\end{myproof}

\begin{myproof}{epigraph of convex optimization is convex}
	Assume $(u_1,v_1,t_1), (u_2,v_2,t_2)\in H$.
	Then there exist $x_1,x_2\in\optdomain$
	such that
	$\fie(x_1) \preceq u_1$,
	$\feq(x_1) = v_1$,
	$\fobj(x_1) \leq t_1$,
	$\fie(x_2) \preceq u_2$,
	$\feq(x_2) = v_2$,
	and
	$\fobj(x_2) \leq t_2$.
	Then for every $0\leq\theta\leq 1$
	\begin{eqnarray*}
	%$$
	&&
	\fie(\theta x_1 + (1-\theta) x_2)
	\preceq \theta \fie(x_1) + (1-\theta) \fie(x_2)
	= \theta u_1 + (1-\theta) u_2
	%$$
	%$$
	\\&&
	\feq(\theta x_1 + (1-\theta) x_2)
	= \theta \feq(x_1) + (1-\theta) \feq(x_2)
	= \theta v_1 + (1-\theta) v_2
	%$$
	%$$
	\\&&
	\fobj(\theta x_1 + (1-\theta) x_2)
	\preceq \theta \fobj(x_1) + (1-\theta) \fobj(x_2)
	= \theta t_1 + (1-\theta) t_2
	%$$
	\end{eqnarray*}
	thus
	$\theta (u_1,v_1,t_1) + (1-\theta) (u_2,v_2,t_2)\in H$,
	hence $H$ is a convex set. \qed\
\end{myproof}
}{}
\eit

}{}

}

\TITLEFOIL{References}{References}
\myfoilhead{}

%\bibliographystyle{unsrt}
%\bibliographystyle{abbrv}
%\bibliographystyle{acm}
%\bibliographystyle{apalike}
\bibliographystyle{alpha}
%\bibliographystyle{ieeetr}
%\bibliographystyle{plain}
%\bibliographystyle{siam}
\bibliography{../../FrequentlyUsed/latex/mybib}


\yesnoexec{\ynindex}{%
\TITLEFOIL{Index}{Index}

{
%\tiny
%\scriptsize
%\footnotesize
%\normalsize
\small \printindex
}

}{}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\titlefoil{Modules}{Modules}
%
%\myfoilhead{Before introducing modules - basis of vector spaces}
%
%\bit
%\item
%	vector space has basis - in standard fact
%
%\vitem
%	not always case for modules
%
%\vitem
%	sometimes they do; most often they do \emph{not}\
%\eit
%\vfill
%
%
%\myfoilhead{Basic definitions}
%
%\begin{mydefinition}{modules}\
%	for ring, $A$,
%	abelian group, $M$, usually written additively,
%	together with operation of $A$ on $M$
%	- viewing $A$ as multiplicative monoid -
%	such that
%	and set, $M$,
%	$$
%		\left(
%			a, b \in A, x, y \in M
%		\right)
%		\left(
%			(a+b)x = ax + bx \mbox{ \& }
%			a(x+y) = ax + ay
%		\right)
%	$$
%	called \define{left module over $A$}
%	or \define{left $A$-module};%
%		\index{left module over $A$}%
%		\index{left $A$-module}%
%		\index{modules!left module over $A$}%
%		\index{modules!left $A$-module}\
%	similarly for \define{right $A$-module};%
%		\index{right module over $A$}%
%		\index{right $A$-module}%
%		\index{modules!right module over $A$}%
%		\index{modules!right $A$-module}\
%	unless otherwise specified,
%	will always mean left modules,
%	hence call left modules,
%	\define{$A$-modules} or just \define{modules}
%		\index{module over $A$}%
%		\index{$A$-module}%
%		\index{modules!module over $A$}%
%		\index{modules!$A$-module}%
%\end{mydefinition}
%
%\begin{mydefinition}{submodules}%
%		\index{modules!submodules}\
%	for $A$-module, $M$,
%	additive subgroup, $N\subset M$, such that $AN\subset N$,
%	called \define{submodules of $M$};
%	submodule, $N$, is module
%	\wrt\ operations induced by those of $A$ on $M$\
%\end{mydefinition}
%
%\bit
%\vitem
%	ring, $A$ is module over itself
%\vitem
%	every abelian group is $\integers$-module
%\vitem
%	additive group consisting of $0$ alone is module over any ring
%\vitem
%	left ideal of ring, $A$, is module over $A$
%\eit
%
%
%\myfoilhead{Vector spaces}
%
%\begin{mydefinition}{vector spaces}%
%		\index{vector spaces!modules}%
%		\index{modules!vector spaces}\
%	module over field,
%	called \define{vector space}
%\end{mydefinition}
%
%
%\bit
%\item
%	for ring, $R$, of all linear maps of vector space into itself,
%	vector space is module over $R$
%
%\item
%	vector space, $K^n$, is module over ring, $K^{n\times n}$
%\eit
%
%
%\myfoilhead{The group of homeomorphisms}
%
%
%\myfoilhead{Direct products and sums of modules}
%
%
%\myfoilhead{Free modules}
%
%
%\myfoilhead{Vector spaces}
%
%
%\myfoilhead{The dual space and dual module}
%
%
%\myfoilhead{Modules over principal rings}
%
%
%\myfoilhead{Euler-Poincar\'{e} maps}
%
%
%\myfoilhead{The snake lemma}
%
%
%\myfoilhead{Direct and inverse limits}
%
%
